from tinygrad.tensor import Tensor                                                                                                                                                                       #     ...grad.tensor.tolist_CLANG.py:     1: G: {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': 'examples/tinygrad.tensor.tolist_CLANG.py', '__cached__': None}    
  from tinygrad.tensor import Tensor                            # noqa: F401                                                                                                                             #     __init__.py                   :     1: G: {'__name__': 'tinygrad', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789735f92200>, '__spec__': ModuleSpec(name='tinygrad', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789735f92200>, origin='/home/lorinbaum/code/tinygrad/tinygrad/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/__init__.cpython-310.pyc'}    
    # inspired by https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py
    from __future__ import annotations                                                                                                                                                                   #     tensor.py                     :     2: G: {'__name__': 'tinygrad.tensor', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789735f907c0>, '__spec__': ModuleSpec(name='tinygrad.tensor', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789735f907c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/tensor.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/tensor.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/tensor.cpython-310.pyc'}    
    import dataclasses                                                                                                                                                                                   #     tensor.py                     :     3: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
    import time, math, itertools, functools, struct, sys, inspect, pathlib                                                                                                                               #     tensor.py                     :     4: G: {'dataclasses': <module 'dataclasses' from '/usr/lib/python3.10/dataclasses.py'>}    
    from contextlib import ContextDecorator                                                                                                                                                              #     tensor.py                     :     5: G: {'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'itertools': <module 'itertools' (built-in)>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'sys': <module 'sys' (built-in)>, 'inspect': <module 'inspect' from '/usr/lib/python3.10/inspect.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>}    
    from typing import List, Tuple, Callable, Optional, ClassVar, Type, Union, Sequence, Dict, DefaultDict, cast, get_args, Set                                                                          #     tensor.py                     :     6: G: {'ContextDecorator': <class 'contextlib.ContextDecorator'>}    
    from collections import defaultdict                                                                                                                                                                  #     tensor.py                     :     7: G: {'List': typing.List, 'Tuple': typing.Tuple, 'Callable': typing.Callable, 'Optional': typing.Optional, 'ClassVar': typing.ClassVar, 'Type': typing.Type, 'Union': typing.Union, 'Sequence': typing.Sequence, 'Dict': typing.Dict, 'DefaultDict': typing.DefaultDict, 'cast': <function cast at 0x789735eb17e0>, 'get_args': <function get_args at 0x789735eb1ab0>, 'Set': typing.Set}    
    import numpy as np                                                                                                                                                                                   #     tensor.py                     :     8: G: {'defaultdict': <class 'collections.defaultdict'>}    
    
    from tinygrad.dtype import DType, DTypeLike, dtypes, ImageDType, ConstType, least_upper_float, least_upper_dtype, sum_acc_dtype, to_dtype                                                            #     tensor.py                     :    10: G: {'np': <module 'numpy' from '/home/lorinbaum/.local/lib/python3.10/site-packages/numpy/__init__.py'>}    
      from typing import Final, Optional, ClassVar, Set, Tuple, Dict, Union                                                                                                                              #     dtype.py                      :     1: G: {'__name__': 'tinygrad.dtype', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278de40>, '__spec__': ModuleSpec(name='tinygrad.dtype', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278de40>, origin='/home/lorinbaum/code/tinygrad/tinygrad/dtype.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/dtype.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/dtype.cpython-310.pyc'}    
      from dataclasses import dataclass                                                                                                                                                                  #     dtype.py                      :     2: G: {'Final': typing.Final, 'Optional': typing.Optional, 'ClassVar': typing.ClassVar, 'Set': typing.Set, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Union': typing.Union}    
      import functools                                                                                                                                                                                   #     dtype.py                      :     3: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
      from tinygrad.helpers import getenv                                                                                                                                                                #     dtype.py                      :     4: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
        from __future__ import annotations                                                                                                                                                               #     helpers.py                    :     1: G: {'__name__': 'tinygrad.helpers', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc'}    
        import os, functools, platform, time, re, contextlib, operator, hashlib, pickle, sqlite3, tempfile, pathlib, string, ctypes, sys, gzip                                                           #     helpers.py                    :     2: G: {'__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
        import itertools, urllib.request, subprocess, shutil, math, json, contextvars                                                                                                                    #     helpers.py                    :     3: G: {'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'platform': <module 'platform' from '/usr/lib/python3.10/platform.py'>, 'time': <module 'time' (built-in)>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'sqlite3': <module 'sqlite3' from '/usr/lib/python3.10/sqlite3/__init__.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'gzip': <module 'gzip' from '/usr/lib/python3.10/gzip.py'>}    
        from dataclasses import dataclass                                                                                                                                                                #     helpers.py                    :     4: G: {'itertools': <module 'itertools' (built-in)>, 'urllib': <module 'urllib' from '/usr/lib/python3.10/urllib/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'math': <module 'math' (built-in)>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'contextvars': <module 'contextvars' from '/usr/lib/python3.10/contextvars.py'>}    
        from typing import Dict, Tuple, Union, List, ClassVar, Optional, Iterable, Any, TypeVar, TYPE_CHECKING, Callable, Sequence                                                                       #     helpers.py                    :     5: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
        if TYPE_CHECKING:  # TODO: remove this and import TypeGuard from typing once minimum python supported version is 3.10                                                                            #     helpers.py                    :     6: G: {'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Union': typing.Union, 'List': typing.List, 'ClassVar': typing.ClassVar, 'Optional': typing.Optional, 'Iterable': typing.Iterable, 'Any': typing.Any, 'TypeVar': <class 'typing.TypeVar'>, 'TYPE_CHECKING': False, 'Callable': typing.Callable, 'Sequence': typing.Sequence}    
        
        T = TypeVar("T")                                                                                                                                                                                 #     helpers.py                    :    10: 
        U = TypeVar("U")                                                                                                                                                                                 #     helpers.py                    :    11: G: {'T': ~T}    
        
        # NOTE: helpers is not allowed to import from anything else in tinygrad
        OSX = platform.system() == "Darwin"                                                                                                                                                              #     helpers.py                    :    16: G: {'U': ~U, 'prod': <function prod at 0x7897327a7520>}    
        CI = os.getenv("CI", "") != ""                                                                                                                                                                   #     helpers.py                    :    17: G: {'OSX': False}    

          class Context(contextlib.ContextDecorator):                                                                                                                                                    #     helpers.py                    :    83: 
            stack: ClassVar[List[dict[str, int]]] = [{}]                                                                                                                                                 #     helpers.py                    :    84: G: {'CI': False, 'dedup': <function dedup at 0x789730f581f0>, 'argfix': <function argfix at 0x789730f58310>, 'argsort': <function argsort at 0x789730f583a0>, 'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'colorize_float': <function colorize_float at 0x789730f585e0>, 'memsize_to_str': <function memsize_to_str at 0x789730f58670>, 'ansistrip': <function ansistrip at 0x789730f58700>, 'ansilen': <function ansilen at 0x789730f58790>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'fromimport': <function fromimport at 0x789730f589d0>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'round_up': <function round_up at 0x789730f58af0>, 'data64': <function data64 at 0x789730f58b80>, 'data64_le': <function data64_le at 0x789730f58c10>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'partition': <function partition at 0x789730f58d30>, 'unwrap': <function unwrap at 0x789730f58dc0>, 'unwrap2': <function unwrap2 at 0x789730f58e50>, 'get_child': <function get_child at 0x789730f58ee0>, 'get_shape': <function get_shape at 0x789730f58f70>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'temp': <function temp at 0x789730f59090>}    L: {'__module__': 'tinygrad.helpers', '__qualname__': 'Context', '__annotations__': {}}    

          class ContextVar:                                                                                                                                                                              #     helpers.py                    :    93: 
            _cache: ClassVar[Dict[str, ContextVar]] = {}                                                                                                                                                 #     helpers.py                    :    94: G: {'Context': <class 'tinygrad.helpers.Context'>}    L: {'__qualname__': 'ContextVar', '__annotations__': {}}    
            value: int                                                                                                                                                                                   #     helpers.py                    :    95: L: {'_cache': {}}    
            key: str                                                                                                                                                                                     #     helpers.py                    :    96: 

        
        DEBUG, IMAGE, BEAM, NOOPT, JIT = ContextVar("DEBUG", 0), ContextVar("IMAGE", 0), ContextVar("BEAM", 0), ContextVar("NOOPT", 0), ContextVar("JIT", 1)                                             #     helpers.py                    :   107: G: {'ContextVar': <class 'tinygrad.helpers.ContextVar'>}    L: {'__name__': 'tinygrad.helpers', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'platform': <module 'platform' from '/usr/lib/python3.10/platform.py'>, 'time': <module 'time' (built-in)>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'sqlite3': <module 'sqlite3' from '/usr/lib/python3.10/sqlite3/__init__.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'gzip': <module 'gzip' from '/usr/lib/python3.10/gzip.py'>, 'itertools': <module 'itertools' (built-in)>, 'urllib': <module 'urllib' from '/usr/lib/python3.10/urllib/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'math': <module 'math' (built-in)>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'contextvars': <module 'contextvars' from '/usr/lib/python3.10/contextvars.py'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Union': typing.Union, 'List': typing.List, 'ClassVar': typing.ClassVar, 'Optional': typing.Optional, 'Iterable': typing.Iterable, 'Any': typing.Any, 'TypeVar': <class 'typing.TypeVar'>, 'TYPE_CHECKING': False, 'Callable': typing.Callable, 'Sequence': typing.Sequence, 'T': ~T, 'U': ~U, 'prod': <function prod at 0x7897327a7520>, 'OSX': False, 'CI': False, 'dedup': <function dedup at 0x789730f581f0>, 'argfix': <function argfix at 0x789730f58310>, 'argsort': <function argsort at 0x789730f583a0>, 'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'colorize_float': <function colorize_float at 0x789730f585e0>, 'memsize_to_str': <function memsize_to_str at 0x789730f58670>, 'ansistrip': <function ansistrip at 0x789730f58700>, 'ansilen': <function ansilen at 0x789730f58790>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'fromimport': <function fromimport at 0x789730f589d0>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'round_up': <function round_up at 0x789730f58af0>, 'data64': <function data64 at 0x789730f58b80>, 'data64_le': <function data64_le at 0x789730f58c10>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'partition': <function partition at 0x789730f58d30>, 'unwrap': <function unwrap at 0x789730f58dc0>, 'unwrap2': <function unwrap2 at 0x789730f58e50>, 'get_child': <function get_child at 0x789730f58ee0>, 'get_shape': <function get_shape at 0x789730f58f70>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'temp': <function temp at 0x789730f59090>, 'Context': <class 'tinygrad.helpers.Context'>}    

          class ContextVar:                                                                                                                                                                              #     helpers.py                    :    93: 
            def __new__(cls, key, default_value):                                                                                                                                                        #     helpers.py                    :    97: 
              if key in ContextVar._cache: return ContextVar._cache[key]                                                                                                                                 #     helpers.py                    :    98: L: {'cls': <class 'tinygrad.helpers.ContextVar'>, 'key': 'DEBUG', 'default_value': 0, '__class__': <class 'tinygrad.helpers.ContextVar'>}    
              instance = ContextVar._cache[key] = super().__new__(cls)                                                                                                                                   #     helpers.py                    :    99: 
              instance.value, instance.key = getenv(key, default_value), key                                                                                                                             #     helpers.py                    :   100: L: {'instance': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>}    

                @functools.lru_cache(maxsize=None)                                                                                                                                                       #     helpers.py                    :    80: 
                def getenv(key:str, default=0): return type(default)(os.getenv(key, default))                                                                                                                 

              return instance                                                                                                                                                                            #     helpers.py                    :   101: 

        WINO, THREEFRY, CAPTURING, TRACEMETA = ContextVar("WINO", 0), ContextVar("THREEFRY", 0), ContextVar("CAPTURING", 1), ContextVar("TRACEMETA", 1)                                                  #     helpers.py                    :   108: G: {'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'IMAGE': <tinygrad.helpers.ContextVar object at 0x789730f3bdf0>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'NOOPT': <tinygrad.helpers.ContextVar object at 0x789730f3be50>, 'JIT': <tinygrad.helpers.ContextVar object at 0x789730f60190>}    L: {'__name__': 'tinygrad.helpers', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'platform': <module 'platform' from '/usr/lib/python3.10/platform.py'>, 'time': <module 'time' (built-in)>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'sqlite3': <module 'sqlite3' from '/usr/lib/python3.10/sqlite3/__init__.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'gzip': <module 'gzip' from '/usr/lib/python3.10/gzip.py'>, 'itertools': <module 'itertools' (built-in)>, 'urllib': <module 'urllib' from '/usr/lib/python3.10/urllib/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'math': <module 'math' (built-in)>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'contextvars': <module 'contextvars' from '/usr/lib/python3.10/contextvars.py'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Union': typing.Union, 'List': typing.List, 'ClassVar': typing.ClassVar, 'Optional': typing.Optional, 'Iterable': typing.Iterable, 'Any': typing.Any, 'TypeVar': <class 'typing.TypeVar'>, 'TYPE_CHECKING': False, 'Callable': typing.Callable, 'Sequence': typing.Sequence, 'T': ~T, 'U': ~U, 'prod': <function prod at 0x7897327a7520>, 'OSX': False, 'CI': False, 'dedup': <function dedup at 0x789730f581f0>, 'argfix': <function argfix at 0x789730f58310>, 'argsort': <function argsort at 0x789730f583a0>, 'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'colorize_float': <function colorize_float at 0x789730f585e0>, 'memsize_to_str': <function memsize_to_str at 0x789730f58670>, 'ansistrip': <function ansistrip at 0x789730f58700>, 'ansilen': <function ansilen at 0x789730f58790>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'fromimport': <function fromimport at 0x789730f589d0>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'round_up': <function round_up at 0x789730f58af0>, 'data64': <function data64 at 0x789730f58b80>, 'data64_le': <function data64_le at 0x789730f58c10>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'partition': <function partition at 0x789730f58d30>, 'unwrap': <function unwrap at 0x789730f58dc0>, 'unwrap2': <function unwrap2 at 0x789730f58e50>, 'get_child': <function get_child at 0x789730f58ee0>, 'get_shape': <function get_shape at 0x789730f58f70>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'temp': <function temp at 0x789730f59090>, 'Context': <class 'tinygrad.helpers.Context'>, 'ContextVar': <class 'tinygrad.helpers.ContextVar'>}    
        GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING = ContextVar("GRAPH", 0), getenv("GRAPHPATH", "/tmp/net"), ContextVar("SAVE_SCHEDULE", 0), ContextVar("RING", 1)                                           #     helpers.py                    :   109: G: {'WINO': <tinygrad.helpers.ContextVar object at 0x789730f600a0>, 'THREEFRY': <tinygrad.helpers.ContextVar object at 0x789730f602e0>, 'CAPTURING': <tinygrad.helpers.ContextVar object at 0x789730f60280>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>}    
        MULTIOUTPUT, PROFILE, PROFILEPATH = ContextVar("MULTIOUTPUT", 1), ContextVar("PROFILE", 0), ContextVar("PROFILEPATH", temp("tinygrad_profile.json"))                                             #     helpers.py                    :   110: G: {'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'GRAPHPATH': '/tmp/net', 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'RING': <tinygrad.helpers.ContextVar object at 0x789730f60130>}    

          def temp(x:str) -> str: return (pathlib.Path(tempfile.gettempdir()) / x).as_posix()                                                                                                            #     helpers.py                    :    81: 

        USE_TC, TC_OPT, TRANSCENDENTAL = ContextVar("TC", 1), ContextVar("TC_OPT", 0), ContextVar("TRANSCENDENTAL", 1)                                                                                   #     helpers.py                    :   111: G: {'MULTIOUTPUT': <tinygrad.helpers.ContextVar object at 0x789730f60430>, 'PROFILE': <tinygrad.helpers.ContextVar object at 0x789730f60580>, 'PROFILEPATH': <tinygrad.helpers.ContextVar object at 0x789730f60550>}    
        FUSE_ARANGE, FUSE_CONV_BW = ContextVar("FUSE_ARANGE", 0), ContextVar("FUSE_CONV_BW", 0)                                                                                                          #     helpers.py                    :   112: G: {'USE_TC': <tinygrad.helpers.ContextVar object at 0x789730f606a0>, 'TC_OPT': <tinygrad.helpers.ContextVar object at 0x789730f60760>, 'TRANSCENDENTAL': <tinygrad.helpers.ContextVar object at 0x789730f60670>}    
        SPLIT_REDUCEOP, ARANGE_DIFF = ContextVar("SPLIT_REDUCEOP", 1), ContextVar("ARANGE_DIFF", 0)                                                                                                      #     helpers.py                    :   113: G: {'FUSE_ARANGE': <tinygrad.helpers.ContextVar object at 0x789730f60490>, 'FUSE_CONV_BW': <tinygrad.helpers.ContextVar object at 0x789730f604c0>}    

          @dataclass(frozen=True)                                                                                                                                                                        #     helpers.py                    :   116: 
          class Metadata:                                                                                                                                                                                     
            name: str                                                                                                                                                                                    #     helpers.py                    :   117: G: {'SPLIT_REDUCEOP': <tinygrad.helpers.ContextVar object at 0x789730f60640>, 'ARANGE_DIFF': <tinygrad.helpers.ContextVar object at 0x789730f60850>}    L: {'__module__': 'tinygrad.helpers', '__qualname__': 'Metadata', '__annotations__': {}}    
            caller: str                                                                                                                                                                                  #     helpers.py                    :   118: 
            backward: bool = False                                                                                                                                                                       #     helpers.py                    :   119: 

        _METADATA: contextvars.ContextVar[Optional[Metadata]] = contextvars.ContextVar("_METADATA", default=None)                                                                                        #     helpers.py                    :   123: G: {'Metadata': <class 'tinygrad.helpers.Metadata'>}    L: {'__name__': 'tinygrad.helpers', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'platform': <module 'platform' from '/usr/lib/python3.10/platform.py'>, 'time': <module 'time' (built-in)>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'sqlite3': <module 'sqlite3' from '/usr/lib/python3.10/sqlite3/__init__.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'gzip': <module 'gzip' from '/usr/lib/python3.10/gzip.py'>, 'itertools': <module 'itertools' (built-in)>, 'urllib': <module 'urllib' from '/usr/lib/python3.10/urllib/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'math': <module 'math' (built-in)>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'contextvars': <module 'contextvars' from '/usr/lib/python3.10/contextvars.py'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Union': typing.Union, 'List': typing.List, 'ClassVar': typing.ClassVar, 'Optional': typing.Optional, 'Iterable': typing.Iterable, 'Any': typing.Any, 'TypeVar': <class 'typing.TypeVar'>, 'TYPE_CHECKING': False, 'Callable': typing.Callable, 'Sequence': typing.Sequence, 'T': ~T, 'U': ~U, 'prod': <function prod at 0x7897327a7520>, 'OSX': False, 'CI': False, 'dedup': <function dedup at 0x789730f581f0>, 'argfix': <function argfix at 0x789730f58310>, 'argsort': <function argsort at 0x789730f583a0>, 'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'colorize_float': <function colorize_float at 0x789730f585e0>, 'memsize_to_str': <function memsize_to_str at 0x789730f58670>, 'ansistrip': <function ansistrip at 0x789730f58700>, 'ansilen': <function ansilen at 0x789730f58790>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'fromimport': <function fromimport at 0x789730f589d0>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'round_up': <function round_up at 0x789730f58af0>, 'data64': <function data64 at 0x789730f58b80>, 'data64_le': <function data64_le at 0x789730f58c10>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'partition': <function partition at 0x789730f58d30>, 'unwrap': <function unwrap at 0x789730f58dc0>, 'unwrap2': <function unwrap2 at 0x789730f58e50>, 'get_child': <function get_child at 0x789730f58ee0>, 'get_shape': <function get_shape at 0x789730f58f70>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'temp': <function temp at 0x789730f59090>, 'Context': <class 'tinygrad.helpers.Context'>, 'ContextVar': <class 'tinygrad.helpers.ContextVar'>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'IMAGE': <tinygrad.helpers.ContextVar object at 0x789730f3bdf0>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'NOOPT': <tinygrad.helpers.ContextVar object at 0x789730f3be50>, 'JIT': <tinygrad.helpers.ContextVar object at 0x789730f60190>, 'WINO': <tinygrad.helpers.ContextVar object at 0x789730f600a0>, 'THREEFRY': <tinygrad.helpers.ContextVar object at 0x789730f602e0>, 'CAPTURING': <tinygrad.helpers.ContextVar object at 0x789730f60280>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>, 'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'GRAPHPATH': '/tmp/net', 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'RING': <tinygrad.helpers.ContextVar object at 0x789730f60130>, 'MULTIOUTPUT': <tinygrad.helpers.ContextVar object at 0x789730f60430>, 'PROFILE': <tinygrad.helpers.ContextVar object at 0x789730f60580>, 'PROFILEPATH': <tinygrad.helpers.ContextVar object at 0x789730f60550>, 'USE_TC': <tinygrad.helpers.ContextVar object at 0x789730f606a0>, 'TC_OPT': <tinygrad.helpers.ContextVar object at 0x789730f60760>, 'TRANSCENDENTAL': <tinygrad.helpers.ContextVar object at 0x789730f60670>, 'FUSE_ARANGE': <tinygrad.helpers.ContextVar object at 0x789730f60490>, 'FUSE_CONV_BW': <tinygrad.helpers.ContextVar object at 0x789730f604c0>, 'SPLIT_REDUCEOP': <tinygrad.helpers.ContextVar object at 0x789730f60640>, 'ARANGE_DIFF': <tinygrad.helpers.ContextVar object at 0x789730f60850>}    

          class GlobalCounters:                                                                                                                                                                          #     helpers.py                    :   127: 
            global_ops: ClassVar[int] = 0                                                                                                                                                                #     helpers.py                    :   128: G: {'_METADATA': <ContextVar name='_METADATA' default=None at 0x789730f470b0>}    L: {'__module__': 'tinygrad.helpers', '__qualname__': 'GlobalCounters', '__annotations__': {}}    
            global_mem: ClassVar[int] = 0                                                                                                                                                                #     helpers.py                    :   129: L: {'global_ops': 0}    
            time_sum_s: ClassVar[float] = 0.0                                                                                                                                                            #     helpers.py                    :   130: L: {'global_mem': 0}    
            kernel_count: ClassVar[int] = 0                                                                                                                                                              #     helpers.py                    :   131: L: {'time_sum_s': 0.0}    
            mem_used: ClassVar[int] = 0   # NOTE: this is not reset                                                                                                                                      #     helpers.py                    :   132: L: {'kernel_count': 0}    

          class ProfileLogger:                                                                                                                                                                           #     helpers.py                    :   166: 
            writers: int = 0                                                                                                                                                                             #     helpers.py                    :   167: G: {'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'Timing': <class 'tinygrad.helpers.Timing'>, '_format_fcn': <function _format_fcn at 0x789730f59360>, 'Profiling': <class 'tinygrad.helpers.Profiling'>}    L: {'__qualname__': 'ProfileLogger', '__annotations__': {}}    
            mjson: List[Dict] = []                                                                                                                                                                       #     helpers.py                    :   168: L: {'writers': 0}    
            actors: Dict[Union[str, Tuple[str, str]], int] = {}                                                                                                                                          #     helpers.py                    :   169: L: {'mjson': []}    

        
        # *** universal database cache ***
        
        _cache_dir: str = getenv("XDG_CACHE_HOME", os.path.expanduser("~/Library/Caches" if OSX else "~/.cache"))                                                                                        #     helpers.py                    :   206: G: {'ProfileLogger': <class 'tinygrad.helpers.ProfileLogger'>}    L: {'__name__': 'tinygrad.helpers', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278e5c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc', '__annotations__': {'_METADATA': 'contextvars.ContextVar[Optional[Metadata]]'}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'platform': <module 'platform' from '/usr/lib/python3.10/platform.py'>, 'time': <module 'time' (built-in)>, 're': <module 're' from '/usr/lib/python3.10/re.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'sqlite3': <module 'sqlite3' from '/usr/lib/python3.10/sqlite3/__init__.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'string': <module 'string' from '/usr/lib/python3.10/string.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'sys': <module 'sys' (built-in)>, 'gzip': <module 'gzip' from '/usr/lib/python3.10/gzip.py'>, 'itertools': <module 'itertools' (built-in)>, 'urllib': <module 'urllib' from '/usr/lib/python3.10/urllib/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'shutil': <module 'shutil' from '/usr/lib/python3.10/shutil.py'>, 'math': <module 'math' (built-in)>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'contextvars': <module 'contextvars' from '/usr/lib/python3.10/contextvars.py'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Union': typing.Union, 'List': typing.List, 'ClassVar': typing.ClassVar, 'Optional': typing.Optional, 'Iterable': typing.Iterable, 'Any': typing.Any, 'TypeVar': <class 'typing.TypeVar'>, 'TYPE_CHECKING': False, 'Callable': typing.Callable, 'Sequence': typing.Sequence, 'T': ~T, 'U': ~U, 'prod': <function prod at 0x7897327a7520>, 'OSX': False, 'CI': False, 'dedup': <function dedup at 0x789730f581f0>, 'argfix': <function argfix at 0x789730f58310>, 'argsort': <function argsort at 0x789730f583a0>, 'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'colorize_float': <function colorize_float at 0x789730f585e0>, 'memsize_to_str': <function memsize_to_str at 0x789730f58670>, 'ansistrip': <function ansistrip at 0x789730f58700>, 'ansilen': <function ansilen at 0x789730f58790>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'fromimport': <function fromimport at 0x789730f589d0>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'round_up': <function round_up at 0x789730f58af0>, 'data64': <function data64 at 0x789730f58b80>, 'data64_le': <function data64_le at 0x789730f58c10>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'partition': <function partition at 0x789730f58d30>, 'unwrap': <function unwrap at 0x789730f58dc0>, 'unwrap2': <function unwrap2 at 0x789730f58e50>, 'get_child': <function get_child at 0x789730f58ee0>, 'get_shape': <function get_shape at 0x789730f58f70>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'temp': <function temp at 0x789730f59090>, 'Context': <class 'tinygrad.helpers.Context'>, 'ContextVar': <class 'tinygrad.helpers.ContextVar'>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'IMAGE': <tinygrad.helpers.ContextVar object at 0x789730f3bdf0>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'NOOPT': <tinygrad.helpers.ContextVar object at 0x789730f3be50>, 'JIT': <tinygrad.helpers.ContextVar object at 0x789730f60190>, 'WINO': <tinygrad.helpers.ContextVar object at 0x789730f600a0>, 'THREEFRY': <tinygrad.helpers.ContextVar object at 0x789730f602e0>, 'CAPTURING': <tinygrad.helpers.ContextVar object at 0x789730f60280>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>, 'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'GRAPHPATH': '/tmp/net', 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'RING': <tinygrad.helpers.ContextVar object at 0x789730f60130>, 'MULTIOUTPUT': <tinygrad.helpers.ContextVar object at 0x789730f60430>, 'PROFILE': <tinygrad.helpers.ContextVar object at 0x789730f60580>, 'PROFILEPATH': <tinygrad.helpers.ContextVar object at 0x789730f60550>, 'USE_TC': <tinygrad.helpers.ContextVar object at 0x789730f606a0>, 'TC_OPT': <tinygrad.helpers.ContextVar object at 0x789730f60760>, 'TRANSCENDENTAL': <tinygrad.helpers.ContextVar object at 0x789730f60670>, 'FUSE_ARANGE': <tinygrad.helpers.ContextVar object at 0x789730f60490>, 'FUSE_CONV_BW': <tinygrad.helpers.ContextVar object at 0x789730f604c0>, 'SPLIT_REDUCEOP': <tinygrad.helpers.ContextVar object at 0x789730f60640>, 'ARANGE_DIFF': <tinygrad.helpers.ContextVar object at 0x789730f60850>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, '_METADATA': <ContextVar name='_METADATA' default=None at 0x789730f470b0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'Timing': <class 'tinygrad.helpers.Timing'>, '_format_fcn': <function _format_fcn at 0x789730f59360>, 'Profiling': <class 'tinygrad.helpers.Profiling'>}    
        CACHEDB: str = getenv("CACHEDB", os.path.abspath(os.path.join(_cache_dir, "tinygrad", "cache.db")))                                                                                              #     helpers.py                    :   207: G: {'_cache_dir': '/home/lorinbaum/.cache'}    
        CACHELEVEL = getenv("CACHELEVEL", 2)                                                                                                                                                             #     helpers.py                    :   208: G: {'CACHEDB': '/home/lorinbaum/.cache/tinygrad/cache.db'}    
        
        VERSION = 16                                                                                                                                                                                     #     helpers.py                    :   210: G: {'CACHELEVEL': 2}    
        _db_connection = None                                                                                                                                                                            #     helpers.py                    :   211: G: {'VERSION': 16}    
        
        _db_tables = set()                                                                                                                                                                               #     helpers.py                    :   240: G: {'_db_connection': None, 'db_connection': <function db_connection at 0x789730f59e10>, 'diskcache_clear': <function diskcache_clear at 0x789730f5a290>, 'diskcache_get': <function diskcache_get at 0x789730f5a320>}    

      
      ConstType = Union[float, int, bool]                                                                                                                                                                #     dtype.py                      :     6: G: {'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>}    

        @dataclass(frozen=True, order=True)                                                                                                                                                              #     dtype.py                      :     9: 
        class DType:                                                                                                                                                                                          
          priority: int  # this determines when things get upcasted                                                                                                                                      #     dtype.py                      :    10: G: {'ConstType': typing.Union[float, int, bool]}    L: {'__module__': 'tinygrad.dtype', '__qualname__': 'DType', '__annotations__': {}}    
          itemsize: int                                                                                                                                                                                  #     dtype.py                      :    11: 
          name: str                                                                                                                                                                                      #     dtype.py                      :    12: 
          fmt: Optional[str]                                                                                                                                                                             #     dtype.py                      :    13: 
          count: int                                                                                                                                                                                     #     dtype.py                      :    14: 

        # dependent typing?                                                                                                                                                                              #     dtype.py                      :    23: 
        @dataclass(frozen=True, repr=False)                                                                                                                                                                   
        class ImageDType(DType):                                                                                                                                                                              
          shape: Tuple[int, ...]   # arbitrary arg for the dtype, used in image for the shape                                                                                                            #     dtype.py                      :    24: G: {'DType': <class 'tinygrad.dtype.DType'>}    L: {'__qualname__': 'ImageDType', '__annotations__': {}}    
          base: DType                                                                                                                                                                                    #     dtype.py                      :    25: 

        class dtypes:                                                                                                                                                                                    #     dtype.py                      :    38: 
          # TODO: priority should be higher than bool
          pyint: Final[DType] = DType(-1, 8, "pyint", None, 1)   # arbitrary precision integer, same itemsize to int64 so min/max works                                                                  #     dtype.py                      :    73: G: {'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>}    L: {'__qualname__': 'dtypes', '__annotations__': {}, 'is_float': <staticmethod(<functools._lru_cache_wrapper object at 0x789730f52350>)>, 'is_int': <staticmethod(<functools._lru_cache_wrapper object at 0x789730f52400>)>, 'is_unsigned': <staticmethod(<functools._lru_cache_wrapper object at 0x789730f524b0>)>, 'from_py': <staticmethod(<function dtypes.from_py at 0x789730f5bd90>)>, 'as_const': <staticmethod(<function dtypes.as_const at 0x789730f701f0>)>, 'min': <staticmethod(<function dtypes.min at 0x789730f70280>)>, 'max': <staticmethod(<function dtypes.max at 0x789730f70310>)>, 'finfo': <staticmethod(<function dtypes.finfo at 0x789730f703a0>)>, 'fields': <staticmethod(<function dtypes.fields at 0x789730f70430>)>}    
          bool: Final[DType] = DType(0, 1, "bool", '?', 1)                                                                                                                                               #     dtype.py                      :    74: 
          int8: Final[DType] = DType(1, 1, "char", 'b', 1)                                                                                                                                               #     dtype.py                      :    75: 
          uint8: Final[DType] = DType(2, 1, "unsigned char", 'B', 1)                                                                                                                                     #     dtype.py                      :    76: 
          int16: Final[DType] = DType(3, 2, "short", 'h', 1)                                                                                                                                             #     dtype.py                      :    77: 
          uint16: Final[DType] = DType(4, 2, "unsigned short", 'H', 1)                                                                                                                                   #     dtype.py                      :    78: 
          int32: Final[DType] = DType(5, 4, "int", 'i', 1)                                                                                                                                               #     dtype.py                      :    79: 
          uint32: Final[DType] = DType(6, 4, "unsigned int", 'I', 1)                                                                                                                                     #     dtype.py                      :    80: 
          int64: Final[DType] = DType(7, 8, "long", 'l', 1)                                                                                                                                              #     dtype.py                      :    81: 
          uint64: Final[DType] = DType(8, 8, "unsigned long", 'L', 1)                                                                                                                                    #     dtype.py                      :    82: 
          float16: Final[DType] = DType(9, 2, "half", 'e', 1)                                                                                                                                            #     dtype.py                      :    83: 
          # bfloat16 has higher priority than float16, so least_upper_dtype(dtypes.int64, dtypes.uint64) = dtypes.float16
          bfloat16: Final[DType] = DType(10, 2, "__bf16", None, 1)                                                                                                                                       #     dtype.py                      :    85: 
          float32: Final[DType] = DType(11, 4, "float", 'f', 1)                                                                                                                                          #     dtype.py                      :    86: 
          float64: Final[DType] = DType(12, 8, "double", 'd', 1)                                                                                                                                         #     dtype.py                      :    87: 
        
          # dtype aliases
          half = float16; float = float32; double = float64 # noqa: E702                                                                                                                                 #     dtype.py                      :    90: 
          uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 # noqa: E702                                                                                                                     #     dtype.py                      :    91: 
          char = int8; short = int16; int = int32; long = int64 # noqa: E702                                                                                                                             #     dtype.py                      :    92: 
        
          default_float: ClassVar[DType] = float32                                                                                                                                                       #     dtype.py                      :   100: L: {'imageh': <staticmethod(<function dtypes.imageh at 0x789730f705e0>)>, 'imagef': <staticmethod(<function dtypes.imagef at 0x789730f70550>)>}    
          default_int: ClassVar[DType] = int32                                                                                                                                                           #     dtype.py                      :   101: 

      
      if (env_default_float := getenv("DEFAULT_FLOAT", "")):                                                                                                                                             #     dtype.py                      :   103: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>}    L: {'__name__': 'tinygrad.dtype', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78973278de40>, '__spec__': ModuleSpec(name='tinygrad.dtype', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78973278de40>, origin='/home/lorinbaum/code/tinygrad/tinygrad/dtype.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/dtype.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/dtype.cpython-310.pyc', 'Final': typing.Final, 'Optional': typing.Optional, 'ClassVar': typing.ClassVar, 'Set': typing.Set, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Union': typing.Union, 'dataclass': <function dataclass at 0x789735c341f0>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'ConstType': typing.Union[float, int, bool], 'DType': <class 'tinygrad.dtype.DType'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>}    
      
      DTypeLike = Union[str, DType]                                                                                                                                                                      #     dtype.py                      :   107: G: {'env_default_float': ''}    
      
      # https://jax.readthedocs.io/en/latest/jep/9407-type-promotion.html
      # we don't support weak type and complex type
      promo_lattice = { dtypes.bool: [dtypes.int8, dtypes.uint8], dtypes.int8: [dtypes.int16], dtypes.int16: [dtypes.int32], dtypes.int32: [dtypes.int64],                                               #     dtype.py                      :   112: G: {'DTypeLike': typing.Union[str, tinygrad.dtype.DType], 'to_dtype': <function to_dtype at 0x7897327a63b0>}    
        dtypes.int64: [dtypes.float16, dtypes.bfloat16], dtypes.uint8: [dtypes.int16, dtypes.uint16], dtypes.uint16: [dtypes.int32, dtypes.uint32],                                                           
        dtypes.uint32: [dtypes.int64, dtypes.uint64], dtypes.uint64: [dtypes.float16, dtypes.bfloat16],                                                                                                       
        dtypes.float16: [dtypes.float32], dtypes.bfloat16: [dtypes.float32], dtypes.float32: [dtypes.float64], }                                                                                              
      
      # HACK: staticmethods are not callable in 3.8 so we have to compare the class
      DTYPES_DICT = {k: v for k, v in dtypes.__dict__.items() if not (k.startswith(('__', 'default', 'pyint')) or v.__class__ is staticmethod)}                                                          #     dtype.py                      :   126: G: {'_get_recursive_parents': <functools._lru_cache_wrapper object at 0x789730f52da0>, 'least_upper_dtype': <functools._lru_cache_wrapper object at 0x789730f52e50>, 'least_upper_float': <function least_upper_float at 0x789730f704c0>}    
      INVERSE_DTYPES_DICT = {v.name:k for k,v in DTYPES_DICT.items()}                                                                                                                                    #     dtype.py                      :   127: 
      INVERSE_DTYPES_DICT['pyint'] = 'pyint'                                                                                                                                                             #     dtype.py                      :   128: G: {'promo_lattice': {dtypes.bool: [dtypes.char, dtypes.uchar], dtypes.char: [dtypes.short], dtypes.short: [dtypes.int], dtypes.int: [dtypes.long], dtypes.long: [dtypes.half, dtypes.bfloat16], dtypes.uchar: [dtypes.short, dtypes.ushort], dtypes.ushort: [dtypes.int, dtypes.uint], dtypes.uint: [dtypes.long, dtypes.ulong], dtypes.ulong: [dtypes.half, dtypes.bfloat16], dtypes.half: [dtypes.float], dtypes.bfloat16: [dtypes.float], dtypes.float: [dtypes.double]}, 'DTYPES_DICT': {'bool': dtypes.bool, 'int8': dtypes.char, 'uint8': dtypes.uchar, 'int16': dtypes.short, 'uint16': dtypes.ushort, 'int32': dtypes.int, 'uint32': dtypes.uint, 'int64': dtypes.long, 'uint64': dtypes.ulong, 'float16': dtypes.half, 'bfloat16': dtypes.bfloat16, 'float32': dtypes.float, 'float64': dtypes.double, 'half': dtypes.half, 'float': dtypes.float, 'double': dtypes.double, 'uchar': dtypes.uchar, 'ushort': dtypes.ushort, 'uint': dtypes.uint, 'ulong': dtypes.ulong, 'char': dtypes.char, 'short': dtypes.short, 'int': dtypes.int, 'long': dtypes.long}, 'INVERSE_DTYPES_DICT': {'bool': 'bool', 'char': 'char', 'unsigned char': 'uchar', 'short': 'short', 'unsigned short': 'ushort', 'int': 'int', 'unsigned int': 'uint', 'long': 'long', 'unsigned long': 'ulong', 'half': 'half', '__bf16': 'bfloat16', 'float': 'float', 'double': 'double'}}    

    from tinygrad.helpers import argfix, make_pair, flatten, prod, all_int, round_up, merge_dicts, argsort, getenv, get_shape, fully_flatten, dedup                                                      #     tensor.py                     :    11: G: {'DType': <class 'tinygrad.dtype.DType'>, 'DTypeLike': typing.Union[str, tinygrad.dtype.DType], 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'ConstType': typing.Union[float, int, bool], 'least_upper_float': <function least_upper_float at 0x789730f704c0>, 'least_upper_dtype': <functools._lru_cache_wrapper object at 0x789730f52e50>, 'sum_acc_dtype': <function sum_acc_dtype at 0x789730f70940>, 'to_dtype': <function to_dtype at 0x7897327a63b0>}    
    from tinygrad.helpers import IMAGE, DEBUG, WINO, THREEFRY, _METADATA, Metadata, TRACEMETA                                                                                                            #     tensor.py                     :    12: G: {'argfix': <function argfix at 0x789730f58310>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'prod': <function prod at 0x7897327a7520>, 'all_int': <function all_int at 0x789730f584c0>, 'round_up': <function round_up at 0x789730f58af0>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'argsort': <function argsort at 0x789730f583a0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'get_shape': <function get_shape at 0x789730f58f70>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'dedup': <function dedup at 0x789730f581f0>}    
    from tinygrad.lazy import LazyBuffer                                                                                                                                                                 #     tensor.py                     :    13: G: {'IMAGE': <tinygrad.helpers.ContextVar object at 0x789730f3bdf0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'WINO': <tinygrad.helpers.ContextVar object at 0x789730f600a0>, 'THREEFRY': <tinygrad.helpers.ContextVar object at 0x789730f602e0>, '_METADATA': <ContextVar name='_METADATA' default=None at 0x789730f470b0>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>}    
      from __future__ import annotations                                                                                                                                                                 #     lazy.py                       :     1: G: {'__name__': 'tinygrad.lazy', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63070>, '__spec__': ModuleSpec(name='tinygrad.lazy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63070>, origin='/home/lorinbaum/code/tinygrad/tinygrad/lazy.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/lazy.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/lazy.cpython-310.pyc'}    
      from typing import Union, Optional, Any, Tuple, List, get_args                                                                                                                                     #     lazy.py                       :     2: G: {'__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
      from tinygrad.dtype import dtypes, DType, DTypeLike, ConstType, to_dtype                                                                                                                           #     lazy.py                       :     3: G: {'Union': typing.Union, 'Optional': typing.Optional, 'Any': typing.Any, 'Tuple': typing.Tuple, 'List': typing.List, 'get_args': <function get_args at 0x789735eb1ab0>}    
      from tinygrad.helpers import prod, getenv, all_int, all_same, DEBUG, _METADATA, Metadata, SPLIT_REDUCEOP                                                                                           #     lazy.py                       :     4: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'DTypeLike': typing.Union[str, tinygrad.dtype.DType], 'ConstType': typing.Union[float, int, bool], 'to_dtype': <function to_dtype at 0x7897327a63b0>}    
      from tinygrad.ops import MetaOps, UnaryOps, BinaryOps, TernaryOps, ReduceOps, Op, exec_alu, python_alu                                                                                             #     lazy.py                       :     5: G: {'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_int': <function all_int at 0x789730f584c0>, 'all_same': <function all_same at 0x789730f58430>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, '_METADATA': <ContextVar name='_METADATA' default=None at 0x789730f470b0>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'SPLIT_REDUCEOP': <tinygrad.helpers.ContextVar object at 0x789730f60640>}    
        from __future__ import annotations                                                                                                                                                               #     ops.py                        :     1: G: {'__name__': 'tinygrad.ops', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc'}    
        from typing import Any, DefaultDict, List, Optional, Set, Union, Tuple, Dict, Callable, cast, TYPE_CHECKING, Sequence                                                                            #     ops.py                        :     2: G: {'__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
        import sys, time, math, operator, ctypes, struct, functools, hashlib, itertools                                                                                                                  #     ops.py                        :     3: G: {'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'List': typing.List, 'Optional': typing.Optional, 'Set': typing.Set, 'Union': typing.Union, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Sequence': typing.Sequence}    
        from collections import defaultdict                                                                                                                                                              #     ops.py                        :     4: G: {'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'itertools': <module 'itertools' (built-in)>}    
        from enum import Enum, auto                                                                                                                                                                      #     ops.py                        :     5: G: {'defaultdict': <class 'collections.defaultdict'>}    
        from dataclasses import dataclass, field                                                                                                                                                         #     ops.py                        :     6: G: {'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>}    
        from tinygrad.dtype import ConstType, ImageDType, PtrDType, dtypes, DType                                                                                                                        #     ops.py                        :     7: G: {'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>}    
        from tinygrad.helpers import pretty_print, prod, getenv, all_same                                                                                                                                #     ops.py                        :     8: G: {'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>}    
        from tinygrad.shape.symbolic import Variable, sint                                                                                                                                               #     ops.py                        :     9: G: {'pretty_print': <function pretty_print at 0x789730f5ae60>, 'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_same': <function all_same at 0x789730f58430>}    
          from __future__ import annotations                                                                                                                                                             #     shape/symbolic.py             :     1: G: {'__name__': 'tinygrad.shape.symbolic', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f62650>, '__spec__': ModuleSpec(name='tinygrad.shape.symbolic', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f62650>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/symbolic.cpython-310.pyc'}    
          import functools                                                                                                                                                                               #     shape/symbolic.py             :     2: G: {'__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
          from math import gcd                                                                                                                                                                           #     shape/symbolic.py             :     3: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
          from tinygrad.helpers import partition                                                                                                                                                         #     shape/symbolic.py             :     4: G: {'gcd': <built-in function gcd>}    
          from typing import List, Dict, Callable, Tuple, Type, Union, Optional, Any, Set, Mapping                                                                                                       #     shape/symbolic.py             :     5: G: {'partition': <function partition at 0x789730f58d30>}    

            class Node:                                                                                                                                                                                  #     shape/symbolic.py             :    10: 
              b: Union[Node, int]                                                                                                                                                                        #     shape/symbolic.py             :    11: G: {'List': typing.List, 'Dict': typing.Dict, 'Callable': typing.Callable, 'Tuple': typing.Tuple, 'Type': typing.Type, 'Union': typing.Union, 'Optional': typing.Optional, 'Any': typing.Any, 'Set': typing.Set, 'Mapping': typing.Mapping}    L: {'__module__': 'tinygrad.shape.symbolic', '__qualname__': 'Node', '__annotations__': {}}    
              min: int                                                                                                                                                                                   #     shape/symbolic.py             :    12: 
              max: sint                                                                                                                                                                                  #     shape/symbolic.py             :    13: 

          
          # symbolic int, these are allowed in a Tensor shape
          sint = Union[int, Variable, MulNode, SumNode]                                                                                                                                                  #     shape/symbolic.py             :   304: G: {'Node': <class 'tinygrad.shape.symbolic.Node'>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>, 'create_node': <function create_node at 0x789730fe4550>, 'create_lt_node': <function create_lt_node at 0x789730fe5d80>, 'create_ge_node': <function create_ge_node at 0x789730fe5e10>, 'OpNode': <class 'tinygrad.shape.symbolic.OpNode'>, 'LtNode': <class 'tinygrad.shape.symbolic.LtNode'>, 'MulNode': <class 'tinygrad.shape.symbolic.MulNode'>, 'DivNode': <class 'tinygrad.shape.symbolic.DivNode'>, 'ModNode': <class 'tinygrad.shape.symbolic.ModNode'>, 'RedNode': <class 'tinygrad.shape.symbolic.RedNode'>, 'SumNode': <class 'tinygrad.shape.symbolic.SumNode'>, 'AndNode': <class 'tinygrad.shape.symbolic.AndNode'>, 'sym_render': <function sym_render at 0x789730fe5ea0>, 'sym_infer': <function sym_infer at 0x789730fe70a0>}    L: {'__name__': 'tinygrad.shape.symbolic', '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f62650>, '__spec__': ModuleSpec(name='tinygrad.shape.symbolic', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f62650>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/symbolic.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'gcd': <built-in function gcd>, 'partition': <function partition at 0x789730f58d30>, 'List': typing.List, 'Dict': typing.Dict, 'Callable': typing.Callable, 'Tuple': typing.Tuple, 'Type': typing.Type, 'Union': typing.Union, 'Optional': typing.Optional, 'Any': typing.Any, 'Set': typing.Set, 'Mapping': typing.Mapping}    
          
          render_python: Dict[Type, Callable[..., str]] = {                                                                                                                                              #     shape/symbolic.py             :   312: G: {'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'render_mulnode': <function render_mulnode at 0x789730fe7130>}    
            Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \                                                      
              else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \                                                             
              else f"{self.expr}"),                                                                                                                                                                           
            NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",                                                                                                             
            MulNode: render_mulnode,                                                                                                                                                                          
            DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",                                                                                                                            
            ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",                                                                                                                             
            LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",                                                                                                          
            SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",                                                                                                   
            AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",                                                                                               
          }                                                                                                                                                                                                   

        if TYPE_CHECKING:                                                                                                                                                                                #     ops.py                        :    10: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    

          # the Enum class doesn't work with mypy, this is static. sorry it's ugly                                                                                                                       #     ops.py                        :    16: 
          # NOTE: MOD, CMPLT don't have to be implemented on vectors, just scalars                                                                                                                            
          # NOTE: many GPUs don't have DIV, but UnaryOps.RECIP doesn't work for integer division                                                                                                              
          class UnaryOps(Enum):                                                                                                                                                                               
            """A -> A (elementwise)"""                                                                                                                                                                        
            """A -> A (elementwise)"""                                                                                                                                                                   #     ops.py                        :    17: L: {'_generate_next_value_': <function Enum._generate_next_value_ at 0x789735e41870>, '__module__': 'tinygrad.ops', '__qualname__': 'UnaryOps'}    
            EXP2 = auto(); LOG2 = auto(); CAST = auto(); BITCAST = auto(); SIN = auto(); SQRT = auto(); RECIP = auto() # noqa: E702                                                                      #     ops.py                        :    18: L: {'__doc__': 'A -> A (elementwise)'}    

          class BinaryOps(Enum):                                                                                                                                                                         #     ops.py                        :    19: 
            """A + A -> A (elementwise)"""                                                                                                                                                                    
            """A + A -> A (elementwise)"""                                                                                                                                                               #     ops.py                        :    20: G: {'UnaryOps': <enum 'UnaryOps'>}    L: {'__qualname__': 'BinaryOps'}    
            ADD = auto(); MUL = auto(); IDIV = auto(); MAX = auto(); MOD = auto(); CMPLT = auto(); CMPNE = auto(); XOR = auto() # noqa: E702                                                             #     ops.py                        :    21: L: {'__doc__': 'A + A -> A (elementwise)'}    
            SHL = auto(); SHR = auto(); OR = auto(); AND = auto(); THREEFRY = auto() # noqa: E702                                                                                                        #     ops.py                        :    22: L: {'ADD': 1, 'MUL': 2, 'IDIV': 3, 'MAX': 4, 'MOD': 5, 'CMPLT': 6, 'CMPNE': 7, 'XOR': 8}    

          class TernaryOps(Enum):                                                                                                                                                                        #     ops.py                        :    23: 
            """A + A + A -> A (elementwise)"""                                                                                                                                                                
            """A + A + A -> A (elementwise)"""                                                                                                                                                           #     ops.py                        :    24: G: {'BinaryOps': <enum 'BinaryOps'>}    L: {'__qualname__': 'TernaryOps'}    
            WHERE = auto(); MULACC = auto() # noqa: E702                                                                                                                                                 #     ops.py                        :    25: L: {'__doc__': 'A + A + A -> A (elementwise)'}    

          class ReduceOps(Enum):                                                                                                                                                                         #     ops.py                        :    26: 
            """A -> B (reduce)"""                                                                                                                                                                             
            """A -> B (reduce)"""                                                                                                                                                                        #     ops.py                        :    27: G: {'TernaryOps': <enum 'TernaryOps'>}    L: {'__qualname__': 'ReduceOps'}    
            SUM = auto(); MAX = auto(); WMMA = auto() # noqa: E702                                                                                                                                       #     ops.py                        :    28: L: {'__doc__': 'A -> B (reduce)'}    

          class MetaOps(Enum):                                                                                                                                                                           #     ops.py                        :    29: 
            EMPTY = auto(); CONST = auto(); COPY = auto(); CONTIGUOUS = auto(); CUSTOM = auto(); ASSIGN = auto(); VIEW = auto() # noqa: E702                                                             #     ops.py                        :    30: G: {'ReduceOps': <enum 'ReduceOps'>}    L: {'__qualname__': 'MetaOps'}    

        Op = Union[UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps]                                                                                                                                  #     ops.py                        :    31: G: {'MetaOps': <enum 'MetaOps'>}    L: {'__name__': 'tinygrad.ops', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'List': typing.List, 'Optional': typing.Optional, 'Set': typing.Set, 'Union': typing.Union, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Sequence': typing.Sequence, 'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'itertools': <module 'itertools' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>, 'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'pretty_print': <function pretty_print at 0x789730f5ae60>, 'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_same': <function all_same at 0x789730f58430>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>}    
        
        # do not preserve f(0) = 0
        UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}                                                                                                                  #     ops.py                        :    34: G: {'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps]}    

          # the order of these UOps controls the order of the toposort                                                                                                                                   #     ops.py                        :    37: 
          class UOps(Enum):                                                                                                                                                                                   
            # ops that aren't rendered
            SINK = auto(); EXT = auto(); EXPAND = auto(); CONTRACT = auto(); SHAPETRACKER = auto()  # noqa: E702                                                                                         #     ops.py                        :    39: G: {'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}}    L: {'_generate_next_value_': <function Enum._generate_next_value_ at 0x789735e41870>, '__module__': 'tinygrad.ops', '__qualname__': 'UOps'}    
            DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702                                                                                         #     ops.py                        :    40: L: {'SINK': 1, 'EXT': 2, 'EXPAND': 3, 'CONTRACT': 4, 'SHAPETRACKER': 5}    
            CONST = auto(); SPECIAL = auto() # noqa: E702                                                                                                                                                #     ops.py                        :    41: L: {'DEFINE_GLOBAL': 6, 'DEFINE_VAR': 7, 'DEFINE_LOCAL': 8, 'DEFINE_ACC': 9}    
            NOOP = auto(); GEP = auto() # noqa: E702                                                                                                                                                     #     ops.py                        :    42: L: {'CONST': 10, 'SPECIAL': 11}    
            # math ops
            CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702                                                                                                                             #     ops.py                        :    44: L: {'NOOP': 12, 'GEP': 13}    
            ALU = auto(); REDUCE = auto(); REDUCE_AXIS = auto(); WMMA = auto() # noqa: E702                                                                                                              #     ops.py                        :    45: L: {'CAST': 14, 'BITCAST': 15, 'VECTORIZE': 16}    
            # memory/assignment ops
            LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702                                                                                                                                     #     ops.py                        :    47: L: {'ALU': 17, 'REDUCE': 18, 'REDUCE_AXIS': 19, 'WMMA': 20}    
            # control flow ops
            BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702                                                                                                                                   #     ops.py                        :    49: L: {'LOAD': 21, 'STORE': 22, 'PHI': 23}    
            # these two are not graph nodes
            ENDRANGE = auto(); ENDIF = auto() # noqa: E702                                                                                                                                               #     ops.py                        :    51: L: {'BARRIER': 24, 'IF': 25, 'RANGE': 26}    

        
        BUFFER_UOPS = {UOps.LOAD, UOps.STORE, UOps.CONST}                                                                                                                                                #     ops.py                        :    53: G: {'UOps': <enum 'UOps'>}    L: {'__name__': 'tinygrad.ops', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'List': typing.List, 'Optional': typing.Optional, 'Set': typing.Set, 'Union': typing.Union, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Sequence': typing.Sequence, 'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'itertools': <module 'itertools' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>, 'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'pretty_print': <function pretty_print at 0x789730f5ae60>, 'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_same': <function all_same at 0x789730f58430>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'MetaOps': <enum 'MetaOps'>, 'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps], 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}}    
        
        END_FOR_UOP = {UOps.IF:(UOps.STORE, UOps.ENDIF), UOps.RANGE:(UOps.PHI, UOps.ENDRANGE)}                                                                                                           #     ops.py                        :    55: G: {'BUFFER_UOPS': {<UOps.CONST: 10>, <UOps.LOAD: 21>, <UOps.STORE: 22>}}    

          @dataclass(frozen=True, eq=False)                                                                                                                                                              #     ops.py                        :    58: 
          class UOp:                                                                                                                                                                                          
            op: UOps                                                                                                                                                                                     #     ops.py                        :    59: G: {'END_FOR_UOP': {<UOps.IF: 25>: (<UOps.STORE: 22>, <UOps.ENDIF: 28>), <UOps.RANGE: 26>: (<UOps.PHI: 23>, <UOps.ENDRANGE: 27>)}}    L: {'__module__': 'tinygrad.ops', '__qualname__': 'UOp', '__annotations__': {}}    
            dtype: Optional[DType] = None                                                                                                                                                                #     ops.py                        :    60: 
            src: Tuple[UOp, ...] = tuple()                                                                                                                                                               #     ops.py                        :    61: 
            arg: Any = None                                                                                                                                                                              #     ops.py                        :    62: L: {'src': ()}    

          @dataclass(frozen=True)                                                                                                                                                                        #     ops.py                        :   187: 
          class KernelInfo:                                                                                                                                                                                   
            local_dims: int = 0           # number of local dimensions  (this is remapping RANGE to SPECIAL)                                                                                             #     ops.py                        :   188: G: {'UOp': <class 'tinygrad.ops.UOp'>}    L: {'__qualname__': 'KernelInfo', '__annotations__': {}}    
            upcasted: int = 0             # count that are upcasted     (this is remapping RANGE to EXPAND)                                                                                              #     ops.py                        :   189: L: {'local_dims': 0}    
            dont_use_locals: bool = False # don't use local indexing                                                                                                                                     #     ops.py                        :   190: L: {'upcasted': 0}    

          @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                     #     ops.py                        :   203: 
          class NOp(UOp):                                                                                                                                                                                     
            name: Optional[str] = None                                                                                                                                                                   #     ops.py                        :   204: G: {'KernelInfo': <class 'tinygrad.ops.KernelInfo'>, 'get_location': <function get_location at 0x789730f72ef0>, 'lines': <functools._lru_cache_wrapper object at 0x789730ff54e0>}    L: {'__qualname__': 'NOp', '__annotations__': {}}    
            src: Tuple[NOp, ...] = tuple()                                                                                                                                                               #     ops.py                        :   205: 
            allow_any_len: bool = False                                                                                                                                                                  #     ops.py                        :   206: L: {'src': ()}    
            location: Tuple[str, int] = field(default_factory=get_location)                                                                                                                              #     ops.py                        :   207: L: {'allow_any_len': False}    

        
        # *** tracking pattern matcher ***
        
        TRACK_MATCH_STATS = getenv("TRACK_MATCH_STATS", 0)                                                                                                                                               #     ops.py                        :   288: G: {'NOp': <class 'tinygrad.ops.NOp'>, 'UPat': <class 'tinygrad.ops.UPat'>, '_match': <function _match at 0x789730ff9870>, 'PatternMatcher': <class 'tinygrad.ops.PatternMatcher'>}    L: {'__name__': 'tinygrad.ops', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc', '__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'List': typing.List, 'Optional': typing.Optional, 'Set': typing.Set, 'Union': typing.Union, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Sequence': typing.Sequence, 'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'itertools': <module 'itertools' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>, 'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'pretty_print': <function pretty_print at 0x789730f5ae60>, 'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_same': <function all_same at 0x789730f58430>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'MetaOps': <enum 'MetaOps'>, 'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps], 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}, 'UOps': <enum 'UOps'>, 'BUFFER_UOPS': {<UOps.CONST: 10>, <UOps.LOAD: 21>, <UOps.STORE: 22>}, 'END_FOR_UOP': {<UOps.IF: 25>: (<UOps.STORE: 22>, <UOps.ENDIF: 28>), <UOps.RANGE: 26>: (<UOps.PHI: 23>, <UOps.ENDRANGE: 27>)}, 'UOp': <class 'tinygrad.ops.UOp'>, 'KernelInfo': <class 'tinygrad.ops.KernelInfo'>, 'get_location': <function get_location at 0x789730f72ef0>, 'lines': <functools._lru_cache_wrapper object at 0x789730ff54e0>}    
        match_stats:Dict[UPat, List[Union[int, float]]] = dict()                                                                                                                                         #     ops.py                        :   289: G: {'TRACK_MATCH_STATS': 0}    
        
        if TRACK_MATCH_STATS:                                                                                                                                                                            #     ops.py                        :   313: G: {'match_stats': {}, 'TrackedPattenMatcher': <class 'tinygrad.ops.TrackedPattenMatcher'>}    
        
        python_alu: Dict[Op, Callable]  = {                                                                                                                                                              #     ops.py                        :   347: G: {'graph_rewrite': <function graph_rewrite at 0x789730ffa200>, 'hook_overflow': <function hook_overflow at 0x789730ffa680>}    
          UnaryOps.LOG2: lambda x: math.log2(x) if x > 0 else -math.inf if x == 0 else math.nan, UnaryOps.EXP2: hook_overflow(math.inf, lambda x: 2**x),                                                      
          UnaryOps.SQRT: lambda x: math.sqrt(x) if x >= 0 else math.nan, UnaryOps.RECIP: lambda x: 1/x if x != 0 else math.copysign(math.inf, x),                                                             
          UnaryOps.SIN: lambda x: math.sin(x) if not math.isinf(x) else math.nan,                                                                                                                             
          BinaryOps.SHR: operator.rshift, BinaryOps.SHL: operator.lshift, BinaryOps.MUL: operator.mul, BinaryOps.ADD: operator.add,                                                                           
          BinaryOps.XOR: operator.xor, BinaryOps.MAX: max, BinaryOps.CMPNE: operator.ne, BinaryOps.CMPLT: operator.lt,                                                                                        
          BinaryOps.OR: operator.or_, BinaryOps.AND: operator.and_,                                                                                                                                           
          BinaryOps.MOD: lambda x,y: abs(int(x))%abs(int(y))*(1,-1)[x<0], BinaryOps.IDIV: lambda x,y: abs(x)//abs(y)*(1,-1)[x*y<0] if y != 0 else x*math.inf,                                                 
          TernaryOps.MULACC: lambda x,y,z: (x*y)+z, TernaryOps.WHERE: lambda x,y,z: y if x else z}                                                                                                            

          def hook_overflow(dv, fxn):                                                                                                                                                                    #     ops.py                        :   341: 
            return wfxn                                                                                                                                                                                  #     ops.py                        :   345: L: {'dv': inf, 'fxn': <function <lambda> at 0x789730ffa7a0>, 'wfxn': <function hook_overflow.<locals>.wfxn at 0x789730ffa830>}    

        
        truncate: Dict[DType, Callable] = {dtypes.bool: bool,                                                                                                                                            #     ops.py                        :   364: G: {'python_alu': {<UnaryOps.LOG2: 2>: <function <lambda> at 0x789730ffa710>, <UnaryOps.EXP2: 1>: <function hook_overflow.<locals>.wfxn at 0x789730ffa830>, <UnaryOps.SQRT: 6>: <function <lambda> at 0x789730ffa8c0>, <UnaryOps.RECIP: 7>: <function <lambda> at 0x789730ffa950>, <UnaryOps.SIN: 5>: <function <lambda> at 0x789730ffa9e0>, <BinaryOps.SHR: 10>: <built-in function rshift>, <BinaryOps.SHL: 9>: <built-in function lshift>, <BinaryOps.MUL: 2>: <built-in function mul>, <BinaryOps.ADD: 1>: <built-in function add>, <BinaryOps.XOR: 8>: <built-in function xor>, <BinaryOps.MAX: 4>: <built-in function max>, <BinaryOps.CMPNE: 7>: <built-in function ne>, <BinaryOps.CMPLT: 6>: <built-in function lt>, <BinaryOps.OR: 11>: <built-in function or_>, <BinaryOps.AND: 12>: <built-in function and_>, <BinaryOps.MOD: 5>: <function <lambda> at 0x789730ffaa70>, <BinaryOps.IDIV: 3>: <function <lambda> at 0x789730ffab00>, <TernaryOps.MULACC: 2>: <function <lambda> at 0x789730ffab90>, <TernaryOps.WHERE: 1>: <function <lambda> at 0x789730ffac20>}, 'truncate_fp16': <function truncate_fp16 at 0x789730ffacb0>}    L: {'__name__': 'tinygrad.ops', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63c10>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc', '__annotations__': {'match_stats': 'Dict[UPat, List[Union[int, float]]]', 'python_alu': 'Dict[Op, Callable]'}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'List': typing.List, 'Optional': typing.Optional, 'Set': typing.Set, 'Union': typing.Union, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Sequence': typing.Sequence, 'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'hashlib': <module 'hashlib' from '/usr/lib/python3.10/hashlib.py'>, 'itertools': <module 'itertools' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>, 'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'pretty_print': <function pretty_print at 0x789730f5ae60>, 'prod': <function prod at 0x7897327a7520>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_same': <function all_same at 0x789730f58430>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'MetaOps': <enum 'MetaOps'>, 'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps], 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}, 'UOps': <enum 'UOps'>, 'BUFFER_UOPS': {<UOps.CONST: 10>, <UOps.LOAD: 21>, <UOps.STORE: 22>}, 'END_FOR_UOP': {<UOps.IF: 25>: (<UOps.STORE: 22>, <UOps.ENDIF: 28>), <UOps.RANGE: 26>: (<UOps.PHI: 23>, <UOps.ENDRANGE: 27>)}, 'UOp': <class 'tinygrad.ops.UOp'>, 'KernelInfo': <class 'tinygrad.ops.KernelInfo'>, 'get_location': <function get_location at 0x789730f72ef0>, 'lines': <functools._lru_cache_wrapper object at 0x789730ff54e0>, 'NOp': <class 'tinygrad.ops.NOp'>, 'UPat': <class 'tinygrad.ops.UPat'>, '_match': <function _match at 0x789730ff9870>, 'PatternMatcher': <class 'tinygrad.ops.PatternMatcher'>, 'TRACK_MATCH_STATS': 0, 'match_stats': {}, 'TrackedPattenMatcher': <class 'tinygrad.ops.TrackedPattenMatcher'>, 'graph_rewrite': <function graph_rewrite at 0x789730ffa200>, 'hook_overflow': <function hook_overflow at 0x789730ffa680>}    
          # TODO: bfloat16                                                                                                                                                                                    
          dtypes.float16: truncate_fp16, dtypes.float32: lambda x: ctypes.c_float(x).value, dtypes.float64: lambda x: ctypes.c_double(x).value,                                                               
          dtypes.uint8: lambda x: ctypes.c_uint8(x).value, dtypes.uint16: lambda x: ctypes.c_uint16(x).value,                                                                                                 
          dtypes.uint32: lambda x: ctypes.c_uint32(x).value, dtypes.uint64: lambda x: ctypes.c_uint64(x).value,                                                                                               
          dtypes.int8: lambda x: ctypes.c_int8(x).value, dtypes.int16: lambda x: ctypes.c_int16(x).value, dtypes.int32: lambda x: ctypes.c_int32(x).value \                                                   
              if isinstance(x,int) else x, dtypes.int64: lambda x: ctypes.c_int64(x).value}                                                                                                                   

      from tinygrad.shape.symbolic import sint, Variable                                                                                                                                                 #     lazy.py                       :     6: G: {'MetaOps': <enum 'MetaOps'>, 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps], 'exec_alu': <function exec_alu at 0x789730ffb2e0>, 'python_alu': {<UnaryOps.LOG2: 2>: <function <lambda> at 0x789730ffa710>, <UnaryOps.EXP2: 1>: <function hook_overflow.<locals>.wfxn at 0x789730ffa830>, <UnaryOps.SQRT: 6>: <function <lambda> at 0x789730ffa8c0>, <UnaryOps.RECIP: 7>: <function <lambda> at 0x789730ffa950>, <UnaryOps.SIN: 5>: <function <lambda> at 0x789730ffa9e0>, <BinaryOps.SHR: 10>: <built-in function rshift>, <BinaryOps.SHL: 9>: <built-in function lshift>, <BinaryOps.MUL: 2>: <built-in function mul>, <BinaryOps.ADD: 1>: <built-in function add>, <BinaryOps.XOR: 8>: <built-in function xor>, <BinaryOps.MAX: 4>: <built-in function max>, <BinaryOps.CMPNE: 7>: <built-in function ne>, <BinaryOps.CMPLT: 6>: <built-in function lt>, <BinaryOps.OR: 11>: <built-in function or_>, <BinaryOps.AND: 12>: <built-in function and_>, <BinaryOps.MOD: 5>: <function <lambda> at 0x789730ffaa70>, <BinaryOps.IDIV: 3>: <function <lambda> at 0x789730ffab00>, <TernaryOps.MULACC: 2>: <function <lambda> at 0x789730ffab90>, <TernaryOps.WHERE: 1>: <function <lambda> at 0x789730ffac20>}}    
      from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                               #     lazy.py                       :     7: G: {'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Variable': <class 'tinygrad.shape.symbolic.Variable'>}    
        # ShapeTracker allows movement operations to a buffer that don't require a copy to be made.
        from __future__ import annotations                                                                                                                                                               #     shape/shapetracker.py         :     2: G: {'__name__': 'tinygrad.shape.shapetracker', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730f63d60>, '__spec__': ModuleSpec(name='tinygrad.shape.shapetracker', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730f63d60>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/shapetracker.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/shapetracker.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/shapetracker.cpython-310.pyc'}    
        import functools                                                                                                                                                                                 #     shape/shapetracker.py         :     3: G: {'__annotations__': {}, 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
        from dataclasses import dataclass                                                                                                                                                                #     shape/shapetracker.py         :     4: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
        from typing import Tuple, List, Optional, Dict, Set, Iterable, cast, Any                                                                                                                         #     shape/shapetracker.py         :     5: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
        from tinygrad.helpers import merge_dicts, getenv                                                                                                                                                 #     shape/shapetracker.py         :     6: G: {'Tuple': typing.Tuple, 'List': typing.List, 'Optional': typing.Optional, 'Dict': typing.Dict, 'Set': typing.Set, 'Iterable': typing.Iterable, 'cast': <function cast at 0x789735eb17e0>, 'Any': typing.Any}    
        from tinygrad.shape.symbolic import Variable, MulNode, Node, SumNode, NumNode, DivNode, ModNode, LtNode, AndNode, sint                                                                           #     shape/shapetracker.py         :     7: G: {'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>}    
        from tinygrad.shape.view import View, strides_for_shape                                                                                                                                          #     shape/shapetracker.py         :     8: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'MulNode': <class 'tinygrad.shape.symbolic.MulNode'>, 'Node': <class 'tinygrad.shape.symbolic.Node'>, 'SumNode': <class 'tinygrad.shape.symbolic.SumNode'>, 'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>, 'DivNode': <class 'tinygrad.shape.symbolic.DivNode'>, 'ModNode': <class 'tinygrad.shape.symbolic.ModNode'>, 'LtNode': <class 'tinygrad.shape.symbolic.LtNode'>, 'AndNode': <class 'tinygrad.shape.symbolic.AndNode'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    
          from __future__ import annotations                                                                                                                                                             #     shape/view.py                 :     1: G: {'__name__': 'tinygrad.shape.view', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730fd5810>, '__spec__': ModuleSpec(name='tinygrad.shape.view', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730fd5810>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/view.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/view.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/view.cpython-310.pyc'}    
          import functools, operator, itertools, math                                                                                                                                                    #     shape/view.py                 :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
          from dataclasses import dataclass                                                                                                                                                              #     shape/view.py                 :     3: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>, 'itertools': <module 'itertools' (built-in)>, 'math': <module 'math' (built-in)>}    
          from typing import Tuple, List, Optional, Dict, Set, cast                                                                                                                                      #     shape/view.py                 :     4: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
          from tinygrad.helpers import prod, all_int, argsort                                                                                                                                            #     shape/view.py                 :     5: G: {'Tuple': typing.Tuple, 'List': typing.List, 'Optional': typing.Optional, 'Dict': typing.Dict, 'Set': typing.Set, 'cast': <function cast at 0x789735eb17e0>}    
          from tinygrad.shape.symbolic import Node, NumNode, Variable, sint, sym_infer, create_lt_node, create_ge_node                                                                                   #     shape/view.py                 :     6: G: {'prod': <function prod at 0x7897327a7520>, 'all_int': <function all_int at 0x789730f584c0>, 'argsort': <function argsort at 0x789730f583a0>}    

            @dataclass(frozen=True)                                                                                                                                                                      #     shape/view.py                 :    85: 
            class View:                                                                                                                                                                                       
              shape:Tuple[sint, ...]                                                                                                                                                                     #     shape/view.py                 :    86: G: {'Node': <class 'tinygrad.shape.symbolic.Node'>, 'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'sym_infer': <function sym_infer at 0x789730fe70a0>, 'create_lt_node': <function create_lt_node at 0x789730fe5d80>, 'create_ge_node': <function create_ge_node at 0x789730fe5e10>, 'canonicalize_strides': <functools._lru_cache_wrapper object at 0x7897305285c0>, 'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>, '_merge_dims': <functools._lru_cache_wrapper object at 0x789730528720>, '_reshape_mask': <functools._lru_cache_wrapper object at 0x789730528880>, 'un1d': <function un1d at 0x789730525990>}    L: {'__module__': 'tinygrad.shape.view', '__qualname__': 'View', '__annotations__': {}}    
              strides:Tuple[sint, ...]                                                                                                                                                                   #     shape/view.py                 :    87: 
              offset:sint                                                                                                                                                                                #     shape/view.py                 :    88: 
              mask:Optional[Tuple[Tuple[sint, sint], ...]]                                                                                                                                               #     shape/view.py                 :    89: 
              contiguous:bool                                                                                                                                                                            #     shape/view.py                 :    90: 

        from tinygrad.dtype import dtypes                                                                                                                                                                #     shape/shapetracker.py         :     9: G: {'View': <class 'tinygrad.shape.view.View'>, 'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>}    
        from tinygrad.ops import UOp, UOps, graph_rewrite                                                                                                                                                #     shape/shapetracker.py         :    10: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>}    
        from tinygrad.codegen.uopgraph import constant_folder                                                                                                                                            #     shape/shapetracker.py         :    11: G: {'UOp': <class 'tinygrad.ops.UOp'>, 'UOps': <enum 'UOps'>, 'graph_rewrite': <function graph_rewrite at 0x789730ffa200>}    
          from __future__ import annotations                                                                                                                                                             #     codegen/uopgraph.py           :     1: G: {'__name__': 'tinygrad.codegen.uopgraph', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730fd5030>, '__spec__': ModuleSpec(name='tinygrad.codegen.uopgraph', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730fd5030>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/uopgraph.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/uopgraph.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/uopgraph.cpython-310.pyc'}    
          from typing import Optional, Tuple, Dict, List, Set, cast, TYPE_CHECKING, Any, DefaultDict, Callable                                                                                           #     codegen/uopgraph.py           :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
          import functools, itertools, heapq, math, operator                                                                                                                                             #     codegen/uopgraph.py           :     3: G: {'Optional': typing.Optional, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'List': typing.List, 'Set': typing.Set, 'cast': <function cast at 0x789735eb17e0>, 'TYPE_CHECKING': False, 'Any': typing.Any, 'DefaultDict': typing.DefaultDict, 'Callable': typing.Callable}    
          from collections import defaultdict                                                                                                                                                            #     codegen/uopgraph.py           :     4: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'itertools': <module 'itertools' (built-in)>, 'heapq': <module 'heapq' from '/usr/lib/python3.10/heapq.py'>, 'math': <module 'math' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>}    
          from tinygrad.dtype import dtypes, PtrDType, ImageDType, DType                                                                                                                                 #     codegen/uopgraph.py           :     5: G: {'defaultdict': <class 'collections.defaultdict'>}    
          from tinygrad.ops import UnaryOps, BinaryOps, exec_alu, UOp, NOp, UOps, UPat, PatternMatcher, END_FOR_UOP, graph_rewrite, type_verify, print_uops                                              #     codegen/uopgraph.py           :     6: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'DType': <class 'tinygrad.dtype.DType'>}    
          from tinygrad.helpers import DEBUG, getenv, flatten, dedup, TRANSCENDENTAL, prod, CI, all_same, partition                                                                                      #     codegen/uopgraph.py           :     7: G: {'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'exec_alu': <function exec_alu at 0x789730ffb2e0>, 'UOp': <class 'tinygrad.ops.UOp'>, 'NOp': <class 'tinygrad.ops.NOp'>, 'UOps': <enum 'UOps'>, 'UPat': <class 'tinygrad.ops.UPat'>, 'PatternMatcher': <class 'tinygrad.ops.PatternMatcher'>, 'END_FOR_UOP': {<UOps.IF: 25>: (<UOps.STORE: 22>, <UOps.ENDIF: 28>), <UOps.RANGE: 26>: (<UOps.PHI: 23>, <UOps.ENDRANGE: 27>)}, 'graph_rewrite': <function graph_rewrite at 0x789730ffa200>, 'type_verify': <function type_verify at 0x789730ffb400>, 'print_uops': <function print_uops at 0x789730ffb490>}    
          from tinygrad.codegen.transcendental import xexp2, xlog2, xsin, TRANSCENDENTAL_SUPPORTED_DTYPES                                                                                                #     codegen/uopgraph.py           :     8: G: {'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'flatten': <function flatten at 0x789730f588b0>, 'dedup': <function dedup at 0x789730f581f0>, 'TRANSCENDENTAL': <tinygrad.helpers.ContextVar object at 0x789730f60670>, 'prod': <function prod at 0x7897327a7520>, 'CI': False, 'all_same': <function all_same at 0x789730f58430>, 'partition': <function partition at 0x789730f58d30>}    
            import math, functools                                                                                                                                                                       #     codegen/transcendental.py     :     1: G: {'__name__': 'tinygrad.codegen.transcendental', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730fd6500>, '__spec__': ModuleSpec(name='tinygrad.codegen.transcendental', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730fd6500>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/transcendental.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/transcendental.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/transcendental.cpython-310.pyc'}    
            from typing import Tuple, List                                                                                                                                                               #     codegen/transcendental.py     :     2: G: {'math': <module 'math' (built-in)>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
            from tinygrad.dtype import dtypes, DType                                                                                                                                                     #     codegen/transcendental.py     :     3: G: {'Tuple': typing.Tuple, 'List': typing.List}    
            from tinygrad.ops import UOp                                                                                                                                                                 #     codegen/transcendental.py     :     4: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>}    
            
            TRANSCENDENTAL_SUPPORTED_DTYPES = {dtypes.float16, dtypes.float32, dtypes.float64}                                                                                                           #     codegen/transcendental.py     :     6: G: {'UOp': <class 'tinygrad.ops.UOp'>}    

          if TYPE_CHECKING: from tinygrad.renderer import Renderer                                                                                                                                       #     codegen/uopgraph.py           :     9: G: {'xexp2': <function xexp2 at 0x789730597370>, 'xlog2': <function xlog2 at 0x789730597400>, 'xsin': <function xsin at 0x7897305972e0>, 'TRANSCENDENTAL_SUPPORTED_DTYPES': {dtypes.double, dtypes.float, dtypes.half}}    
          
          float4_folding = PatternMatcher([                                                                                                                                                              #     codegen/uopgraph.py           :    83: G: {'fold_expanded': <function fold_expanded at 0x789730594ca0>, 'vectorize_reduce': <function vectorize_reduce at 0x789730597490>, 'vectorize_alu': <function vectorize_alu at 0x789730597520>, 'fix_unfoldable_image_load': <function fix_unfoldable_image_load at 0x7897305975b0>}    
            (UPat(UOps.EXPAND, src=UPat(UOps.LOAD, src=(UPat(name="buf"), UPat()), allow_any_len=True), name="ex"), fold_expanded),                                                                           
            (UPat({UOps.BARRIER, UOps.SINK}, src=UPat(UOps.STORE, src=(UPat(name="buf"), UPat(), UPat()), allow_any_len=True), name="ex"), fold_expanded),                                                    
            (UPat(UOps.VECTORIZE, src=UPat(UOps.REDUCE), name="vec"), vectorize_reduce),                                                                                                                      
            (UPat(UOps.VECTORIZE, src=UPat({UOps.ALU, UOps.CAST, UOps.BITCAST}), name="vec"), vectorize_alu),                                                                                                 
          ])                                                                                                                                                                                                  

            class UPat:                                                                                                                                                                                  #     ops.py                        :   223: 
              def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,                                                   #     ops.py                        :   224: 
                           name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False, location=None):                                                                  
                self.op: Optional[Tuple[UOps, ...]] = None if op is None else (tuple(op) if isinstance(op, set) else (op,))                                                                              #     ops.py                        :   226: G: {'truncate': {dtypes.bool: <class 'bool'>, dtypes.half: <function truncate_fp16 at 0x789730ffacb0>, dtypes.float: <function <lambda> at 0x789730ffad40>, dtypes.double: <function <lambda> at 0x789730ffadd0>, dtypes.uchar: <function <lambda> at 0x789730ffae60>, dtypes.ushort: <function <lambda> at 0x789730ffaef0>, dtypes.uint: <function <lambda> at 0x789730ffaf80>, dtypes.ulong: <function <lambda> at 0x789730ffb010>, dtypes.char: <function <lambda> at 0x789730ffb0a0>, dtypes.short: <function <lambda> at 0x789730ffb130>, dtypes.int: <function <lambda> at 0x789730ffb1c0>, dtypes.long: <function <lambda> at 0x789730ffb250>}, 'exec_alu': <function exec_alu at 0x789730ffb2e0>, 'uop_alu_resolve': <function uop_alu_resolve at 0x789730ffb370>, 'type_verify': <function type_verify at 0x789730ffb400>, 'print_uops': <function print_uops at 0x789730ffb490>, 'flops_mem': <function flops_mem at 0x789730ffb520>}    L: {'name': 'buf', 'allow_any_len': False}    
                self.dtype: Optional[Tuple[DType, ...]] = None if dtype is None else (tuple(dtype) if isinstance(dtype, set) else (dtype,))                                                              #     ops.py                        :   227: 
                self.arg, self.name = arg, name                                                                                                                                                          #     ops.py                        :   228: 
                self.src: Any = None                                                                                                                                                                     #     ops.py                        :   229: 
                # try all permutations if it's a list
                if isinstance(src, list): self.src = list(itertools.permutations(src)) if not all_same(src) else [src]                                                                                   #     ops.py                        :   231: 
                # only one if it's a tuple
                elif isinstance(src, tuple): self.src = [src]                                                                                                                                            #     ops.py                        :   233: 
                # repeat if it's a UPat
                elif isinstance(src, UPat): self.src = [itertools.repeat(src)]                                                                                                                           #     ops.py                        :   235: 
            
                self.allowed_len: int = 0 if allow_any_len or isinstance(src, UPat) or src is None else len(src)                                                                                         #     ops.py                        :   237: 
                self.location = location or get_location()                                                                                                                                               #     ops.py                        :   238: L: {'self': UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None))}    

                  def get_location() -> Tuple[str, int]:                                                                                                                                                 #     ops.py                        :   194: 
                    frm = sys._getframe(1)                                                                                                                                                               #     ops.py                        :   195: 
                    # no matchers in ops.py, find the real frame
                    while (frm.f_code.co_filename.endswith("/ops.py") or frm.f_code.co_filename == '<string>') and frm.f_back is not None: frm = frm.f_back                                              #     ops.py                        :   197: L: {'frm': <frame at 0x789732fb3040, file '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', line 238, code __init__>}    
                    return frm.f_code.co_filename, frm.f_lineno                                                                                                                                          #     ops.py                        :   198: L: {'frm': <frame at 0x7897331f9780, file '/home/lorinbaum/code/tinygrad/tinygrad/codegen/uopgraph.py', line 84, code <module>>}    

            class PatternMatcher:                                                                                                                                                                        #     ops.py                        :   267: 
              def __init__(self, patterns:Sequence[Tuple[Union[UPat, NOp], Callable]]):                                                                                                                  #     ops.py                        :   268: 
                self.patterns = [(p.upat if isinstance(p, NOp) else p, fxn) for p,fxn in patterns]                                                                                                       #     ops.py                        :   269: L: {'self': <tinygrad.ops.PatternMatcher object at 0x7897305a8b20>, 'patterns': [(UPat((UOps.EXPAND), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.LOAD), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),)), <function fold_expanded at 0x789730594ca0>), (UPat((UOps.BARRIER, UOps.SINK), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.STORE), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),)), <function fold_expanded at 0x789730594ca0>), (UPat((UOps.VECTORIZE), None, name='vec', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.REDUCE), None, name=None, dtype=None, allow_any_len=True, src=(None)),)), <function vectorize_reduce at 0x789730597490>), (UPat((UOps.VECTORIZE), None, name='vec', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.ALU, UOps.BITCAST, UOps.CAST), None, name=None, dtype=None, allow_any_len=True, src=(None)),)), <function vectorize_alu at 0x789730597520>)]}    
                self.pdict: DefaultDict[Tuple[UOps, Any], List[Tuple[UPat, Callable, Set]]] = defaultdict(list)                                                                                          #     ops.py                        :   270: 
                # uop is required, arg is optional
                for p,fxn in self.patterns:                                                                                                                                                              #     ops.py                        :   272: 
                  assert p.op is not None                                                                                                                                                                #     ops.py                        :   273: L: {'p': UPat((UOps.EXPAND), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.LOAD), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),)), 'fxn': <function fold_expanded at 0x789730594ca0>}    
                  for uop in p.op: self.pdict[(uop, p.arg)].append((p, fxn, p.early_reject))                                                                                                             #     ops.py                        :   274: 

                    class UPat:                                                                                                                                                                          #     ops.py                        :   223: 
                      @functools.cached_property                                                                                                                                                         #     ops.py                        :   241: 
                      def early_reject(self) -> Set[Tuple[UOps, Any]]:                                                                                                                                        
                        # TODO: this can be improved to support some allowed_len == 0 patterns
                        return set((pp.op[0], pp.arg) for pp in self.src[0] if pp.op is not None and len(pp.op) == 1) if self.allowed_len else set()                                                     #     ops.py                        :   243: L: {'self': UPat((UOps.EXPAND), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.LOAD), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),))}    

          
          # this is symbolic 2.0
          constant_folder = PatternMatcher([                                                                                                                                                             #     codegen/uopgraph.py           :   204: G: {'float4_folding': <tinygrad.ops.PatternMatcher object at 0x7897305a8b20>, '_get_add_chain': <function _get_add_chain at 0x7897305976d0>, 'mod_folding': <function mod_folding at 0x78973059de10>, 'div_folding': <function div_folding at 0x78973059e170>, 'transcendental_folding': <functools._lru_cache_wrapper object at 0x789730585430>, 'threefry2x32': <function threefry2x32 at 0x78973059e200>, 'reduce_before_expand': <function reduce_before_expand at 0x78973059e3b0>, 'loop_collapse': <function loop_collapse at 0x78973059e440>, 'index_collapse': <function index_collapse at 0x78973059e4d0>}    
            # bool ADD is OR, MUL is AND. prevents other rules to rewrite bool ADD/MUL incorrectly                                                                                                            
            (UPat(UOps.ALU, BinaryOps.ADD, dtype=dtypes.bool, name="x"), lambda x: UOp(x.op, x.dtype, x.src, BinaryOps.OR)),                                                                                  
            (UPat(UOps.ALU, BinaryOps.MUL, dtype=dtypes.bool, name="x"), lambda x: UOp(x.op, x.dtype, x.src, BinaryOps.AND)),                                                                                 
            # VECTORIZE/GEP                                                                                                                                                                                   
            (NOp(UOps.GEP, src=(NOp(UOps.VECTORIZE, name="cast"),), name="gep"), lambda gep, cast: cast.src[gep.arg]),                                                                                        
            *[(NOp(UOps.VECTORIZE, dtypes.float.vec(i), tuple(NOp(UOps.GEP, dtypes.float,                                                                                                                     
                                   src=(NOp.var('x', dtype=dtypes.float.vec(i)),), arg=j) for j in range(i))), lambda x: x) for i in [2, 4, 8, 16]],                                                          
            *[(NOp(UOps.VECTORIZE, dtypes.half.vec(i), tuple(NOp(UOps.GEP, dtypes.half,                                                                                                                       
                                   src=(NOp.var('x', dtype=dtypes.half.vec(i)),), arg=j) for j in range(i))), lambda x: x) for i in [2, 4, 8, 16]],                                                           
            # tensor core with a 0 input is acc                                                                                                                                                               
            *[(NOp(UOps.WMMA, src=(NOp(UOps.VECTORIZE, src=tuple(NOp.const(None, 0.0) for _ in range(i))), NOp.var(), NOp.var('acc'))),                                                                       
               lambda acc: acc) for i in [2, 4, 8]],                                                                                                                                                          
            *[(NOp(UOps.WMMA, src=(NOp.var(), NOp(UOps.VECTORIZE, src=tuple(NOp.const(None, 0.0) for _ in range(i))), NOp.var('acc'))),                                                                       
               lambda acc: acc) for i in [2, 4, 8]],                                                                                                                                                          
            # tensor core cleanups                                                                                                                                                                            
            *[(NOp(UOps.REDUCE, src=(NOp(UOps.EXPAND, src=tuple(NOp(UOps.GEP, dtypes.float, src=(NOp.var('x'),), arg=i) for i in range(j)), name="expand"),)                                                  
              ,name="reduce", allow_any_len=True), reduce_before_expand) for j in [2,4,8]],                                                                                                                   
            (NOp.var("add") + NOp(UOps.WMMA, name="wmma"),                                                                                                                                                    
              lambda add, wmma: UOp(wmma.op, wmma.dtype, (wmma.src[0], wmma.src[1], wmma.src[2]+add), wmma.arg)),                                                                                             
            # threefry                                                                                                                                                                                        
            (NOp(UOps.ALU, dtype=dtypes.uint64, src=(NOp.var("x"), NOp.var("seed")), arg=BinaryOps.THREEFRY), threefry2x32),                                                                                  
            # extra arange loop folding because we don't fold adds. TODO: fold adds                                                                                                                           
            (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng") +                                                       
                                    NOp.var("idx2") + NOp.var("idx3"))                                                                                                                                        
             .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),                                              
            (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng") +                                                       
                                    NOp.var("idx2"))                                                                                                                                                          
             .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),                                              
            # arange loop folding (reduce)                                                                                                                                                                    
            (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))                                                        
             .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),                                              
            (NOp(UOps.REDUCE, src=((NOp.var("idx") - NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))                                                                           
             .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True),                                                              
             lambda **kwargs: loop_collapse(mval=UOp.const(dtypes.int, -1), **kwargs)),                                                                                                                       
            # arange loop folding (unrolled)                                                                                                                                                                  
            (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))                                                        
             .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)) + NOp.var("extra"),),                                                                                                  
             arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),                                                                                                                           
            # indexing (with a multiply offset)!                                                                                                                                                              
            (NOp(UOps.REDUCE, src=(NOp.var('idx').eq(NOp(UOps.RANGE, name="rng")).cast()*                                                                                                                     
              NOp(UOps.LOAD, src=(NOp.var("buf"), NOp.var('add')+NOp.var('mul')*NOp(UOps.RANGE, name="rng")), name="ld"),),                                                                                   
              arg=BinaryOps.ADD, name="reduce", allow_any_len=True), index_collapse),                                                                                                                         
            (NOp(UOps.REDUCE, src=(NOp.var('idx').ne(NOp(UOps.RANGE, name="rng")).__neg__().cast()*                                                                                                           
              NOp(UOps.LOAD, src=(NOp.var("buf"), NOp(UOps.RANGE, name="rng")), name="ld"),),                                                                                                                 
              arg=BinaryOps.ADD, name="reduce", allow_any_len=True),                                                                                                                                          
              lambda **kwargs: index_collapse(add=UOp.const(dtypes.int, 0), mul=UOp.const(dtypes.int, 1), **kwargs)),                                                                                         
            (NOp(UOps.REDUCE, src=(NOp.var('idx').eq(NOp(UOps.RANGE, name="rng")).where(                                                                                                                      
              NOp(UOps.LOAD, src=(NOp.var("buf"), NOp.var('add')+NOp.var('mul')*NOp(UOps.RANGE, name="rng")), name="ld"), NOp.const(None, 0.0)),),                                                            
              arg=BinaryOps.ADD, name="reduce", allow_any_len=True), index_collapse),                                                                                                                         
            # max folding                                                                                                                                                                                     
            (NOp.max(NOp.var('x'), NOp.var('y')), lambda x,y: x if x.vmin.arg >= y.vmax.arg else y if x.vmax.arg <= y.vmin.arg else None),                                                                    
            # const rules                                                                                                                                                                                     
            (NOp(UOps.GEP, src=(NOp.cvar("c"),), name="root"), lambda root, c: root.const(c.arg)),                                                                                                            
            (UPat(UOps.CAST, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: root.const(c.arg)),                                                                                                
            # a REDUCE without ranges is a NOOP                                                                                                                                                               
            (NOp(UOps.REDUCE, src=(NOp.var('x'),)), lambda x: x),                                                                                                                                             
            # GEP on a const is the const                                                                                                                                                                     
            (NOp(UOps.GEP, src=(NOp.cvar("x"),), name="root"), lambda root,x: root.const(x.arg)),                                                                                                             
            # a conditional with the same results either way is a noop, also fold const conditionals                                                                                                          
            (NOp.var().where(NOp.var("val"), NOp.var("val")), lambda val: val),                                                                                                                               
            (NOp.cvar('gate').where(NOp.var('c0'), NOp.var('c1')), lambda gate, c0, c1: c0 if gate.arg else c1),                                                                                              
            # ** constant folding **                                                                                                                                                                          
            (UPat(UOps.ALU, name="root", src=UPat(UOps.CONST)), lambda root: root.const(exec_alu(root.arg, root.dtype, [x.arg for x in root.src]))),                                                          
            # ** self folding **                                                                                                                                                                              
            (NOp.var('x') + 0, lambda x: x),    # x+0 -> x                                                                                                                                                    
            (NOp.var('x') * 1, lambda x: x),    # x*1 -> x                                                                                                                                                    
            (NOp.var('x') // NOp.var('x'), lambda x: x.const(1)), # x//x -> 1                                                                                                                                 
            (NOp.var('x') // 1, lambda x: x),   # x//1 -> x                                                                                                                                                   
            (NOp.var('x') // -1, lambda x: -x), # x//-1 -> -x                                                                                                                                                 
            (NOp.var('x') / NOp.var('x'), lambda x: x.const(1)), # x/x -> 1                                                                                                                                   
            (NOp.var('x', dtype=dtypes.bool) & NOp.cvar('c'), lambda x,c: x if c.arg else c),                                                                                                                 
            (NOp.var('x', dtype=dtypes.bool) | NOp.cvar('c'), lambda x,c: c if c.arg else x),                                                                                                                 
            # ** zero folding **                                                                                                                                                                              
            # x*0 -> 0 or 0*x -> 0                                                                                                                                                                            
            # if x is nan or inf it should render the nan value.                                                                                                                                              
            # NOTE: this can be wrong for loaded NaN                                                                                                                                                          
            (NOp.var('x') * 0, lambda x: x.const(float('nan') if isinstance(x.arg, float) and (math.isnan(x.arg) or math.isinf(x.arg)) else 0)),                                                              
            # min==max -> CONST (slow!)                                                                                                                                                                       
            (UPat({UOps.ALU, UOps.DEFINE_VAR}, name='x'), lambda x: x.const(x.vmin.arg) if x.vmin.arg == x.vmax.arg else None),                                                                               
            # ** load/store folding **                                                                                                                                                                        
            (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.load(NOp.var("buf"), NOp.var("idx"))), lambda buf,idx:UOp(UOps.NOOP)),                                                                             
            # ** two stage add/mul folding **                                                                                                                                                                 
            ((NOp.var('x') + NOp.cvar('c1')) + NOp.cvar('c2'), lambda x,c1,c2: x+x.const(exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, c2.arg]))),                                                                
            ((NOp.var("x") * NOp.cvar("c1")) * NOp.cvar("c2"), lambda x,c1,c2: x*x.const(exec_alu(BinaryOps.MUL, x.dtype, [c1.arg, c2.arg]))),                                                                
            # *** rules from symbolic ***                                                                                                                                                                     
            # ** lt **                                                                                                                                                                                        
            # c0*x<c1 for positive int c0,c1                                                                                                                                                                  
            ((NOp.cvar('c0')*NOp.var('x')).lt(NOp.cvar('c1')),                                                                                                                                                
             lambda x,c0,c1: x.lt(math.ceil(c1.arg/c0.arg)) if dtypes.is_int(x.dtype) and c0.arg > 0 and c1.arg > 0 else None),                                                                               
            # mul add lt                                                                                                                                                                                      
            (((NOp.cvar('c0')*NOp.var('x'))+NOp.var('x2')).lt(NOp.cvar('c1')),                                                                                                                                
             lambda x,x2,c0,c1: x.lt(c1.arg//c0.arg) if c1.arg % c0.arg == 0 and c0.arg > x2.vmax.arg and x2.vmin.arg >= 0 else None),                                                                        
            # generic lt folding (use div)                                                                                                                                                                    
            (NOp.var('x').lt(NOp.cvar('c')), lambda x,c: newx.src[0].lt(newx.src[1]) if 0 < c.arg and dtypes.is_int(x.dtype) and \                                                                            
             not dtypes.is_unsigned(x.dtype) and (newx:=div_folding(x,c.arg)) is not None and newx.op is UOps.ALU and newx.arg is BinaryOps.IDIV else None),                                                  
            # ** div **                                                                                                                                                                                       
            # # div folding                                                                                                                                                                                   
            (NOp.var('x') // NOp.cvar('c'), lambda x,c:                                                                                                                                                       
             newx if 0 < c.arg and not dtypes.is_unsigned(x.dtype) and (newx:=div_folding(x,c.arg)) is not None else None),                                                                                   
            # ** mod **                                                                                                                                                                                       
            # mod folding                                                                                                                                                                                     
            (NOp.var('x') % NOp.cvar('c'), lambda x,c: newx if 0 < c.arg and (newx:=mod_folding(x,c.arg)) is not None else None),                                                                             
            # mul mod                                                                                                                                                                                         
            ((NOp.cvar('c0')*NOp.var('x')) % NOp.cvar('c1'), lambda x,c0,c1: (x%(c1.arg//c0.arg))*c0 if c1.arg%c0.arg == 0 else None),                                                                        
            # (x%c)+(x//c)*c = x                                                                                                                                                                              
            (NOp.var('x')%NOp.cvar('c')+(NOp.var('x')//NOp.cvar('c'))*NOp.cvar('c'), lambda x,c: x),                                                                                                          
            # ** combine terms **                                                                                                                                                                             
            # -(x+y) -> -x + -y                                                                                                                                                                               
            (-(NOp.var("x") + NOp.var("y")), lambda x,y: (-x)+(-y)),                                                                                                                                          
            # (x+c0)*c1 -> x*c1+c0*c1. only for signed int, float have inf*0=nan issue                                                                                                                        
            ((NOp.var("x") + NOp.cvar("c0")) * NOp.cvar("c1"), lambda x,c0,c1:                                                                                                                                
             x*c1+c0.arg*c1.arg if dtypes.is_int(x.dtype) and not dtypes.is_unsigned(x.dtype) else None),                                                                                                     
            # (x*c0)+(x*c1) -> x*(c0+c1)                                                                                                                                                                      
            (NOp.var("x") * NOp.cvar("c0") + NOp.var("x") * NOp.cvar("c1"), lambda x,c0,c1: x*exec_alu(BinaryOps.ADD, x.dtype, [c0.arg, c1.arg])),                                                            
            # (x+x*c)-> x*(c+1)                                                                                                                                                                               
            (NOp.var("x") + NOp.var("x") * NOp.cvar("c"), lambda x,c: x*(c.arg+1)),                                                                                                                           
            # (x+x)-> x*2                                                                                                                                                                                     
            (NOp.var("x") + NOp.var("x"), lambda x: x*2),                                                                                                                                                     
            # (x*c0)+(y*c0) -> (x+y)*c0                                                                                                                                                                       
            #((NOp.var("x") * NOp.cvar("c0")) + (NOp.var("y") * NOp.cvar("c0")), lambda x,y,c0: c0*(x+y)),                                                                                                    
            # (x*x2)/x2 -> x                                                                                                                                                                                  
            ((NOp.var("x") * NOp.var("x2")) / NOp.var("x2"), lambda x,x2: x),                                                                                                                                 
            # (x//c0)//c1 -> x//(c0*c1)                                                                                                                                                                       
            ((NOp.var("x") // NOp.cvar("c0")) // NOp.cvar("c1"), lambda x,c0,c1: x//x.const(exec_alu(BinaryOps.MUL, x.dtype, [c0.arg, c1.arg]))),                                                             
            # (x/x1)/x2 -> x/(x1*x2)                                                                                                                                                                          
            ((NOp.var("x") / NOp.var("x2")) / NOp.var("x3"), lambda x,x2,x3: x/(x2*x3)),                                                                                                                      
            # c0 + x < c1 -> x < c1 - c0                                                                                                                                                                      
            ((NOp.cvar("c0") + NOp.var("x")).lt(NOp.cvar("c1")), lambda x,c0,c1: UOp.lt(x, x.const(exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, -c0.arg])))),                                                    
            # x!=0 -> (bool)x                                                                                                                                                                                 
            (NOp.var("x").ne(0), lambda x: x.cast(dtypes.bool)),                                                                                                                                              
            # TODO: can do the invert of this (flip alt/load) when we fix double ops                                                                                                                          
            (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.var("gate").where(NOp.var("alt"), NOp.load(NOp.var("buf"), NOp.var("idx")))),                                                                      
             lambda buf, idx, gate, alt: UOp.store(buf, idx, alt, gate)),                                                                                                                                     
            # cast NOOP (NOTE: it's str to deal with PtrDType)                                                                                                                                                
            (NOp(UOps.CAST, name="root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),                                                                                   
            (NOp(UOps.VECTORIZE, name="root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),                                                                              
            # fold gated LOAD/STORE                                                                                                                                                                           
            (NOp.load(NOp.var("buf"), NOp.var("idx"), NOp.var("var"), NOp.const(dtypes.bool, True)), lambda buf,idx,var: UOp.load(buf, idx, dtype=var.dtype)),                                                
            (NOp.load(NOp.var("buf"), NOp.var("idx"), NOp.var("var"), NOp.const(dtypes.bool, True), NOp.var("barrier")),                                                                                      
             lambda buf,idx,var,barrier: UOp.load(buf, idx, barrier, dtype=var.dtype)),                                                                                                                       
            (NOp.load(NOp.var(), NOp.var(), NOp.var("var"), NOp.const(dtypes.bool, False)), lambda var: var),                                                                                                 
            (NOp.load(NOp.var(), NOp.var(), NOp.var("var"), NOp.const(dtypes.bool, False), NOp.var()), lambda var: var),                                                                                      
            (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.var("val"), NOp.const(dtypes.bool, True)), UOp.store),                                                                                             
            (NOp.store(NOp.var(), NOp.var(), NOp.var(), NOp.const(dtypes.bool, False)), lambda: UOp(UOps.NOOP)),                                                                                              
            # remove NOOPs from SINK                                                                                                                                                                          
            (NOp(UOps.SINK, name="root"),                                                                                                                                                                     
              lambda root: UOp(UOps.SINK, root.dtype, a, root.arg) if len(a:=tuple(x for x in root.src if x.op is not UOps.NOOP)) != len(root.src) else None),                                                
            # ** move add consts to end (NOTE: this is still happening before constant folding) **                                                                                                            
            (UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(UOps.CONST, name='c1'), UPat(name='x'))), lambda c1,x: x+c1 if x.op is not UOps.CONST else None),                                                        
            (UPat(UOps.ALU, BinaryOps.ADD, src=[UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name='x'), UPat(UOps.CONST, name='c1'))), UPat(name='y')]),                                                           
              lambda x,c1,y: (x+y)+c1),                                                                                                                                                                       
          ])                                                                                                                                                                                                  

            @dataclass(frozen=True, order=True)                                                                                                                                                          #     dtype.py                      :     9: 
            class DType:                                                                                                                                                                                      
              def vec(self, sz:int):                                                                                                                                                                     #     dtype.py                      :    16: 
                assert sz > 1 and self.count == 1, f"can't vectorize {self} with size {sz}"                                                                                                              #     dtype.py                      :    17: G: {'sum_acc_dtype': <function sum_acc_dtype at 0x789730f70940>}    L: {'self': dtypes.float, 'sz': 2}    
                return DType(self.priority, self.itemsize*sz, f"{INVERSE_DTYPES_DICT[self.name]}{sz}", None, sz)                                                                                         #     dtype.py                      :    18: 

            @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                   #     ops.py                        :   203: 
            class NOp(UOp):                                                                                                                                                                                   
              @staticmethod                                                                                                                                                                              #     ops.py                        :   211: 
              @functools.lru_cache(None)                                                                                                                                                                      
              def var(name:Optional[str]=None, dtype:Optional[DType]=None): return NOp(UOps.NOOP, dtype=dtype, name=name)                                                                                     

            @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                   #     ops.py                        :   203: 
            class NOp(UOp):                                                                                                                                                                                   
              def const(self:Union[UOp, DType, None], b:ConstType|Variable): return NOp((x:=UOp.const(self, b)).op, x.dtype, x.src, x.arg)                                                               #     ops.py                        :   215: 

                @dataclass(frozen=True, eq=False)                                                                                                                                                        #     ops.py                        :    58: 
                class UOp:                                                                                                                                                                                    
                  def const(self:Union[UOp, DType, None], b:ConstType|Variable): return UOp._const(self.dtype if isinstance(self, UOp) else self, b)                                                     #     ops.py                        :   110: 

                    @dataclass(frozen=True, eq=False)                                                                                                                                                    #     ops.py                        :    58: 
                    class UOp:                                                                                                                                                                                
                      @staticmethod                                                                                                                                                                      #     ops.py                        :   115: 
                      @functools.lru_cache(maxsize=None)                                                                                                                                                      
                      def _const(dtype:Optional[DType], b:ConstType|Variable):                                                                                                                                
                        # TODO: fix dtype of b.max after Variable is just an UOp
                        if isinstance(b, Variable): return UOp(UOps.DEFINE_VAR, dtype, (UOp.const(dtypes.int, b.min), UOp.const(dtypes.int, cast(int,b.max))), b)                                        #     ops.py                        :   117: L: {'b': 0.0}    
                        if dtype is not None and dtype != (sdtype := dtype.scalar()):                                                                                                                    #     ops.py                        :   118: 
                        return UOp(UOps.CONST, dtype, arg=dtypes.as_const(b, dtype) if dtype is not None else b)                                                                                         #     ops.py                        :   120: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __add__(self, x): return self.alu(BinaryOps.ADD, self.ufix(x))                                                                                                                         #     ops.py                        :    91: 

                @dataclass(frozen=True, eq=False)                                                                                                                                                        #     ops.py                        :    58: 
                class UOp:                                                                                                                                                                                    
                  def ufix(self, x): return self.const(x) if not isinstance(x, UOp) else x                                                                                                               #     ops.py                        :    86: 

                @dataclass(frozen=True, eq=False)                                                                                                                                                        #     ops.py                        :    58: 
                class UOp:                                                                                                                                                                                    
                  def alu(self, arg, *src:UOp):                                                                                                                                                          #     ops.py                        :   121: 
                    return type(self)(UOps.ALU, dtypes.bool if arg in {BinaryOps.CMPLT, BinaryOps.CMPNE} else (self, *src)[-1].dtype, (self,)+src, arg)                                                  #     ops.py                        :   122: L: {'self': NOp(UOps.NOOP, None, arg=None, src=()), 'arg': <BinaryOps.ADD: 1>, 'src': (NOp(UOps.WMMA, None, arg=None, src=()),)}    

            @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                   #     ops.py                        :   203: 
            class NOp(UOp):                                                                                                                                                                                   
              @staticmethod                                                                                                                                                                              #     ops.py                        :   214: 
              @functools.lru_cache(None)                                                                                                                                                                      
              def cvar(name:Optional[str]=None, dtype:Optional[DType]=None): return NOp(UOps.CONST, dtype=dtype, name=name)                                                                                   

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __mul__(self, x): return self.alu(BinaryOps.MUL, self.ufix(x))                                                                                                                         #     ops.py                        :    94: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def lt(self, x): return self.alu(BinaryOps.CMPLT, self.ufix(x))                                                                                                                            #     ops.py                        :   104: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def where(self, x, y): return self.alu(TernaryOps.WHERE, x, y)                                                                                                                             #     ops.py                        :   108: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __sub__(self, x): return self.alu(BinaryOps.ADD, self.ufix(-x))                                                                                                                        #     ops.py                        :    93: 

                @dataclass(frozen=True, eq=False)                                                                                                                                                        #     ops.py                        :    58: 
                class UOp:                                                                                                                                                                                    
                  def __neg__(self): return self*(-1) if self.dtype != dtypes.bool else self.ne(True)                                                                                                    #     ops.py                        :    90: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def eq(self, x): return -self.ne(x)                                                                                                                                                        #     ops.py                        :   103: 

                @dataclass(frozen=True, eq=False)                                                                                                                                                        #     ops.py                        :    58: 
                class UOp:                                                                                                                                                                                    
                  def ne(self, x): return self.alu(BinaryOps.CMPNE, self.ufix(x))                                                                                                                        #     ops.py                        :   102: 

                @dataclass(frozen=True, order=True)                                                                                                                                                      #     dtype.py                      :     9: 
                class DType:                                                                                                                                                                                  
                  def scalar(self): return DTYPES_DICT[self.name[:-len(str(self.count))]] if self.count > 1 else self                                                                                    #     dtype.py                      :    19: 

                class dtypes:                                                                                                                                                                            #     dtype.py                      :    38: 
                  @staticmethod                                                                                                                                                                          #     dtype.py                      :    57: 
                  def as_const(val: ConstType, dtype:DType): return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)                                                 

                    class dtypes:                                                                                                                                                                        #     dtype.py                      :    38: 
                      @staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool                                                                                           #     dtype.py                      :    44: 
                      @functools.lru_cache(None)                                                                                                                                                              
                      def is_int(x: DType) -> bool: return x.scalar() in {dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64, dtypes.pyint} or dtypes.is_unsigned(x)                                       

                        class dtypes:                                                                                                                                                                    #     dtype.py                      :    38: 
                          @staticmethod                                                                                                                                                                  #     dtype.py                      :    47: 
                          @functools.lru_cache(None)                                                                                                                                                          
                          def is_unsigned(x: DType) -> bool: return x.scalar() in {dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64}                                                                 

                    class dtypes:                                                                                                                                                                        #     dtype.py                      :    38: 
                      @staticmethod                                                                                                                                                                      #     dtype.py                      :    41: 
                      @functools.lru_cache(None)                                                                                                                                                              
                      def is_float(x: DType) -> bool: return x.scalar() in {dtypes.float16, dtypes.bfloat16, dtypes.float32, dtypes.float64}                                                                  

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def cast(self, dtype=None): return type(self)(UOps.CAST, dtype, (self,))                                                                                                                   #     ops.py                        :    87: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def max(self, x): return self.alu(BinaryOps.MAX, x)                                                                                                                                        #     ops.py                        :   106: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __floordiv__(self, x): return self.alu(BinaryOps.IDIV, self.ufix(x))                                                                                                                   #     ops.py                        :    96: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __truediv__(self, x): return self.alu(BinaryOps.MUL, self.ufix(x).alu(UnaryOps.RECIP))                                                                                                 #     ops.py                        :    97: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __and__(self, x): return self.alu(BinaryOps.AND, self.ufix(x))                                                                                                                         #     ops.py                        :   100: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __or__(self, x): return self.alu(BinaryOps.OR, self.ufix(x))                                                                                                                           #     ops.py                        :   101: 

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              @staticmethod                                                                                                                                                                              #     ops.py                        :   124: 
              def load(*src:UOp, dtype:Optional[DType]=None, **kwargs): return type(src[0])(UOps.LOAD, dtype, tuple(src)+tuple(kwargs.values()))                                                              

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              @staticmethod                                                                                                                                                                              #     ops.py                        :   126: 
              def store(*src:UOp, **kwargs): return type((src:=(*src, *kwargs.values()))[0])(UOps.STORE, None, src)                                                                                           

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def __mod__(self, x): return self.alu(BinaryOps.MOD, self.ufix(x))                                                                                                                         #     ops.py                        :    98: 

            def all_same(items:Union[Tuple[T, ...], List[T]]): return all(x == items[0] for x in items)                                                                                                  #     helpers.py                    :    26: 

            @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                   #     ops.py                        :   203: 
            class NOp(UOp):                                                                                                                                                                                   
              @functools.cached_property                                                                                                                                                                 #     ops.py                        :   218: 
              def upat(self:NOp) -> UPat:                                                                                                                                                                     
                return UPat(name=self.name, dtype=self.dtype, location=self.location) if self.op is UOps.NOOP else \                                                                                     #     ops.py                        :   219: L: {'self': NOp(UOps.GEP, None, arg=None, src=(\n  NOp(UOps.VECTORIZE, None, arg=None, src=()),))}    
                  UPat(self.op, self.arg, (list if self.commutative() else tuple)([src.upat for src in self.src]) or None, self.name,                                                                         
                       self.dtype, self.allow_any_len, location=self.location)                                                                                                                                

                  @dataclass(frozen=True, eq=False)                                                                                                                                                      #     ops.py                        :    58: 
                  class UOp:                                                                                                                                                                                  
                    def commutative(self) -> bool:                                                                                                                                                       #     ops.py                        :    63: 
                      return (self.op is UOps.ALU and \                                                                                                                                                  #     ops.py                        :    64: 
                        self.arg in {BinaryOps.ADD, BinaryOps.MUL, BinaryOps.MAX, BinaryOps.CMPNE, BinaryOps.XOR, BinaryOps.AND, BinaryOps.OR})                                                               

          
          acc_number = 0                                                                                                                                                                                 #     codegen/uopgraph.py           :   396: G: {'constant_folder': <tinygrad.ops.PatternMatcher object at 0x78973055a470>, '_expand_arg_to_idx': <function _expand_arg_to_idx at 0x78973059c700>, '_choices_from_args': <function _choices_from_args at 0x78973059c5e0>, '_swizzle_args': <functools._lru_cache_wrapper object at 0x7897305f9220>, 'do_expand': <function do_expand at 0x78973059c040>}    
          
          expander = PatternMatcher([                                                                                                                                                                    #     codegen/uopgraph.py           :   439: G: {'acc_number': 0, 'do_reduce': <function do_reduce at 0x78973059c310>, 'do_contract': <function do_contract at 0x78973059c3a0>, 'no_vectorized_alu': <function no_vectorized_alu at 0x78973059c280>, 'create_gate': <function create_gate at 0x78973059c430>}    
            # create gate MUST BE BEFORE expander                                                                                                                                                             
            (NOp(UOps.STORE, name="root"), create_gate),                                                                                                                                                      
            # do expansion                                                                                                                                                                                    
            (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST, UOps.GEP, UOps.WMMA, UOps.LOAD, UOps.STORE,                                                                                                             
                   UOps.VECTORIZE, UOps.REDUCE, UOps.EXPAND, UOps.IF}, name="root"), do_expand),                                                                                                              
            (NOp(UOps.CONTRACT, name="con"), do_contract),                                                                                                                                                    
            # remove EXPANDs from SINK                                                                                                                                                                        
            (NOp(UOps.SINK, name="root"),                                                                                                                                                                     
             lambda root: UOp(UOps.SINK, root.dtype, a, root.arg)                                                                                                                                             
              if len(a:=tuple(flatten(x.src if x.op is UOps.EXPAND else (x,) for x in root.src))) != len(root.src) else None),                                                                                
            # BARRIERs aren't actually expanded                                                                                                                                                               
            (NOp(UOps.BARRIER, src=(NOp(UOps.EXPAND, name="ex"),)), lambda ex: UOp(UOps.EXPAND, None, (UOp(UOps.BARRIER, None, ex.src),)*len(ex.src), ex.arg)),                                               
            # empty EXPAND is NOOP                                                                                                                                                                            
            (NOp(UOps.EXPAND, src=(NOp.var('x'),), arg=()), lambda x: x),                                                                                                                                     
            # EXPAND GEP (needed for WMMA, generalize this) -> vectorized ALU                                                                                                                                 
            (NOp(UOps.EXPAND, name="ex", src=tuple(NOp.var('x').gep(i)+NOp.var('y').gep(i) for i in range(8))),                                                                                               
              lambda ex,x,y: UOp(UOps.EXPAND, ex.dtype, tuple((x+y).gep(i) for i in range(8)), ex.arg)),                                                                                                      
          ])                                                                                                                                                                                                  

            @dataclass(frozen=True, eq=False)                                                                                                                                                            #     ops.py                        :    58: 
            class UOp:                                                                                                                                                                                        
              def gep(self, i:int): return type(self)(UOps.GEP, self.dtype.scalar() if self.dtype is not None else None, (self,), i)                                                                     #     ops.py                        :    89: 

          
          reducer = PatternMatcher([                                                                                                                                                                     #     codegen/uopgraph.py           :   467: G: {'expander': <tinygrad.ops.PatternMatcher object at 0x7897305aaef0>, 'delete_redundant_gates': <function delete_redundant_gates at 0x78973059e9e0>}    
            (NOp(UOps.REDUCE, name="root"), do_reduce),                                                                                                                                                       
            # no ALU on vectorized dtypes                                                                                                                                                                     
            (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST}, name="alu"), no_vectorized_alu),                                                                                                                       
            # delete_redundant_gates (after expand, is this still needed?)                                                                                                                                    
            (NOp(UOps.STORE, name="root"), delete_redundant_gates),                                                                                                                                           
            # late fixup of unfoldable image loads                                                                                                                                                            
            (UPat(UOps.LOAD, src=(UPat(name="buf"), UPat()), allow_any_len=True, name="load"), fix_unfoldable_image_load),                                                                                    
          ])                                                                                                                                                                                                  
          
          no_pyint = PatternMatcher([(UPat({UOps.CONST, UOps.ALU, UOps.SPECIAL, UOps.RANGE, UOps.EXPAND}, dtype=dtypes.pyint, name="x"),                                                                 #     codegen/uopgraph.py           :   477: G: {'reducer': <tinygrad.ops.PatternMatcher object at 0x7897305abe80>}    
              lambda x: UOp(x.op, dtypes.int32, x.src, x.arg))])                                                                                                                                              
          
          linearize_cnt = 0                                                                                                                                                                              #     codegen/uopgraph.py           :   493: G: {'no_pyint': <tinygrad.ops.PatternMatcher object at 0x7897305abd60>, 'get_children_dfs': <function get_children_dfs at 0x78973059eb00>}    

        render_ops: Any = { NumNode: lambda self, ops, ctx: UOp.const(dtypes.pyint, self.b),                                                                                                             #     shape/shapetracker.py         :    15: G: {'constant_folder': <tinygrad.ops.PatternMatcher object at 0x78973055a470>, 'variable_to_uop': <function variable_to_uop at 0x789730ffbd00>}    
                            MulNode: lambda self, ops, ctx: self.a.render(ops, ctx)*variable_to_uop(self.b, ctx),                                                                                             
                            DivNode: lambda self, ops, ctx: self.a.render(ops, ctx)//variable_to_uop(self.b, ctx),                                                                                            
                            ModNode: lambda self, ops, ctx: self.a.render(ops, ctx)%variable_to_uop(self.b, ctx),                                                                                             
                            LtNode: lambda self, ops, ctx: self.a.render(ops, ctx).lt(variable_to_uop(self.b, ctx)),                                                                                          
          Variable: lambda self,ops,ctx: ctx[self] if ctx is not None and self in ctx else \                                                                                                                  
            UOp(UOps.DEFINE_VAR, dtypes.int, (UOp.const(dtypes.int, self.min), UOp.const(dtypes.int, self.max)), self),                                                                                       
          SumNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a+b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)),                                                                    
          AndNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a*b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)) }                                                                   

          @dataclass(frozen=True)                                                                                                                                                                        #     shape/shapetracker.py         :    36: 
          class ShapeTracker:                                                                                                                                                                                 
            views: Tuple[View, ...]                                                                                                                                                                      #     shape/shapetracker.py         :    37: G: {'render_ops': {<class 'tinygrad.shape.symbolic.NumNode'>: <function <lambda> at 0x78973059ecb0>, <class 'tinygrad.shape.symbolic.MulNode'>: <function <lambda> at 0x78973059ed40>, <class 'tinygrad.shape.symbolic.DivNode'>: <function <lambda> at 0x78973059edd0>, <class 'tinygrad.shape.symbolic.ModNode'>: <function <lambda> at 0x78973059ee60>, <class 'tinygrad.shape.symbolic.LtNode'>: <function <lambda> at 0x78973059eef0>, <class 'tinygrad.shape.symbolic.Variable'>: <function <lambda> at 0x78973059ef80>, <class 'tinygrad.shape.symbolic.SumNode'>: <function <lambda> at 0x78973059f010>, <class 'tinygrad.shape.symbolic.AndNode'>: <function <lambda> at 0x78973059f0a0>}, '_uop_view': <function _uop_view at 0x78973059f130>}    L: {'__module__': 'tinygrad.shape.shapetracker', '__qualname__': 'ShapeTracker', '__annotations__': {}}    

      from tinygrad.device import Buffer                                                                                                                                                                 #     lazy.py                       :     8: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>}    
        from __future__ import annotations                                                                                                                                                               #     device.py                     :     1: G: {'__name__': 'tinygrad.device', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730fd5660>, '__spec__': ModuleSpec(name='tinygrad.device', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730fd5660>, origin='/home/lorinbaum/code/tinygrad/tinygrad/device.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/device.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/device.cpython-310.pyc'}    
        import multiprocessing, decimal, statistics, random                                                                                                                                              #     device.py                     :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
        from dataclasses import dataclass                                                                                                                                                                #     device.py                     :     3: G: {'multiprocessing': <module 'multiprocessing' from '/usr/lib/python3.10/multiprocessing/__init__.py'>, 'decimal': <module 'decimal' from '/usr/lib/python3.10/decimal.py'>, 'statistics': <module 'statistics' from '/usr/lib/python3.10/statistics.py'>, 'random': <module 'random' from '/usr/lib/python3.10/random.py'>}    
        from collections import defaultdict                                                                                                                                                              #     device.py                     :     4: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
        from typing import List, Optional, Dict, Tuple, Any, cast, Protocol, Type                                                                                                                        #     device.py                     :     5: G: {'defaultdict': <class 'collections.defaultdict'>}    
        import importlib, inspect, functools, pathlib, os, ctypes, atexit, time, contextlib, array                                                                                                       #     device.py                     :     6: G: {'List': typing.List, 'Optional': typing.Optional, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Any': typing.Any, 'cast': <function cast at 0x789735eb17e0>, 'Protocol': <class 'typing.Protocol'>, 'Type': typing.Type}    
        from tinygrad.helpers import SAVE_SCHEDULE, getenv, diskcache_get, diskcache_put, DEBUG, GlobalCounters, flat_mv, from_mv, ProfileLogger, PROFILE                                                #     device.py                     :     7: G: {'importlib': <module 'importlib' from '/usr/lib/python3.10/importlib/__init__.py'>, 'inspect': <module 'inspect' from '/usr/lib/python3.10/inspect.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'atexit': <module 'atexit' (built-in)>, 'time': <module 'time' (built-in)>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'array': <module 'array' (built-in)>}    
        from tinygrad.dtype import DType, ImageDType                                                                                                                                                     #     device.py                     :     8: G: {'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'diskcache_get': <function diskcache_get at 0x789730f5a320>, 'diskcache_put': <function diskcache_put at 0x789730f5a3b0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'flat_mv': <function flat_mv at 0x789730f5aa70>, 'from_mv': <function from_mv at 0x789730f5a680>, 'ProfileLogger': <class 'tinygrad.helpers.ProfileLogger'>, 'PROFILE': <tinygrad.helpers.ContextVar object at 0x789730f60580>}    
        from tinygrad.renderer import Renderer                                                                                                                                                           #     device.py                     :     9: G: {'DType': <class 'tinygrad.dtype.DType'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>}    
          from typing import Optional, List, Tuple, Dict, Callable, Any                                                                                                                                  #     renderer/__init__.py          :     1: G: {'__name__': 'tinygrad.renderer', '__doc__': None, '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730563ca0>, '__spec__': ModuleSpec(name='tinygrad.renderer', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730563ca0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad/renderer']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad/renderer'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/__init__.cpython-310.pyc'}    
          import functools                                                                                                                                                                               #     renderer/__init__.py          :     2: G: {'Optional': typing.Optional, 'List': typing.List, 'Tuple': typing.Tuple, 'Dict': typing.Dict, 'Callable': typing.Callable, 'Any': typing.Any}    
          from dataclasses import dataclass, field                                                                                                                                                       #     renderer/__init__.py          :     3: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
          from tinygrad.helpers import to_function_name, dedup                                                                                                                                           #     renderer/__init__.py          :     4: G: {'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>}    
          from tinygrad.ops import Op, UOps, UOp, flops_mem                                                                                                                                              #     renderer/__init__.py          :     5: G: {'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'dedup': <function dedup at 0x789730f581f0>}    
          from tinygrad.shape.symbolic import sym_infer, sint, Variable                                                                                                                                  #     renderer/__init__.py          :     6: G: {'Op': typing.Union[tinygrad.ops.UnaryOps, tinygrad.ops.BinaryOps, tinygrad.ops.ReduceOps, tinygrad.ops.MetaOps, tinygrad.ops.TernaryOps], 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'flops_mem': <function flops_mem at 0x789730ffb520>}    
          from tinygrad.dtype import DType                                                                                                                                                               #     renderer/__init__.py          :     7: G: {'sym_infer': <function sym_infer at 0x789730fe70a0>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Variable': <class 'tinygrad.shape.symbolic.Variable'>}    

            @dataclass(frozen=True)                                                                                                                                                                      #     renderer/__init__.py          :    10: 
            class TensorCore: # D = A * B + C, A is (M x K), B is (K x N), C and D are (M x N)                                                                                                                
              dims: Tuple[int,int,int] # N, M, K                                                                                                                                                         #     renderer/__init__.py          :    11: G: {'DType': <class 'tinygrad.dtype.DType'>}    L: {'__module__': 'tinygrad.renderer', '__qualname__': 'TensorCore', '__annotations__': {}}    
              dtype_in: DType # dtype for A and B                                                                                                                                                        #     renderer/__init__.py          :    12: 
              dtype_out: DType # dtype for C and D                                                                                                                                                       #     renderer/__init__.py          :    13: 
              threads: List[Tuple[int,int]] # list of (TC dim,amt) that construct the warp thread structure                                                                                              #     renderer/__init__.py          :    14: 

            @dataclass                                                                                                                                                                                   #     renderer/__init__.py          :    18: 
            class Program:                                                                                                                                                                                    
              name:str                                                                                                                                                                                   #     renderer/__init__.py          :    19: G: {'TensorCore': <class 'tinygrad.renderer.TensorCore'>}    L: {'__qualname__': 'Program', '__annotations__': {}}    
              src:str                                                                                                                                                                                    #     renderer/__init__.py          :    20: 
              dname:str                                                                                                                                                                                  #     renderer/__init__.py          :    21: 
              uops:Optional[List[UOp]]=None                                                                                                                                                              #     renderer/__init__.py          :    22: 
              mem_estimate:sint=0  # TODO: get this from the load/store uops once min/max are good                                                                                                       #     renderer/__init__.py          :    23: 
            
              # filled in from uops (if we have uops)
              global_size:Optional[List[int]]=None                                                                                                                                                       #     renderer/__init__.py          :    26: L: {'mem_estimate': 0}    
              local_size:Optional[List[int]]=None                                                                                                                                                        #     renderer/__init__.py          :    27: 
              vars:List[Variable]=field(default_factory=list)                                                                                                                                            #     renderer/__init__.py          :    28: 
              globals:List[int]=field(default_factory=list)                                                                                                                                              #     renderer/__init__.py          :    29: L: {'vars': Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x789735decf10>,default_factory=<class 'list'>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=<dataclasses._MISSING_TYPE object at 0x789735decf10>,_field_type=None)}    
              outs:List[int]=field(default_factory=list)                                                                                                                                                 #     renderer/__init__.py          :    30: L: {'globals': Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x789735decf10>,default_factory=<class 'list'>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=<dataclasses._MISSING_TYPE object at 0x789735decf10>,_field_type=None)}    
              _ran_post_init:bool=False  # NOTE: this is needed if you call replace on the Program                                                                                                       #     renderer/__init__.py          :    31: L: {'outs': Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x789735decf10>,default_factory=<class 'list'>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=<dataclasses._MISSING_TYPE object at 0x789735decf10>,_field_type=None)}    

            class Renderer:                                                                                                                                                                              #     renderer/__init__.py          :    71: 
              device: str = ""                                                                                                                                                                           #     renderer/__init__.py          :    72: G: {'Program': <class 'tinygrad.renderer.Program'>}    L: {'__qualname__': 'Renderer', '__annotations__': {}}    
              suffix: str = ""                                                                                                                                                                           #     renderer/__init__.py          :    73: L: {'device': ''}    
              # TODO: make this generic with a list of supported types
              supports_float4: bool = True                                                                                                                                                               #     renderer/__init__.py          :    75: L: {'suffix': ''}    
              has_local: bool = True                                                                                                                                                                     #     renderer/__init__.py          :    76: L: {'supports_float4': True}    
              has_shared: bool = True                                                                                                                                                                    #     renderer/__init__.py          :    77: L: {'has_local': True}    
              # NOTE: these two should be in (x,y,z) order to match the max_sizes argument in get_grouped_dims
              global_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now                                                                                   #     renderer/__init__.py          :    79: L: {'has_shared': True}    
              local_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now                                                                                    #     renderer/__init__.py          :    80: L: {'global_max': (2415919103, 2415919103, 2415919103)}    
              shared_max: int = 32768                                                                                                                                                                    #     renderer/__init__.py          :    81: L: {'local_max': (2415919103, 2415919103, 2415919103)}    
              tensor_cores: List[TensorCore] = []                                                                                                                                                        #     renderer/__init__.py          :    82: L: {'shared_max': 32768}    
              extra_matcher: Any = None                                                                                                                                                                  #     renderer/__init__.py          :    83: L: {'tensor_cores': []}    
              code_for_op: Dict[Op, Callable] = {}                                                                                                                                                       #     renderer/__init__.py          :    84: 

        Device = _Device()                                                                                                                                                                               #     device.py                     :    40: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>, '_Device': <class 'tinygrad.device._Device'>}    

          class _Device:                                                                                                                                                                                 #     device.py                     :    13: 
            def __init__(self) -> None: self._devices: List[str] = [x.stem[len("ops_"):].upper() for x in (pathlib.Path(__file__).parent/"runtime").iterdir() if x.stem.startswith("ops_")]  # noqa: E501 #     device.py                     :    14: 

          @dataclass(frozen=True, eq=True)                                                                                                                                                               #     device.py                     :    45: 
          class BufferOptions:                                                                                                                                                                                
            image: Optional[ImageDType] = None                                                                                                                                                           #     device.py                     :    46: G: {'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    L: {'__module__': 'tinygrad.device', '__qualname__': 'BufferOptions', '__annotations__': {}}    
            uncached: bool = False                                                                                                                                                                       #     device.py                     :    47: 
            cpu_access: bool = False                                                                                                                                                                     #     device.py                     :    48: L: {'uncached': False}    
            host: bool = False                                                                                                                                                                           #     device.py                     :    49: L: {'cpu_access': False}    
            nolru: bool = False                                                                                                                                                                          #     device.py                     :    50: L: {'host': False}    

        
        MallocAllocator = _MallocAllocator()                                                                                                                                                             #     device.py                     :   170: G: {'BufferOptions': <class 'tinygrad.device.BufferOptions'>, 'Buffer': <class 'tinygrad.device.Buffer'>, 'Allocator': <class 'tinygrad.device.Allocator'>, 'LRUAllocator': <class 'tinygrad.device.LRUAllocator'>, '_MallocAllocator': <class 'tinygrad.device._MallocAllocator'>}    L: {'__name__': 'tinygrad.device', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789730fd5660>, '__spec__': ModuleSpec(name='tinygrad.device', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789730fd5660>, origin='/home/lorinbaum/code/tinygrad/tinygrad/device.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/device.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/device.cpython-310.pyc', 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'multiprocessing': <module 'multiprocessing' from '/usr/lib/python3.10/multiprocessing/__init__.py'>, 'decimal': <module 'decimal' from '/usr/lib/python3.10/decimal.py'>, 'statistics': <module 'statistics' from '/usr/lib/python3.10/statistics.py'>, 'random': <module 'random' from '/usr/lib/python3.10/random.py'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'defaultdict': <class 'collections.defaultdict'>, 'List': typing.List, 'Optional': typing.Optional, 'Dict': typing.Dict, 'Tuple': typing.Tuple, 'Any': typing.Any, 'cast': <function cast at 0x789735eb17e0>, 'Protocol': <class 'typing.Protocol'>, 'Type': typing.Type, 'importlib': <module 'importlib' from '/usr/lib/python3.10/importlib/__init__.py'>, 'inspect': <module 'inspect' from '/usr/lib/python3.10/inspect.py'>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'atexit': <module 'atexit' (built-in)>, 'time': <module 'time' (built-in)>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'array': <module 'array' (built-in)>, 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'diskcache_get': <function diskcache_get at 0x789730f5a320>, 'diskcache_put': <function diskcache_put at 0x789730f5a3b0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'flat_mv': <function flat_mv at 0x789730f5aa70>, 'from_mv': <function from_mv at 0x789730f5a680>, 'ProfileLogger': <class 'tinygrad.helpers.ProfileLogger'>, 'PROFILE': <tinygrad.helpers.ContextVar object at 0x789730f60580>, 'DType': <class 'tinygrad.dtype.DType'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'Renderer': <class 'tinygrad.renderer.Renderer'>, '_Device': <class 'tinygrad.device._Device'>, 'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    

          class LRUAllocator(Allocator):  # pylint: disable=abstract-method                                                                                                                              #     device.py                     :   143: 
            """                                                                                                                                                                                               
            The LRU Allocator is responsible for caching buffers.                                                                                                                                             
            It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.                                                                                                   
            """                                                                                                                                                                                               
            def __init__(self): self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)                                                                                           #     device.py                     :   148: 

          def hcq_command(func):                                                                                                                                                                         #     device.py                     :   200: 
            """                                                                                                                                                                                               
            Decorator for HWCommandQueue commands. Enables command indexing and stores metadata for command updates.                                                                                          
                                                                                                                                                                                                              
            For example:                                                                                                                                                                                      
              ```python                                                                                                                                                                                       
                @hcq_command                                                                                                                                                                                  
                def command_method(self, ...): ...                                                                                                                                                            
              ```                                                                                                                                                                                             
            """                                                                                                                                                                                               
            return __wrapper                                                                                                                                                                             #     device.py                     :   216: G: {'MallocAllocator': <tinygrad.device._MallocAllocator object at 0x78972db5ff10>, 'CompileError': <class 'tinygrad.device.CompileError'>, 'Compiler': <class 'tinygrad.device.Compiler'>, 'Compiled': <class 'tinygrad.device.Compiled'>, 'hcq_command': <function hcq_command at 0x7897305d2290>}    L: {'func': <function HWCommandQueue.signal at 0x78972db69b40>, '__wrapper': <function hcq_command.<locals>.__wrapper at 0x78972db69bd0>}    

          class HCQCompiled(Compiled):                                                                                                                                                                   #     device.py                     :   482: 
            """                                                                                                                                                                                               
            A base class for devices compatible with the HCQ (Hardware Command Queue) API.                                                                                                                    
            """                                                                                                                                                                                               
            """                                                                                                                                                                                          #     device.py                     :   483: G: {'HWCommandQueue': <class 'tinygrad.device.HWCommandQueue'>, 'HWComputeQueue': <class 'tinygrad.device.HWComputeQueue'>, 'HWCopyQueue': <class 'tinygrad.device.HWCopyQueue'>, 'HCQSignal': <class 'tinygrad.device.HCQSignal'>, 'hcq_profile': <function hcq_profile at 0x78972db6b010>, 'HCQArgsState': <class 'tinygrad.device.HCQArgsState'>, 'HCQProgram': <class 'tinygrad.device.HCQProgram'>}    L: {'__module__': 'tinygrad.device', '__qualname__': 'HCQCompiled', '__annotations__': {}}    
            A base class for devices compatible with the HCQ (Hardware Command Queue) API.                                                                                                                    
            """                                                                                                                                                                                               
            devices: List[HCQCompiled] = []                                                                                                                                                              #     device.py                     :   486: L: {'__doc__': '\n  A base class for devices compatible with the HCQ (Hardware Command Queue) API.\n  '}    
            gpu2cpu_copy_time_diff: decimal.Decimal = decimal.Decimal('nan')                                                                                                                             #     device.py                     :   487: L: {'devices': []}    
            gpu2cpu_compute_time_diff: decimal.Decimal = decimal.Decimal('nan')                                                                                                                          #     device.py                     :   488: L: {'gpu2cpu_copy_time_diff': Decimal('NaN')}    

          # Protocol for hcq compatible allocators for allocated buffers to contain VA address and it's size.                                                                                            #     device.py                     :   600: 
          class HCQBuffer(Protocol): va_addr:int; size:int # noqa: E702                                                                                                                                       

      from weakref import ref, ReferenceType, WeakValueDictionary                                                                                                                                        #     lazy.py                       :     9: G: {'Buffer': <class 'tinygrad.device.Buffer'>}    
      
      lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()                                                                                                                            #     lazy.py                       :    11: G: {'ref': <class 'weakref.ReferenceType'>, 'ReferenceType': <class 'weakref.ReferenceType'>, 'WeakValueDictionary': <class 'weakref.WeakValueDictionary'>}    
      
      view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}                                                                                                                   #     lazy.py                       :    25: G: {'lazycache': <WeakValueDictionary at 0x789730f62950>, 'create_lazybuffer': <function create_lazybuffer at 0x7897305d0670>}    

    from tinygrad.multi import MultiLazyBuffer                                                                                                                                                           #     tensor.py                     :    14: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    
      from __future__ import annotations                                                                                                                                                                 #     multi.py                      :     1: G: {'__name__': 'tinygrad.multi', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7897305a96c0>, '__spec__': ModuleSpec(name='tinygrad.multi', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7897305a96c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/multi.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/multi.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/multi.cpython-310.pyc'}    
      from typing import Optional, Union, Any, Tuple, List, Dict                                                                                                                                         #     multi.py                      :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
      import functools, itertools, operator                                                                                                                                                              #     multi.py                      :     3: G: {'Optional': typing.Optional, 'Union': typing.Union, 'Any': typing.Any, 'Tuple': typing.Tuple, 'List': typing.List, 'Dict': typing.Dict}    
      from tinygrad.helpers import all_same, all_int, dedup, prod, DEBUG, RING, getenv                                                                                                                   #     multi.py                      :     4: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'itertools': <module 'itertools' (built-in)>, 'operator': <module 'operator' from '/usr/lib/python3.10/operator.py'>}    
      from tinygrad.dtype import DType, ConstType                                                                                                                                                        #     multi.py                      :     5: G: {'all_same': <function all_same at 0x789730f58430>, 'all_int': <function all_int at 0x789730f584c0>, 'dedup': <function dedup at 0x789730f581f0>, 'prod': <function prod at 0x7897327a7520>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'RING': <tinygrad.helpers.ContextVar object at 0x789730f60130>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>}    
      from tinygrad.ops import BinaryOps, MetaOps, UnaryOps, TernaryOps, ReduceOps                                                                                                                       #     multi.py                      :     6: G: {'DType': <class 'tinygrad.dtype.DType'>, 'ConstType': typing.Union[float, int, bool]}    
      from tinygrad.lazy import LazyBuffer                                                                                                                                                               #     multi.py                      :     7: G: {'BinaryOps': <enum 'BinaryOps'>, 'MetaOps': <enum 'MetaOps'>, 'UnaryOps': <enum 'UnaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>}    
      from tinygrad.shape.shapetracker import sint                                                                                                                                                       #     multi.py                      :     8: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    

    from tinygrad.ops import MetaOps, truncate                                                                                                                                                           #     tensor.py                     :    15: G: {'MultiLazyBuffer': <class 'tinygrad.multi.MultiLazyBuffer'>}    
    from tinygrad.device import Device, Buffer, BufferOptions                                                                                                                                            #     tensor.py                     :    16: G: {'MetaOps': <enum 'MetaOps'>, 'truncate': {dtypes.bool: <class 'bool'>, dtypes.half: <function truncate_fp16 at 0x789730ffacb0>, dtypes.float: <function <lambda> at 0x789730ffad40>, dtypes.double: <function <lambda> at 0x789730ffadd0>, dtypes.uchar: <function <lambda> at 0x789730ffae60>, dtypes.ushort: <function <lambda> at 0x789730ffaef0>, dtypes.uint: <function <lambda> at 0x789730ffaf80>, dtypes.ulong: <function <lambda> at 0x789730ffb010>, dtypes.char: <function <lambda> at 0x789730ffb0a0>, dtypes.short: <function <lambda> at 0x789730ffb130>, dtypes.int: <function <lambda> at 0x789730ffb1c0>, dtypes.long: <function <lambda> at 0x789730ffb250>}}    
    from tinygrad.shape.symbolic import sint, Variable, MulNode, SumNode, NumNode, Node                                                                                                                  #     tensor.py                     :    17: G: {'Device': <tinygrad.device._Device object at 0x78972db5c9a0>, 'Buffer': <class 'tinygrad.device.Buffer'>, 'BufferOptions': <class 'tinygrad.device.BufferOptions'>}    
    from tinygrad.engine.realize import run_schedule, memory_planner                                                                                                                                     #     tensor.py                     :    18: G: {'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'MulNode': <class 'tinygrad.shape.symbolic.MulNode'>, 'SumNode': <class 'tinygrad.shape.symbolic.SumNode'>, 'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>, 'Node': <class 'tinygrad.shape.symbolic.Node'>}    
      from typing import List, Dict, Optional, cast, Generator, Tuple, Union                                                                                                                             #     engine/realize.py             :     1: G: {'__name__': 'tinygrad.engine.realize', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db5efe0>, '__spec__': ModuleSpec(name='tinygrad.engine.realize', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db5efe0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/realize.cpython-310.pyc'}    
      import time, pprint                                                                                                                                                                                #     engine/realize.py             :     2: G: {'__annotations__': {}, 'List': typing.List, 'Dict': typing.Dict, 'Optional': typing.Optional, 'cast': <function cast at 0x789735eb17e0>, 'Generator': typing.Generator, 'Tuple': typing.Tuple, 'Union': typing.Union}    
      from collections import defaultdict                                                                                                                                                                #     engine/realize.py             :     3: G: {'time': <module 'time' (built-in)>, 'pprint': <module 'pprint' from '/usr/lib/python3.10/pprint.py'>}    
      from dataclasses import dataclass, replace                                                                                                                                                         #     engine/realize.py             :     4: G: {'defaultdict': <class 'collections.defaultdict'>}    
      from tinygrad.helpers import colored, getenv, DEBUG, GlobalCounters, ansilen, BEAM, NOOPT, all_int, CAPTURING, Metadata, Context, TRACEMETA, dedup                                                 #     engine/realize.py             :     5: G: {'dataclass': <function dataclass at 0x789735c341f0>, 'replace': <function replace at 0x789735c34700>}    
      from tinygrad.ops import MetaOps, UOps, UOp                                                                                                                                                        #     engine/realize.py             :     6: G: {'colored': <function colored at 0x789730f58550>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'ansilen': <function ansilen at 0x789730f58790>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'NOOPT': <tinygrad.helpers.ContextVar object at 0x789730f3be50>, 'all_int': <function all_int at 0x789730f584c0>, 'CAPTURING': <tinygrad.helpers.ContextVar object at 0x789730f60280>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'Context': <class 'tinygrad.helpers.Context'>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>, 'dedup': <function dedup at 0x789730f581f0>}    
      from tinygrad.dtype import dtypes                                                                                                                                                                  #     engine/realize.py             :     7: G: {'MetaOps': <enum 'MetaOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>}    
      from tinygrad.device import Device, Buffer                                                                                                                                                         #     engine/realize.py             :     8: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>}    
      from tinygrad.shape.symbolic import Variable, sym_infer, sint                                                                                                                                      #     engine/realize.py             :     9: G: {'Device': <tinygrad.device._Device object at 0x78972db5c9a0>, 'Buffer': <class 'tinygrad.device.Buffer'>}    
      from tinygrad.renderer import Renderer, Program                                                                                                                                                    #     engine/realize.py             :    10: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sym_infer': <function sym_infer at 0x789730fe70a0>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    
      from tinygrad.codegen.kernel import Kernel                                                                                                                                                         #     engine/realize.py             :    11: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>, 'Program': <class 'tinygrad.renderer.Program'>}    
        from __future__ import annotations                                                                                                                                                               #     codegen/kernel.py             :     1: G: {'__name__': 'tinygrad.codegen.kernel', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db9c160>, '__spec__': ModuleSpec(name='tinygrad.codegen.kernel', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db9c160>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/kernel.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/kernel.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/kernel.cpython-310.pyc'}    
        import itertools, functools                                                                                                                                                                      #     codegen/kernel.py             :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
        from dataclasses import dataclass, replace                                                                                                                                                       #     codegen/kernel.py             :     3: G: {'itertools': <module 'itertools' (built-in)>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
        from collections import defaultdict                                                                                                                                                              #     codegen/kernel.py             :     4: G: {'dataclass': <function dataclass at 0x789735c341f0>, 'replace': <function replace at 0x789735c34700>}    
        from typing import Literal, Optional, List, Tuple, Union, cast, Dict, Final, DefaultDict                                                                                                         #     codegen/kernel.py             :     5: G: {'defaultdict': <class 'collections.defaultdict'>}    
        
        from tinygrad.ops import BinaryOps, ReduceOps, UNSAFE_PAD_OPS, KernelInfo, BUFFER_UOPS, UOp, UOps, print_uops, type_verify                                                                       #     codegen/kernel.py             :     7: G: {'Literal': typing.Literal, 'Optional': typing.Optional, 'List': typing.List, 'Tuple': typing.Tuple, 'Union': typing.Union, 'cast': <function cast at 0x789735eb17e0>, 'Dict': typing.Dict, 'Final': typing.Final, 'DefaultDict': typing.DefaultDict}    
        from tinygrad.device import Device                                                                                                                                                               #     codegen/kernel.py             :     8: G: {'BinaryOps': <enum 'BinaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}, 'KernelInfo': <class 'tinygrad.ops.KernelInfo'>, 'BUFFER_UOPS': {<UOps.CONST: 10>, <UOps.LOAD: 21>, <UOps.STORE: 22>}, 'UOp': <class 'tinygrad.ops.UOp'>, 'UOps': <enum 'UOps'>, 'print_uops': <function print_uops at 0x789730ffb490>, 'type_verify': <function type_verify at 0x789730ffb400>}    
        from tinygrad.renderer import Renderer, TensorCore, Program                                                                                                                                      #     codegen/kernel.py             :     9: G: {'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    
        from tinygrad.dtype import DType, ImageDType, PtrDType                                                                                                                                           #     codegen/kernel.py             :    10: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>, 'TensorCore': <class 'tinygrad.renderer.TensorCore'>, 'Program': <class 'tinygrad.renderer.Program'>}    
        from tinygrad.helpers import all_same, colored, ansilen, dedup, getenv, prod, DEBUG, TC_OPT, USE_TC, round_up, all_int, \                                                                        #     codegen/kernel.py             :    11: G: {'DType': <class 'tinygrad.dtype.DType'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>}    
                                     get_contraction, to_function_name, diskcache_put, ContextVar                                                                                                             
        from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                             #     codegen/kernel.py             :    13: G: {'all_same': <function all_same at 0x789730f58430>, 'colored': <function colored at 0x789730f58550>, 'ansilen': <function ansilen at 0x789730f58790>, 'dedup': <function dedup at 0x789730f581f0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'prod': <function prod at 0x7897327a7520>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'TC_OPT': <tinygrad.helpers.ContextVar object at 0x789730f60760>, 'USE_TC': <tinygrad.helpers.ContextVar object at 0x789730f606a0>, 'round_up': <function round_up at 0x789730f58af0>, 'all_int': <function all_int at 0x789730f584c0>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'to_function_name': <functools._lru_cache_wrapper object at 0x789732849170>, 'diskcache_put': <function diskcache_put at 0x789730f5a3b0>, 'ContextVar': <class 'tinygrad.helpers.ContextVar'>}    
        from tinygrad.shape.symbolic import Variable, sint                                                                                                                                               #     codegen/kernel.py             :    14: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>}    
        from tinygrad.shape.view import strides_for_shape                                                                                                                                                #     codegen/kernel.py             :    15: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    
        from tinygrad.codegen.uopgraph import linearize_uop, full_graph_rewrite                                                                                                                          #     codegen/kernel.py             :    16: G: {'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>}    
        from tinygrad.codegen.lowerer import ast_to_uop                                                                                                                                                  #     codegen/kernel.py             :    17: G: {'linearize_uop': <function linearize_uop at 0x78973059ec20>, 'full_graph_rewrite': <function full_graph_rewrite at 0x78973059eb90>}    
          from __future__ import annotations                                                                                                                                                             #     codegen/lowerer.py            :     1: G: {'__name__': 'tinygrad.codegen.lowerer', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db9da20>, '__spec__': ModuleSpec(name='tinygrad.codegen.lowerer', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db9da20>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/lowerer.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/lowerer.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/lowerer.cpython-310.pyc'}    
          import functools                                                                                                                                                                               #     codegen/lowerer.py            :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
          from typing import List, Tuple, cast, Optional, Dict                                                                                                                                           #     codegen/lowerer.py            :     3: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>}    
          from tinygrad.shape.shapetracker import ShapeTracker, variable_to_uop                                                                                                                          #     codegen/lowerer.py            :     4: G: {'List': typing.List, 'Tuple': typing.Tuple, 'cast': <function cast at 0x789735eb17e0>, 'Optional': typing.Optional, 'Dict': typing.Dict}    
          from tinygrad.shape.symbolic import sint                                                                                                                                                       #     codegen/lowerer.py            :     5: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>, 'variable_to_uop': <function variable_to_uop at 0x789730ffbd00>}    
          from tinygrad.dtype import dtypes, DType                                                                                                                                                       #     codegen/lowerer.py            :     6: G: {'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    
          from tinygrad.ops import ReduceOps, KernelInfo, BinaryOps, BUFFER_UOPS, UOp, UOps                                                                                                              #     codegen/lowerer.py            :     7: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>}    
          from tinygrad.renderer import Renderer                                                                                                                                                         #     codegen/lowerer.py            :     8: G: {'ReduceOps': <enum 'ReduceOps'>, 'KernelInfo': <class 'tinygrad.ops.KernelInfo'>, 'BinaryOps': <enum 'BinaryOps'>, 'BUFFER_UOPS': {<UOps.CONST: 10>, <UOps.LOAD: 21>, <UOps.STORE: 22>}, 'UOp': <class 'tinygrad.ops.UOp'>, 'UOps': <enum 'UOps'>}    
          from tinygrad.helpers import all_int, get_contraction, prod, partition, flatten                                                                                                                #     codegen/lowerer.py            :     9: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>}    

        from enum import Enum, auto                                                                                                                                                                      #     codegen/kernel.py             :    18: G: {'ast_to_uop': <function ast_to_uop at 0x78972dbabbe0>}    

          class OptOps(Enum):                                                                                                                                                                            #     codegen/kernel.py             :    20: 
            TC = auto(); UPCAST = auto(); UPCASTMID = auto(); UNROLL = auto(); LOCAL = auto() # noqa: E702                                                                                               #     codegen/kernel.py             :    21: G: {'Enum': <enum 'Enum'>, 'auto': <class 'enum.auto'>}    L: {'_generate_next_value_': <function Enum._generate_next_value_ at 0x789735e41870>, '__module__': 'tinygrad.codegen.kernel', '__qualname__': 'OptOps'}    
            GROUP = auto(); GROUPTOP = auto(); NOLOCALS = auto(); PADTO = auto(); SWAP = auto() # noqa: E702                                                                                             #     codegen/kernel.py             :    22: L: {'TC': 1, 'UPCAST': 2, 'UPCASTMID': 3, 'UNROLL': 4, 'LOCAL': 5}    

          @dataclass(frozen=True, order=True)                                                                                                                                                            #     codegen/kernel.py             :    31: 
          class Opt:                                                                                                                                                                                          
            op: OptOps                                                                                                                                                                                   #     codegen/kernel.py             :    32: G: {'OptOps': <enum 'OptOps'>, 'KernelOptError': <class 'tinygrad.codegen.kernel.KernelOptError'>, 'check': <function check at 0x78972dbab400>}    L: {'__qualname__': 'Opt', '__annotations__': {}}    
            axis: Optional[int] = None                                                                                                                                                                   #     codegen/kernel.py             :    33: 
            amt: Optional[int] = None                                                                                                                                                                    #     codegen/kernel.py             :    34: 

          @dataclass                                                                                                                                                                                     #     codegen/kernel.py             :    43: 
          class TensorCoreOptions:                                                                                                                                                                            
            axes: Tuple[int, ...] # the location of the original N and M axes if still in the shape                                                                                                      #     codegen/kernel.py             :    44: G: {'Opt': <class 'tinygrad.codegen.kernel.Opt'>}    L: {'__qualname__': 'TensorCoreOptions', '__annotations__': {}}    
            axes_exist: Tuple[bool, ...] # true if the original N and M axes are still in the shape                                                                                                      #     codegen/kernel.py             :    45: 
            axis_pads: Tuple[Tuple[int, int], ...]                                                                                                                                                       #     codegen/kernel.py             :    46: 

          class Kernel:                                                                                                                                                                                  #     codegen/kernel.py             :    54: 
          
            # **** kernel outputs ****
          
            kernel_cnt: Final[DefaultDict[str, int]] = defaultdict(int)                                                                                                                                  #     codegen/kernel.py             :   619: G: {'TensorCoreOptions': <class 'tinygrad.codegen.kernel.TensorCoreOptions'>}    L: {'__qualname__': 'Kernel', '__annotations__': {}, '__init__': <function Kernel.__init__ at 0x789727100820>, 'copy': <function Kernel.copy at 0x789727100940>, 'membufs': <property object at 0x78972dbd7380>, 'float4_axis': <function Kernel.float4_axis at 0x789727100a60>, 'upcasted_axis': <function Kernel.upcasted_axis at 0x789727100af0>, 'first_reduce': <property object at 0x78972dbd7330>, 'first_upcast': <property object at 0x78972dbd7240>, 'reduceop': <property object at 0x78972dbd7290>, 'output_shape': <property object at 0x78972dbd72e0>, 'full_shape': <property object at 0x78972dbd7420>, 'full_unupcasted_shape': <property object at 0x78972dbd73d0>, 'shape_len': <property object at 0x78972dbd7470>, 'upcast_in_mid_reduce_axes': <property object at 0x78972dbd74c0>, 'global_dims': <property object at 0x78972dbd7510>, 'colors': <function Kernel.colors at 0x789727101090>, 'colored_shape': <function Kernel.colored_shape at 0x789727101120>, 'reshape_and_permute': <function Kernel.reshape_and_permute at 0x7897271011b0>, 'upcast': <function Kernel.upcast at 0x7897271012d0>, 'shift_to': <function Kernel.shift_to at 0x789727101240>, 'simplify_ones': <function Kernel.simplify_ones at 0x789727101360>, 'simplify_merge_adjacent': <function Kernel.simplify_merge_adjacent at 0x7897271013f0>, '_create_tc_opts': <function Kernel._create_tc_opts at 0x789727101480>, '_apply_tc_opt': <function Kernel._apply_tc_opt at 0x789727101510>, 'apply_tensor_cores': <function Kernel.apply_tensor_cores at 0x7897271015a0>, 'apply_opt': <function Kernel.apply_opt at 0x789727101630>, 'required_optimizations': <function Kernel.required_optimizations at 0x7897271016c0>, 'hand_coded_optimizations': <function Kernel.hand_coded_optimizations at 0x789727101750>}    

      from tinygrad.engine.schedule import ScheduleItem                                                                                                                                                  #     engine/realize.py             :    12: G: {'Kernel': <class 'tinygrad.codegen.kernel.Kernel'>}    
        import sys, pickle, atexit, importlib, contextlib                                                                                                                                                #     engine/schedule.py            :     1: G: {'__name__': 'tinygrad.engine.schedule', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db9c550>, '__spec__': ModuleSpec(name='tinygrad.engine.schedule', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db9c550>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/schedule.cpython-310.pyc'}    
        from collections import defaultdict, deque                                                                                                                                                       #     engine/schedule.py            :     2: G: {'__annotations__': {}, 'sys': <module 'sys' (built-in)>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'atexit': <module 'atexit' (built-in)>, 'importlib': <module 'importlib' from '/usr/lib/python3.10/importlib/__init__.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>}    
        from dataclasses import dataclass, field                                                                                                                                                         #     engine/schedule.py            :     3: G: {'defaultdict': <class 'collections.defaultdict'>, 'deque': <class 'collections.deque'>}    
        from typing import Tuple, List, Dict, Optional, Set, DefaultDict, get_args                                                                                                                       #     engine/schedule.py            :     4: G: {'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>}    
        from tinygrad.ops import MetaOps, ReduceOps, UNSAFE_PAD_OPS, UnaryOps, UOp, UOps                                                                                                                 #     engine/schedule.py            :     5: G: {'Tuple': typing.Tuple, 'List': typing.List, 'Dict': typing.Dict, 'Optional': typing.Optional, 'Set': typing.Set, 'DefaultDict': typing.DefaultDict, 'get_args': <function get_args at 0x789735eb1ab0>}    
        from tinygrad.engine.graph import log_lazybuffer, realized_lazybuffer                                                                                                                            #     engine/schedule.py            :     6: G: {'MetaOps': <enum 'MetaOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}, 'UnaryOps': <enum 'UnaryOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'UOps': <enum 'UOps'>}    
          import os, atexit, functools, contextlib                                                                                                                                                       #     engine/graph.py               :     1: G: {'__name__': 'tinygrad.engine.graph', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db9f3a0>, '__spec__': ModuleSpec(name='tinygrad.engine.graph', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db9f3a0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/graph.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/graph.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/graph.cpython-310.pyc'}    
          from collections import defaultdict                                                                                                                                                            #     engine/graph.py               :     2: G: {'__annotations__': {}, 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'atexit': <module 'atexit' (built-in)>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>}    
          from typing import List, Any, DefaultDict                                                                                                                                                      #     engine/graph.py               :     3: G: {'defaultdict': <class 'collections.defaultdict'>}    
          from tinygrad.ops import UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps, UOps, UOp                                                                                                        #     engine/graph.py               :     4: G: {'List': typing.List, 'Any': typing.Any, 'DefaultDict': typing.DefaultDict}    
          from tinygrad.device import Device                                                                                                                                                             #     engine/graph.py               :     5: G: {'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'MetaOps': <enum 'MetaOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>}    
          from tinygrad.helpers import GRAPHPATH, DEBUG, GlobalCounters                                                                                                                                  #     engine/graph.py               :     6: G: {'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    
          from tinygrad.shape.symbolic import NumNode                                                                                                                                                    #     engine/graph.py               :     7: G: {'GRAPHPATH': '/tmp/net', 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>}    
          from tinygrad.lazy import LazyBuffer                                                                                                                                                           #     engine/graph.py               :     8: G: {'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>}    
          
          with contextlib.suppress(ImportError): import networkx as nx                                                                                                                                   #     engine/graph.py               :    10: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    
          if DEBUG >= 2: atexit.register(print_globalcounters)                                                                                                                                           #     engine/graph.py               :    18: G: {'nx': <module 'networkx' from '/home/lorinbaum/.local/lib/python3.10/site-packages/networkx/__init__.py'>, 'print_globalcounters': <function print_globalcounters at 0x789727103520>}    

            class ContextVar:                                                                                                                                                                            #     helpers.py                    :    93: 
              def __ge__(self, x): return self.value >= x                                                                                                                                                #     helpers.py                    :   103: 

          
          G:Any = None                                                                                                                                                                                   #     engine/graph.py               :    25: G: {'save_graph': <function save_graph at 0x789727103640>}    
          
          counts: DefaultDict[type, int] = defaultdict(int)                                                                                                                                              #     engine/graph.py               :    32: G: {'G': None, 'init_graph': <function init_graph at 0x78971f386200>}    
          
          top_colors = {MetaOps: '#FFFFa0', UnaryOps: "#c0c0c0", ReduceOps: "#FFA0A0", BinaryOps: "#c0c0c0", TernaryOps: "#c0c0c0"}                                                                      #     engine/graph.py               :    45: G: {'counts': defaultdict(<class 'int'>, {}), 'nm': <function nm at 0x78971f386290>, 'realized_lazybuffer': <function realized_lazybuffer at 0x78971f3a6170>}    
          
          graph_uops_cnt = 0                                                                                                                                                                             #     engine/graph.py               :    74: G: {'top_colors': {<enum 'MetaOps'>: '#FFFFa0', <enum 'UnaryOps'>: '#c0c0c0', <enum 'ReduceOps'>: '#FFA0A0', <enum 'BinaryOps'>: '#c0c0c0', <enum 'TernaryOps'>: '#c0c0c0'}, 'log_lazybuffer': <function log_lazybuffer at 0x78971f3a6200>}    

        from tinygrad.helpers import GRAPH, DEBUG, MULTIOUTPUT, SAVE_SCHEDULE, FUSE_CONV_BW, FUSE_ARANGE, \                                                                                              #     engine/schedule.py            :     7: G: {'log_lazybuffer': <function log_lazybuffer at 0x78971f3a6200>, 'realized_lazybuffer': <function realized_lazybuffer at 0x78971f3a6170>}    
                                     GlobalCounters, colored, prod, dedup, all_int, merge_dicts, getenv, Metadata                                                                                             
        from tinygrad.shape.symbolic import Variable, sint                                                                                                                                               #     engine/schedule.py            :     9: G: {'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'MULTIOUTPUT': <tinygrad.helpers.ContextVar object at 0x789730f60430>, 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'FUSE_CONV_BW': <tinygrad.helpers.ContextVar object at 0x789730f604c0>, 'FUSE_ARANGE': <tinygrad.helpers.ContextVar object at 0x789730f60490>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'colored': <function colored at 0x789730f58550>, 'prod': <function prod at 0x7897327a7520>, 'dedup': <function dedup at 0x789730f581f0>, 'all_int': <function all_int at 0x789730f584c0>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'Metadata': <class 'tinygrad.helpers.Metadata'>}    
        from tinygrad.dtype import ConstType, ImageDType, PtrDType, dtypes                                                                                                                               #     engine/schedule.py            :    10: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode]}    
        from tinygrad.lazy import LazyBuffer                                                                                                                                                             #     engine/schedule.py            :    11: G: {'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>}    
        from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                             #     engine/schedule.py            :    12: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    
        from tinygrad.device import Buffer                                                                                                                                                               #     engine/schedule.py            :    13: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>}    
        from tinygrad.shape.view import View, strides_for_shape                                                                                                                                          #     engine/schedule.py            :    14: G: {'Buffer': <class 'tinygrad.device.Buffer'>}    
        
        # creation can recurse a lot
        sys.setrecursionlimit(10000)                                                                                                                                                                     #     engine/schedule.py            :    17: G: {'View': <class 'tinygrad.shape.view.View'>, 'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>}    
        
        # optionally log the ops to disk
        logops = open(getenv("LOGOPS", ""), "a") if getenv("LOGOPS", "") else None                                                                                                                       #     engine/schedule.py            :    20: 

          @dataclass(frozen=True)                                                                                                                                                                        #     engine/schedule.py            :    25: 
          class ScheduleItem:                                                                                                                                                                                 
            ast: UOp                                                                                                                                                                                     #     engine/schedule.py            :    26: G: {'logops': None}    L: {'__module__': 'tinygrad.engine.schedule', '__qualname__': 'ScheduleItem', '__annotations__': {}}    
            bufs: Tuple[Buffer, ...]                                                                                                                                                                     #     engine/schedule.py            :    27: 
            metadata: Optional[List[Metadata]] = None                                                                                                                                                    #     engine/schedule.py            :    28: 

          @dataclass(frozen=True)                                                                                                                                                                        #     engine/schedule.py            :    39: 
          class LBScheduleItem:                                                                                                                                                                               
            ast: UOp                                                                                                                                                                                     #     engine/schedule.py            :    40: G: {'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>}    L: {'__qualname__': 'LBScheduleItem', '__annotations__': {}}    
            outputs: List[LazyBuffer]                                                                                                                                                                    #     engine/schedule.py            :    41: 
            inputs: List[LazyBuffer]                                                                                                                                                                     #     engine/schedule.py            :    42: 
            var_vals: Dict[Variable, int] = field(default_factory=dict)                                                                                                                                  #     engine/schedule.py            :    43: 
            metadata: List[Metadata] = field(default_factory=list)                                                                                                                                       #     engine/schedule.py            :    44: L: {'var_vals': Field(name=None,type=None,default=<dataclasses._MISSING_TYPE object at 0x789735decf10>,default_factory=<class 'dict'>,init=True,repr=True,hash=None,compare=True,metadata=mappingproxy({}),kw_only=<dataclasses._MISSING_TYPE object at 0x789735decf10>,_field_type=None)}    

        
        SCHEDULES: List[Tuple[DefaultDict[LBScheduleItem, List[LBScheduleItem]], DefaultDict[LBScheduleItem, int]]] = []                                                                                 #     engine/schedule.py            :   262: G: {'LBScheduleItem': <class 'tinygrad.engine.schedule.LBScheduleItem'>, '_recursive_uop': <function _recursive_uop at 0x7897271032e0>, '_permute_reduce': <function _permute_reduce at 0x78971f3a6b90>, '_recurse_reduceops': <function _recurse_reduceops at 0x78971f3a6cb0>, '_lower_lazybuffer': <function _lower_lazybuffer at 0x78971f3a6e60>, '_recurse_lb': <function _recurse_lb at 0x78971f3a6ef0>, '_is_padding_okay': <function _is_padding_okay at 0x78971f3a6f80>, '_recursive_group': <function _recursive_group at 0x78971f3a7010>, '_get_isolated_children': <function _get_isolated_children at 0x78971f3a70a0>}    L: {'__name__': 'tinygrad.engine.schedule', '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db9c550>, '__spec__': ModuleSpec(name='tinygrad.engine.schedule', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db9c550>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/schedule.cpython-310.pyc', '__annotations__': {}, 'sys': <module 'sys' (built-in)>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'atexit': <module 'atexit' (built-in)>, 'importlib': <module 'importlib' from '/usr/lib/python3.10/importlib/__init__.py'>, 'contextlib': <module 'contextlib' from '/usr/lib/python3.10/contextlib.py'>, 'defaultdict': <class 'collections.defaultdict'>, 'deque': <class 'collections.deque'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'field': <function field at 0x789735c26dd0>, 'Tuple': typing.Tuple, 'List': typing.List, 'Dict': typing.Dict, 'Optional': typing.Optional, 'Set': typing.Set, 'DefaultDict': typing.DefaultDict, 'get_args': <function get_args at 0x789735eb1ab0>, 'MetaOps': <enum 'MetaOps'>, 'ReduceOps': <enum 'ReduceOps'>, 'UNSAFE_PAD_OPS': {<UnaryOps.EXP2: 1>, <BinaryOps.IDIV: 3>, <UnaryOps.RECIP: 7>, <UnaryOps.LOG2: 2>}, 'UnaryOps': <enum 'UnaryOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'UOps': <enum 'UOps'>, 'log_lazybuffer': <function log_lazybuffer at 0x78971f3a6200>, 'realized_lazybuffer': <function realized_lazybuffer at 0x78971f3a6170>, 'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'MULTIOUTPUT': <tinygrad.helpers.ContextVar object at 0x789730f60430>, 'SAVE_SCHEDULE': <tinygrad.helpers.ContextVar object at 0x789730f60460>, 'FUSE_CONV_BW': <tinygrad.helpers.ContextVar object at 0x789730f604c0>, 'FUSE_ARANGE': <tinygrad.helpers.ContextVar object at 0x789730f60490>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'colored': <function colored at 0x789730f58550>, 'prod': <function prod at 0x7897327a7520>, 'dedup': <function dedup at 0x789730f581f0>, 'all_int': <function all_int at 0x789730f584c0>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'ConstType': typing.Union[float, int, bool], 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>, 'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>, 'Buffer': <class 'tinygrad.device.Buffer'>, 'View': <class 'tinygrad.shape.view.View'>, 'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>, 'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>}    

      
      # **************** Program Creation ****************
      
      logkerns, logkerns_level = open(getenv("LOGKERNS", ""), "a") if getenv("LOGKERNS", "") else None, getenv("LOGKERNS_LEVEL", 1)                                                                      #     engine/realize.py             :    16: G: {'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>}    
      
      # **************** method cache ****************
      
      method_cache: Dict[Tuple[str, bytes, int, int, bool], CompiledRunner] = {}                                                                                                                         #     engine/realize.py             :   149: G: {'logkerns': None, 'logkerns_level': 1, 'get_kernel': <function get_kernel at 0x78972db6f400>, 'Runner': <class 'tinygrad.engine.realize.Runner'>, 'CompiledRunner': <class 'tinygrad.engine.realize.CompiledRunner'>, 'CustomOp': <class 'tinygrad.engine.realize.CustomOp'>, 'EmptyOp': <class 'tinygrad.engine.realize.EmptyOp'>, 'ViewOp': <class 'tinygrad.engine.realize.ViewOp'>, 'BufferCopy': <class 'tinygrad.engine.realize.BufferCopy'>, 'BufferXfer': <class 'tinygrad.engine.realize.BufferXfer'>}    

        @dataclass(frozen=True)                                                                                                                                                                          #     engine/realize.py             :   167: 
        class ExecItem:                                                                                                                                                                                       
          prg: Runner                                                                                                                                                                                    #     engine/realize.py             :   168: G: {'method_cache': {}, 'get_runner': <function get_runner at 0x789727101bd0>}    L: {'__module__': 'tinygrad.engine.realize', '__qualname__': 'ExecItem', '__annotations__': {}}    
          bufs: List[Optional[Buffer]]                                                                                                                                                                   #     engine/realize.py             :   169: 
          metadata: Optional[List[Metadata]] = None                                                                                                                                                      #     engine/realize.py             :   170: 

      
      # **************** main run function ****************
      
      capturing: List = []  # put classes with an add method in here                                                                                                                                     #     engine/realize.py             :   218: G: {'ExecItem': <class 'tinygrad.engine.realize.ExecItem'>, 'lower_schedule_item': <function lower_schedule_item at 0x78971f3a7be0>, 'lower_schedule': <function lower_schedule at 0x78971f3a7eb0>}    L: {'__name__': 'tinygrad.engine.realize', '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db5efe0>, '__spec__': ModuleSpec(name='tinygrad.engine.realize', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db5efe0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/realize.cpython-310.pyc', '__annotations__': {'method_cache': typing.Dict[typing.Tuple[str, bytes, int, int, bool], tinygrad.engine.realize.CompiledRunner]}, 'List': typing.List, 'Dict': typing.Dict, 'Optional': typing.Optional, 'cast': <function cast at 0x789735eb17e0>, 'Generator': typing.Generator, 'Tuple': typing.Tuple, 'Union': typing.Union, 'time': <module 'time' (built-in)>, 'pprint': <module 'pprint' from '/usr/lib/python3.10/pprint.py'>, 'defaultdict': <class 'collections.defaultdict'>, 'dataclass': <function dataclass at 0x789735c341f0>, 'replace': <function replace at 0x789735c34700>, 'colored': <function colored at 0x789730f58550>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'ansilen': <function ansilen at 0x789730f58790>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'NOOPT': <tinygrad.helpers.ContextVar object at 0x789730f3be50>, 'all_int': <function all_int at 0x789730f584c0>, 'CAPTURING': <tinygrad.helpers.ContextVar object at 0x789730f60280>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'Context': <class 'tinygrad.helpers.Context'>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>, 'dedup': <function dedup at 0x789730f581f0>, 'MetaOps': <enum 'MetaOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'Device': <tinygrad.device._Device object at 0x78972db5c9a0>, 'Buffer': <class 'tinygrad.device.Buffer'>, 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sym_infer': <function sym_infer at 0x789730fe70a0>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Renderer': <class 'tinygrad.renderer.Renderer'>, 'Program': <class 'tinygrad.renderer.Program'>, 'Kernel': <class 'tinygrad.codegen.kernel.Kernel'>, 'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>, 'logkerns_level': 1, 'get_kernel': <function get_kernel at 0x78972db6f400>, 'Runner': <class 'tinygrad.engine.realize.Runner'>, 'CompiledRunner': <class 'tinygrad.engine.realize.CompiledRunner'>, 'CustomOp': <class 'tinygrad.engine.realize.CustomOp'>, 'EmptyOp': <class 'tinygrad.engine.realize.EmptyOp'>, 'ViewOp': <class 'tinygrad.engine.realize.ViewOp'>, 'BufferCopy': <class 'tinygrad.engine.realize.BufferCopy'>, 'BufferXfer': <class 'tinygrad.engine.realize.BufferXfer'>, 'method_cache': {}, 'get_runner': <function get_runner at 0x789727101bd0>}    

    from tinygrad.engine.schedule import ScheduleItem, create_schedule_with_vars                                                                                                                         #     tensor.py                     :    19: G: {'run_schedule': <function run_schedule at 0x78971f1c81f0>, 'memory_planner': <function memory_planner at 0x78971f1c8310>}    
    
    import tinygrad.function as F                                                                                                                                                                        #     tensor.py                     :    42: G: {'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>, 'create_schedule_with_vars': <function create_schedule_with_vars at 0x78971f3a71c0>, 'Function': <class 'tinygrad.tensor.Function'>}    
      """This is where the forwards and backwards passes live."""                                                                                                                                        #     function.py                   :     1: G: {'__name__': 'tinygrad.function', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78972db5dc60>, '__spec__': ModuleSpec(name='tinygrad.function', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78972db5dc60>, origin='/home/lorinbaum/code/tinygrad/tinygrad/function.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/function.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/function.cpython-310.pyc'}    
      import math                                                                                                                                                                                        #     function.py                   :     2: G: {'__doc__': 'This is where the forwards and backwards passes live.'}    
      from typing import Tuple, Optional                                                                                                                                                                 #     function.py                   :     3: G: {'math': <module 'math' (built-in)>}    
      from tinygrad.helpers import argsort                                                                                                                                                               #     function.py                   :     4: G: {'Tuple': typing.Tuple, 'Optional': typing.Optional}    
      from tinygrad.dtype import dtypes, DType, sum_acc_dtype                                                                                                                                            #     function.py                   :     5: G: {'argsort': <function argsort at 0x789730f583a0>}    
      from tinygrad.ops import UnaryOps, BinaryOps, TernaryOps, ReduceOps                                                                                                                                #     function.py                   :     6: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'sum_acc_dtype': <function sum_acc_dtype at 0x789730f70940>}    
      from tinygrad.tensor import Function                                                                                                                                                               #     function.py                   :     7: G: {'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'ReduceOps': <enum 'ReduceOps'>}    
      from tinygrad.lazy import LazyBuffer                                                                                                                                                               #     function.py                   :     8: G: {'Function': <class 'tinygrad.tensor.Function'>}    
      from tinygrad.shape.symbolic import sint                                                                                                                                                           #     function.py                   :     9: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    

      class Tensor:                                                                                                                                                                                      #     tensor.py                     :    92: 
        """                                                                                                                                                                                                   
        A `Tensor` is a multi-dimensional matrix containing elements of a single data type.                                                                                                                   
                                                                                                                                                                                                              
        ```python exec="true" session="tensor"                                                                                                                                                                
        from tinygrad import Tensor, dtypes, nn                                                                                                                                                               
        import numpy as np                                                                                                                                                                                    
        import math                                                                                                                                                                                           
        np.set_printoptions(precision=4)                                                                                                                                                                      
        ```                                                                                                                                                                                                   
        """                                                                                                                                                                                                   
        """                                                                                                                                                                                              #     tensor.py                     :    93: G: {'F': <module 'tinygrad.function' from '/home/lorinbaum/code/tinygrad/tinygrad/function.py'>, '_metaop': <function _metaop at 0x789735dc31c0>, '_from_np_dtype': <function _from_np_dtype at 0x78971f1c8ee0>, '_to_np_dtype': <function _to_np_dtype at 0x78971f1cad40>, '_fromnp': <function _fromnp at 0x78971f1cadd0>, '_frompy': <function _frompy at 0x78971f1cae60>, '_get_winograd_matcols': <function _get_winograd_matcols at 0x78971f1caef0>, '_apply_winograd_matrix': <function _apply_winograd_matrix at 0x78971f1caf80>, '_pad_left': <function _pad_left at 0x78971f1cb010>, '_broadcast_shape': <function _broadcast_shape at 0x78971f1cb0a0>}    L: {'__module__': 'tinygrad.tensor', '__qualname__': 'Tensor', '__annotations__': {}}    
        A `Tensor` is a multi-dimensional matrix containing elements of a single data type.                                                                                                                   
                                                                                                                                                                                                              
        ```python exec="true" session="tensor"                                                                                                                                                                
        from tinygrad import Tensor, dtypes, nn                                                                                                                                                               
        import numpy as np                                                                                                                                                                                    
        import math                                                                                                                                                                                           
        np.set_printoptions(precision=4)                                                                                                                                                                      
        ```                                                                                                                                                                                                   
        """                                                                                                                                                                                                   
        __slots__ = "lazydata", "requires_grad", "grad", "_ctx"                                                                                                                                          #     tensor.py                     :   103: L: {'__doc__': '\n  A `Tensor` is a multi-dimensional matrix containing elements of a single data type.\n\n  ```python exec="true" session="tensor"\n  from tinygrad import Tensor, dtypes, nn\n  import numpy as np\n  import math\n  np.set_printoptions(precision=4)\n  ```\n  '}    
        __deletable__ = ('_ctx',)                                                                                                                                                                        #     tensor.py                     :   104: L: {'__slots__': ('lazydata', 'requires_grad', 'grad', '_ctx')}    
        training: ClassVar[bool] = False                                                                                                                                                                 #     tensor.py                     :   105: L: {'__deletable__': ('_ctx',)}    
        no_grad: ClassVar[bool] = False                                                                                                                                                                  #     tensor.py                     :   106: L: {'training': False}    
      
        _seed: int = int(time.time())                                                                                                                                                                    #     tensor.py                     :   390: L: {'no_grad': False, '__init__': <function Tensor.__init__ at 0x78971f1cb1c0>, 'train': <class 'tinygrad.tensor.Tensor.train'>, 'test': <class 'tinygrad.tensor.Tensor.test'>, '__repr__': <function Tensor.__repr__ at 0x78971f1cb250>, '__hash__': <function Tensor.__hash__ at 0x78971f1cb640>, '__bool__': <function Tensor.__bool__ at 0x78971f1cb6d0>, '__len__': <function Tensor.__len__ at 0x78971f1cb760>, 'device': <property object at 0x78971f3af600>, 'shape': <property object at 0x78971f1da3e0>, 'dtype': <property object at 0x78971f1da390>, 'schedule_with_vars': <function Tensor.schedule_with_vars at 0x78971f1cb9a0>, 'schedule': <function Tensor.schedule at 0x78971f1cba30>, 'realize': <function Tensor.realize at 0x78971f1cbb50>, 'replace': <function Tensor.replace at 0x78971f1cbac0>, 'assign': <function Tensor.assign at 0x78971f1cbbe0>, 'detach': <function Tensor.detach at 0x78971f1cbc70>, '_data': <function Tensor._data at 0x78971f1cbd00>, 'data': <function Tensor.data at 0x78971f1cbd90>, 'item': <function Tensor.item at 0x78971f1cbe20>, 'tolist': <function Tensor.tolist at 0x78971f1cbeb0>, 'numpy': <function Tensor.numpy at 0x78971f1cbf40>, 'to': <function Tensor.to at 0x78971f1ec040>, 'to_': <function Tensor.to_ at 0x78971f1ec0d0>, 'shard': <function Tensor.shard at 0x78971f1ec160>, 'shard_': <function Tensor.shard_ at 0x78971f1ec1f0>, 'from_node': <staticmethod(<function Tensor.from_node at 0x78971f1ec280>)>, '_metaop': <staticmethod(<function Tensor._metaop at 0x78971f1ec310>)>, 'empty': <staticmethod(<function Tensor.empty at 0x78971f1ec3a0>)>}    
        _rng_counter: Optional[Tensor] = None                                                                                                                                                            #     tensor.py                     :   391: L: {'_seed': 1724415666}    

    
    # register functions to move between devices
    for device in Device._devices: setattr(Tensor, f"{device.lower()}", functools.partialmethod(Tensor.to, device))                                                                                      #     tensor.py                     :  3326: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    L: {'__name__': 'tinygrad.tensor', '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x789735f907c0>, '__spec__': ModuleSpec(name='tinygrad.tensor', loader=<_frozen_importlib_external.SourceFileLoader object at 0x789735f907c0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/tensor.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/tensor.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/tensor.cpython-310.pyc', 'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216), 'dataclasses': <module 'dataclasses' from '/usr/lib/python3.10/dataclasses.py'>, 'time': <module 'time' (built-in)>, 'math': <module 'math' (built-in)>, 'itertools': <module 'itertools' (built-in)>, 'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>, 'sys': <module 'sys' (built-in)>, 'inspect': <module 'inspect' from '/usr/lib/python3.10/inspect.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'ContextDecorator': <class 'contextlib.ContextDecorator'>, 'List': typing.List, 'Tuple': typing.Tuple, 'Callable': typing.Callable, 'Optional': typing.Optional, 'ClassVar': typing.ClassVar, 'Type': typing.Type, 'Union': typing.Union, 'Sequence': typing.Sequence, 'Dict': typing.Dict, 'DefaultDict': typing.DefaultDict, 'cast': <function cast at 0x789735eb17e0>, 'get_args': <function get_args at 0x789735eb1ab0>, 'Set': typing.Set, 'defaultdict': <class 'collections.defaultdict'>, 'np': <module 'numpy' from '/home/lorinbaum/.local/lib/python3.10/site-packages/numpy/__init__.py'>, 'DType': <class 'tinygrad.dtype.DType'>, 'DTypeLike': typing.Union[str, tinygrad.dtype.DType], 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'ConstType': typing.Union[float, int, bool], 'least_upper_float': <function least_upper_float at 0x789730f704c0>, 'least_upper_dtype': <functools._lru_cache_wrapper object at 0x789730f52e50>, 'sum_acc_dtype': <function sum_acc_dtype at 0x789730f70940>, 'to_dtype': <function to_dtype at 0x7897327a63b0>, 'argfix': <function argfix at 0x789730f58310>, 'make_pair': <function make_pair at 0x789730f58820>, 'flatten': <function flatten at 0x789730f588b0>, 'prod': <function prod at 0x7897327a7520>, 'all_int': <function all_int at 0x789730f584c0>, 'round_up': <function round_up at 0x789730f58af0>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'argsort': <function argsort at 0x789730f583a0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'get_shape': <function get_shape at 0x789730f58f70>, 'fully_flatten': <function fully_flatten at 0x789730f58940>, 'dedup': <function dedup at 0x789730f581f0>, 'IMAGE': <tinygrad.helpers.ContextVar object at 0x789730f3bdf0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'WINO': <tinygrad.helpers.ContextVar object at 0x789730f600a0>, 'THREEFRY': <tinygrad.helpers.ContextVar object at 0x789730f602e0>, '_METADATA': <ContextVar name='_METADATA' default=None at 0x789730f470b0>, 'Metadata': <class 'tinygrad.helpers.Metadata'>, 'TRACEMETA': <tinygrad.helpers.ContextVar object at 0x789730f60250>, 'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>, 'MultiLazyBuffer': <class 'tinygrad.multi.MultiLazyBuffer'>, 'MetaOps': <enum 'MetaOps'>, 'truncate': {dtypes.bool: <class 'bool'>, dtypes.half: <function truncate_fp16 at 0x789730ffacb0>, dtypes.float: <function <lambda> at 0x789730ffad40>, dtypes.double: <function <lambda> at 0x789730ffadd0>, dtypes.uchar: <function <lambda> at 0x789730ffae60>, dtypes.ushort: <function <lambda> at 0x789730ffaef0>, dtypes.uint: <function <lambda> at 0x789730ffaf80>, dtypes.ulong: <function <lambda> at 0x789730ffb010>, dtypes.char: <function <lambda> at 0x789730ffb0a0>, dtypes.short: <function <lambda> at 0x789730ffb130>, dtypes.int: <function <lambda> at 0x789730ffb1c0>, dtypes.long: <function <lambda> at 0x789730ffb250>}, 'Device': <tinygrad.device._Device object at 0x78972db5c9a0>, 'Buffer': <class 'tinygrad.device.Buffer'>, 'BufferOptions': <class 'tinygrad.device.BufferOptions'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'MulNode': <class 'tinygrad.shape.symbolic.MulNode'>, 'SumNode': <class 'tinygrad.shape.symbolic.SumNode'>, 'NumNode': <class 'tinygrad.shape.symbolic.NumNode'>, 'Node': <class 'tinygrad.shape.symbolic.Node'>, 'run_schedule': <function run_schedule at 0x78971f1c81f0>, 'memory_planner': <function memory_planner at 0x78971f1c8310>, 'ScheduleItem': <class 'tinygrad.engine.schedule.ScheduleItem'>, 'create_schedule_with_vars': <function create_schedule_with_vars at 0x78971f3a71c0>, 'Function': <class 'tinygrad.tensor.Function'>, 'F': <module 'tinygrad.function' from '/home/lorinbaum/code/tinygrad/tinygrad/function.py'>, '_metaop': <function _metaop at 0x789735dc31c0>, '_from_np_dtype': <function _from_np_dtype at 0x78971f1c8ee0>, '_to_np_dtype': <function _to_np_dtype at 0x78971f1cad40>, '_fromnp': <function _fromnp at 0x78971f1cadd0>, '_frompy': <function _frompy at 0x78971f1cae60>, '_get_winograd_matcols': <function _get_winograd_matcols at 0x78971f1caef0>, '_apply_winograd_matrix': <function _apply_winograd_matrix at 0x78971f1caf80>, '_pad_left': <function _pad_left at 0x78971f1cb010>, '_broadcast_shape': <function _broadcast_shape at 0x78971f1cb0a0>}    
    
    if IMAGE:                                                                                                                                                                                            #     tensor.py                     :  3328: G: {'device': 'AMD'}    

      class ContextVar:                                                                                                                                                                                  #     helpers.py                    :    93: 
        def __bool__(self): return bool(self.value)                                                                                                                                                      #     helpers.py                    :   102: 

    
    if TRACEMETA >= 1:                                                                                                                                                                                   #     tensor.py                     :  3373: G: {'custom_random': <function custom_random at 0x78971f1cb130>, '_metadata_wrapper': <function _metadata_wrapper at 0x78971f1f3760>}    
      for name, fn in inspect.getmembers(Tensor, inspect.isfunction):                                                                                                                                    #     tensor.py                     :  3374: 
        if name in ["__class__", "__init__", "__new__", "__repr__", "backward", "sequential"]: continue                                                                                                  #     tensor.py                     :  3375: G: {'name': '__add__', 'fn': <function Tensor.__add__ at 0x78971f1f13f0>}    
        setattr(Tensor, name, functools.wraps(fn)(_metadata_wrapper(fn)))                                                                                                                                #     tensor.py                     :  3376: 

          def _metadata_wrapper(fn):                                                                                                                                                                     #     tensor.py                     :  3341: 
            return _wrapper                                                                                                                                                                              #     tensor.py                     :  3371: L: {'_wrapper': <function _metadata_wrapper.<locals>._wrapper at 0x78971f1f3e20>}    

  from tinygrad.engine.jit import TinyJit                       # noqa: F401                                                                                                                             #     __init__.py                   :     2: G: {'helpers': <module 'tinygrad.helpers' from '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'>, 'dtype': <module 'tinygrad.dtype' from '/home/lorinbaum/code/tinygrad/tinygrad/dtype.py'>, 'shape': <module 'tinygrad.shape' from '/home/lorinbaum/code/tinygrad/tinygrad/shape/__init__.py'>, 'ops': <module 'tinygrad.ops' from '/home/lorinbaum/code/tinygrad/tinygrad/ops.py'>, 'codegen': <module 'tinygrad.codegen' from '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__init__.py'>, 'renderer': <module 'tinygrad.renderer' from '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__init__.py'>, 'device': <module 'tinygrad.device' from '/home/lorinbaum/code/tinygrad/tinygrad/device.py'>, 'lazy': <module 'tinygrad.lazy' from '/home/lorinbaum/code/tinygrad/tinygrad/lazy.py'>, 'multi': <module 'tinygrad.multi' from '/home/lorinbaum/code/tinygrad/tinygrad/multi.py'>, 'engine': <module 'tinygrad.engine' from '/home/lorinbaum/code/tinygrad/tinygrad/engine/__init__.py'>, 'function': <module 'tinygrad.function' from '/home/lorinbaum/code/tinygrad/tinygrad/function.py'>, 'tensor': <module 'tinygrad.tensor' from '/home/lorinbaum/code/tinygrad/tinygrad/tensor.py'>, 'Tensor': <class 'tinygrad.tensor.Tensor'>}    
    from __future__ import annotations                                                                                                                                                                   #     engine/jit.py                 :     1: G: {'__name__': 'tinygrad.engine.jit', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f216530>, '__spec__': ModuleSpec(name='tinygrad.engine.jit', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f216530>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/jit.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/jit.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/jit.cpython-310.pyc'}    
    from typing import TypeVar, Generic, Callable, List, Tuple, Union, Dict, cast, Optional, Any                                                                                                         #     engine/jit.py                 :     2: G: {'annotations': _Feature((3, 7, 0, 'beta', 1), (3, 11, 0, 'alpha', 0), 16777216)}    
    import functools, itertools, collections                                                                                                                                                             #     engine/jit.py                 :     3: G: {'TypeVar': <class 'typing.TypeVar'>, 'Generic': <class 'typing.Generic'>, 'Callable': typing.Callable, 'List': typing.List, 'Tuple': typing.Tuple, 'Union': typing.Union, 'Dict': typing.Dict, 'cast': <function cast at 0x789735eb17e0>, 'Optional': typing.Optional, 'Any': typing.Any}    
    from tinygrad.tensor import Tensor                                                                                                                                                                   #     engine/jit.py                 :     4: G: {'functools': <module 'functools' from '/usr/lib/python3.10/functools.py'>, 'itertools': <module 'itertools' (built-in)>, 'collections': <module 'collections' from '/usr/lib/python3.10/collections/__init__.py'>}    
    from tinygrad.lazy import LazyBuffer                                                                                                                                                                 #     engine/jit.py                 :     5: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    
    from tinygrad.helpers import flatten, merge_dicts, DEBUG, Context, GRAPH, BEAM, getenv, all_int, colored, JIT, dedup                                                                                 #     engine/jit.py                 :     6: G: {'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    
    from tinygrad.device import Buffer, Compiled, Device                                                                                                                                                 #     engine/jit.py                 :     7: G: {'flatten': <function flatten at 0x789730f588b0>, 'merge_dicts': <function merge_dicts at 0x789730f58ca0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'Context': <class 'tinygrad.helpers.Context'>, 'GRAPH': <tinygrad.helpers.ContextVar object at 0x789730f601c0>, 'BEAM': <tinygrad.helpers.ContextVar object at 0x789730f3bb20>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'all_int': <function all_int at 0x789730f584c0>, 'colored': <function colored at 0x789730f58550>, 'JIT': <tinygrad.helpers.ContextVar object at 0x789730f60190>, 'dedup': <function dedup at 0x789730f581f0>}    
    from tinygrad.dtype import DType                                                                                                                                                                     #     engine/jit.py                 :     8: G: {'Buffer': <class 'tinygrad.device.Buffer'>, 'Compiled': <class 'tinygrad.device.Compiled'>, 'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    
    from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                                 #     engine/jit.py                 :     9: G: {'DType': <class 'tinygrad.dtype.DType'>}    
    from tinygrad.shape.symbolic import Variable, sint, sym_infer                                                                                                                                        #     engine/jit.py                 :    10: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>}    
    from tinygrad.engine.realize import ExecItem, capturing, EmptyOp, ViewOp, BufferXfer, CompiledRunner, Runner, _internal_memory_planner                                                               #     engine/jit.py                 :    11: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>, 'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'sym_infer': <function sym_infer at 0x789730fe70a0>}    
    from tinygrad.nn.state import get_parameters                                                                                                                                                         #     engine/jit.py                 :    12: G: {'ExecItem': <class 'tinygrad.engine.realize.ExecItem'>, 'capturing': [], 'EmptyOp': <class 'tinygrad.engine.realize.EmptyOp'>, 'ViewOp': <class 'tinygrad.engine.realize.ViewOp'>, 'BufferXfer': <class 'tinygrad.engine.realize.BufferXfer'>, 'CompiledRunner': <class 'tinygrad.engine.realize.CompiledRunner'>, 'Runner': <class 'tinygrad.engine.realize.Runner'>, '_internal_memory_planner': <function _internal_memory_planner at 0x78971f1c8280>}    
      import math                                                                                                                                                                                        #     nn/__init__.py                :     1: G: {'__name__': 'tinygrad.nn', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f216dd0>, '__spec__': ModuleSpec(name='tinygrad.nn', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f216dd0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad/nn']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad/nn'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/__init__.cpython-310.pyc'}    
      from typing import Optional, Union, Tuple                                                                                                                                                          #     nn/__init__.py                :     2: G: {'math': <module 'math' (built-in)>}    
      from tinygrad.tensor import Tensor                                                                                                                                                                 #     nn/__init__.py                :     3: G: {'Optional': typing.Optional, 'Union': typing.Union, 'Tuple': typing.Tuple}    
      from tinygrad.helpers import prod                                                                                                                                                                  #     nn/__init__.py                :     4: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    
      from tinygrad.nn import optim, state, datasets  # noqa: F401                                                                                                                                       #     nn/__init__.py                :     5: G: {'prod': <function prod at 0x7897327a7520>}    
        # sorted in order of increasing complexity
        from typing import List                                                                                                                                                                          #     nn/optim.py                   :     2: G: {'__name__': 'tinygrad.nn.optim', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f217dc0>, '__spec__': ModuleSpec(name='tinygrad.nn.optim', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f217dc0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/optim.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/optim.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/optim.cpython-310.pyc'}    
        from tinygrad.helpers import dedup, flatten, getenv                                                                                                                                              #     nn/optim.py                   :     3: G: {'List': typing.List}    
        from tinygrad.tensor import Tensor                                                                                                                                                               #     nn/optim.py                   :     4: G: {'dedup': <function dedup at 0x789730f581f0>, 'flatten': <function flatten at 0x789730f588b0>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>}    
        from tinygrad.dtype import dtypes, least_upper_dtype                                                                                                                                             #     nn/optim.py                   :     5: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    

        import os, json, pathlib, zipfile, pickle, tarfile, struct                                                                                                                                       #     nn/state.py                   :     1: G: {'__name__': 'tinygrad.nn.state', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f24c2b0>, '__spec__': ModuleSpec(name='tinygrad.nn.state', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f24c2b0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/state.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/state.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/state.cpython-310.pyc'}    
        from typing import Dict, Union, List, Optional, Any, Tuple                                                                                                                                       #     nn/state.py                   :     2: G: {'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'json': <module 'json' from '/usr/lib/python3.10/json/__init__.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'zipfile': <module 'zipfile' from '/usr/lib/python3.10/zipfile.py'>, 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>, 'tarfile': <module 'tarfile' from '/usr/lib/python3.10/tarfile.py'>, 'struct': <module 'struct' from '/usr/lib/python3.10/struct.py'>}    
        from tinygrad.tensor import Tensor                                                                                                                                                               #     nn/state.py                   :     3: G: {'Dict': typing.Dict, 'Union': typing.Union, 'List': typing.List, 'Optional': typing.Optional, 'Any': typing.Any, 'Tuple': typing.Tuple}    
        from tinygrad.dtype import dtypes                                                                                                                                                                #     nn/state.py                   :     4: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    
        from tinygrad.helpers import prod, argsort, DEBUG, Timing, CI, unwrap, GlobalCounters, tqdm                                                                                                      #     nn/state.py                   :     5: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>}    
        from tinygrad.shape.view import strides_for_shape                                                                                                                                                #     nn/state.py                   :     6: G: {'prod': <function prod at 0x7897327a7520>, 'argsort': <function argsort at 0x789730f583a0>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'Timing': <class 'tinygrad.helpers.Timing'>, 'CI': False, 'unwrap': <function unwrap at 0x789730f58dc0>, 'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'tqdm': <class 'tinygrad.helpers.tqdm'>}    
        from tinygrad.multi import MultiLazyBuffer                                                                                                                                                       #     nn/state.py                   :     7: G: {'strides_for_shape': <functools._lru_cache_wrapper object at 0x789730528670>}    
        
        safe_dtypes = {"BOOL":dtypes.bool, "I8":dtypes.int8, "U8":dtypes.uint8, "I16":dtypes.int16, "U16":dtypes.uint16, "I32":dtypes.int, "U32":dtypes.uint,                                            #     nn/state.py                   :     9: G: {'MultiLazyBuffer': <class 'tinygrad.multi.MultiLazyBuffer'>}    
                       "I64":dtypes.int64, "U64":dtypes.uint64, "F16":dtypes.float16, "BF16":dtypes.bfloat16, "F32":dtypes.float32, "F64":dtypes.float64}                                                     
        inverse_safe_dtypes = {v:k for k,v in safe_dtypes.items()}                                                                                                                                       #     nn/state.py                   :    11: G: {'safe_dtypes': {'BOOL': dtypes.bool, 'I8': dtypes.char, 'U8': dtypes.uchar, 'I16': dtypes.short, 'U16': dtypes.ushort, 'I32': dtypes.int, 'U32': dtypes.uint, 'I64': dtypes.long, 'U64': dtypes.ulong, 'F16': dtypes.half, 'BF16': dtypes.bfloat16, 'F32': dtypes.float, 'F64': dtypes.double}}    
        
        # state dict
        
        from collections import OrderedDict                                                                                                                                                              #     nn/state.py                   :    62: G: {'inverse_safe_dtypes': {dtypes.bool: 'BOOL', dtypes.char: 'I8', dtypes.uchar: 'U8', dtypes.short: 'I16', dtypes.ushort: 'U16', dtypes.int: 'I32', dtypes.uint: 'U32', dtypes.long: 'I64', dtypes.ulong: 'U64', dtypes.half: 'F16', dtypes.bfloat16: 'BF16', dtypes.float: 'F32', dtypes.double: 'F64'}, 'safe_load_metadata': <function safe_load_metadata at 0x78971f223c70>, 'safe_load': <function safe_load at 0x78971f270040>, 'safe_save': <function safe_save at 0x78971f29da20>}    

        from tinygrad.tensor import Tensor                                                                                                                                                               #     nn/datasets.py                :     1: G: {'__name__': 'tinygrad.nn.datasets', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f24f940>, '__spec__': ModuleSpec(name='tinygrad.nn.datasets', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f24f940>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/datasets.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/datasets.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/datasets.cpython-310.pyc'}    
        from tinygrad.helpers import fetch                                                                                                                                                               #     nn/datasets.py                :     2: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    
        from tinygrad.nn.state import tar_extract                                                                                                                                                        #     nn/datasets.py                :     3: G: {'fetch': <function fetch at 0x789730f5a4d0>}    

      BatchNorm2d = BatchNorm3d = BatchNorm                                                                                                                                                              #     nn/__init__.py                :    60: G: {'optim': <module 'tinygrad.nn.optim' from '/home/lorinbaum/code/tinygrad/tinygrad/nn/optim.py'>, 'state': <module 'tinygrad.nn.state' from '/home/lorinbaum/code/tinygrad/tinygrad/nn/state.py'>, 'datasets': <module 'tinygrad.nn.datasets' from '/home/lorinbaum/code/tinygrad/tinygrad/nn/datasets.py'>, 'BatchNorm': <class 'tinygrad.nn.BatchNorm'>}    

    from dataclasses import dataclass                                                                                                                                                                    #     engine/jit.py                 :    13: G: {'get_parameters': <function get_parameters at 0x78971f29db40>}    
    from weakref import WeakKeyDictionary                                                                                                                                                                #     engine/jit.py                 :    14: G: {'dataclass': <function dataclass at 0x789735c341f0>}    
    
    ReturnType = TypeVar('ReturnType')                                                                                                                                                                   #     engine/jit.py                 :   130: G: {'WeakKeyDictionary': <class 'weakref.WeakKeyDictionary'>, 'GraphException': <class 'tinygrad.engine.jit.GraphException'>, 'apply_graph_to_jit': <function apply_graph_to_jit at 0x78971f221090>, 'get_input_replace': <function get_input_replace at 0x78971f221120>, 'GraphRunner': <class 'tinygrad.engine.jit.GraphRunner'>, 'MultiGraphRunner': <class 'tinygrad.engine.jit.MultiGraphRunner'>}    

      @dataclass                                                                                                                                                                                         #     engine/jit.py                 :   132: 
      class CapturedJit(Generic[ReturnType]):                                                                                                                                                                 
        ret: Any  # includes the Tensors or any other returned object                                                                                                                                    #     engine/jit.py                 :   133: G: {'ReturnType': ~ReturnType}    L: {'__module__': 'tinygrad.engine.jit', '__qualname__': 'CapturedJit', '__annotations__': {}}    
        jit_cache: List[ExecItem]                                                                                                                                                                        #     engine/jit.py                 :   134: 
        input_replace: Dict[Tuple[int, int], int]                                                                                                                                                        #     engine/jit.py                 :   135: 
        extra_view_inputs: List[Tuple[int, int, str, int, DType]]                                                                                                                                        #     engine/jit.py                 :   136: 
        expected_names: List[Union[int, str]]                                                                                                                                                            #     engine/jit.py                 :   137: 
        expected_st_vars_dtype_device: List[Tuple[ShapeTracker, Tuple[Variable, ...], DType, str]]                                                                                                       #     engine/jit.py                 :   138: 

  from tinygrad.shape.symbolic import Variable                  # noqa: F401                                                                                                                             #     __init__.py                   :     3: G: {'nn': <module 'tinygrad.nn' from '/home/lorinbaum/code/tinygrad/tinygrad/nn/__init__.py'>, 'TinyJit': <class 'tinygrad.engine.jit.TinyJit'>}    
  from tinygrad.dtype import dtypes                             # noqa: F401                                                                                                                             #     __init__.py                   :     4: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>}    
  from tinygrad.helpers import GlobalCounters, fetch, Context   # noqa: F401                                                                                                                             #     __init__.py                   :     5: G: {'dtypes': <class 'tinygrad.dtype.dtypes'>}    
  from tinygrad.device import Device                            # noqa: F401                                                                                                                             #     __init__.py                   :     6: G: {'GlobalCounters': <class 'tinygrad.helpers.GlobalCounters'>, 'fetch': <function fetch at 0x789730f5a4d0>, 'Context': <class 'tinygrad.helpers.Context'>}    

a = (Tensor([1,2,3], device="CLANG") + 2)                                                                                                                                                                #     ...grad.tensor.tolist_CLANG.py:     2: G: {'Tensor': <class 'tinygrad.tensor.Tensor'>}    

  class Tensor:                                                                                                                                                                                          #     tensor.py                     :    92: 
    def __init__(self, data:Union[None, ConstType, List, Tuple, LazyBuffer, np.ndarray, bytes, MultiLazyBuffer, Variable, pathlib.Path],                                                                 #     tensor.py                     :   108: 
                 device:Optional[Union[str, tuple, list]]=None, dtype:Optional[DTypeLike]=None, requires_grad:Optional[bool]=None):                                                                           
      if dtype is not None: dtype = to_dtype(dtype)                                                                                                                                                      #     tensor.py                     :   110: G: {'name': 'zeros_like', 'fn': <function Tensor.zeros_like at 0x78971f1ec8b0>}    L: {'data': [1, 2, 3], 'device': 'CLANG'}    
      assert dtype is None or isinstance(dtype, DType), f"invalid dtype {dtype}"                                                                                                                         #     tensor.py                     :   111: 
      if device is None and isinstance(data, pathlib.Path): device = f"DISK:{data.resolve()}"  # keep it on the disk if device is None                                                                   #     tensor.py                     :   112: 
      device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)                                                                       #     tensor.py                     :   113: 

        class _Device:                                                                                                                                                                                   #     device.py                     :    13: 
          # NOTE: you can't cache canonicalize in case Device.DEFAULT changes                                                                                                                            #     device.py                     :    18: 
          def canonicalize(self, device:Optional[str]) -> str: return self._canonicalize(device) if device is not None else Device.DEFAULT                                                                    

            class _Device:                                                                                                                                                                               #     device.py                     :    13: 
              @functools.lru_cache(maxsize=None)  # this class is a singleton, pylint: disable=method-cache-max-size-none                                                                                #     device.py                     :    16: 
              def _canonicalize(self, device:str) -> str: return (device.split(":", 1)[0].upper() + ((":"+device.split(":", 1)[1]) if ':' in device else '')).replace(":0", "")   # noqa: E501                

  
      # tensors can have gradients if you have called .backward
      self.grad: Optional[Tensor] = None                                                                                                                                                                 #     tensor.py                     :   116: L: {'device': 'CLANG'}    
  
      # NOTE: this can be in three states. False and None: no gradient, True: gradient
      # None (the default) will be updated to True if it's put in an optimizer
      self.requires_grad: Optional[bool] = requires_grad                                                                                                                                                 #     tensor.py                     :   120: 
  
      # internal variable used for autograd graph construction
      self._ctx: Optional[Function] = None                                                                                                                                                               #     tensor.py                     :   123: 
  
      # create a LazyBuffer from the different types of inputs
      if isinstance(data, LazyBuffer): assert dtype is None or dtype == data.dtype, "dtype doesn't match, and casting isn't supported"                                                                   #     tensor.py                     :   126: 
      elif isinstance(data, get_args(ConstType)): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data), device, data)                                                                    #     tensor.py                     :   127: 
      elif isinstance(data, Variable): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data.unbind()[1]), device, data)                                                                   #     tensor.py                     :   128: 
      elif isinstance(data, bytes): data = _frompy(data, dtypes.uint8 if dtype is None else dtype)                                                                                                       #     tensor.py                     :   129: 
      elif isinstance(data, (list, tuple)):                                                                                                                                                              #     tensor.py                     :   130: 
        if dtype is None:                                                                                                                                                                                #     tensor.py                     :   131: 
          if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool                                                                                                     #     tensor.py                     :   132: 

            def fully_flatten(l): return [item for sublist in l for item in (fully_flatten(sublist) if isinstance(sublist, (tuple, list)) else [sublist])]                                               #     helpers.py                    :    35: 

          else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float                                                                                                                 #     tensor.py                     :   133: L: {'d': [1, 2, 3]}    

            def all_int(t: Sequence[Any]) -> TypeGuard[Tuple[int, ...]]: return all(isinstance(s, int) for s in t)                                                                                       #     helpers.py                    :    27: 

        if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata                                                                    #     tensor.py                     :   134: L: {'dtype': dtypes.int}    
        else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))                                                                                                                                 #     tensor.py                     :   135: 

          def _to_np_dtype(dtype:DType) -> Optional[type]: return np.dtype(dtype.fmt).type if dtype.fmt is not None else None                                                                            #     tensor.py                     :    49: 

          def _fromnp(x: np.ndarray) -> LazyBuffer:                                                                                                                                                      #     tensor.py                     :    51: 
            ret = LazyBuffer.metaop(MetaOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")                                                                                                              #     tensor.py                     :    52: L: {'x': array([1, 2, 3], dtype=int32)}    

              def _from_np_dtype(npdtype:np.dtype) -> DType: return dtypes.fields()[np.dtype(npdtype).name]                                                                                              #     tensor.py                     :    48: 

                class dtypes:                                                                                                                                                                            #     dtype.py                      :    38: 
                  @staticmethod                                                                                                                                                                          #     dtype.py                      :    71: 
                  def fields() -> Dict[str, DType]: return DTYPES_DICT                                                                                                                                        

              class LazyBuffer:                                                                                                                                                                          #     lazy.py                       :    26: 
                @staticmethod                                                                                                                                                                            #     lazy.py                       :    70: 
                def metaop(op, shape:Tuple[sint,...], dtype:DTypeLike, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:                                                
                  assert isinstance(src, tuple)                                                                                                                                                          #     lazy.py                       :    71: G: {'view_supported_devices': {'CLANG', 'DISK', 'CUDA', 'LLVM', 'AMD', 'NV', 'METAL'}, 'LazyBuffer': <class 'tinygrad.lazy.LazyBuffer'>}    L: {'op': <MetaOps.EMPTY: 1>, 'shape': (3,), 'dtype': dtypes.int, 'device': 'NPY', 'src': (), 'enable_cache': False}    
                  return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)                                                                       #     lazy.py                       :    72: 

                    @dataclass(frozen=True)                                                                                                                                                              #     shape/shapetracker.py         :    36: 
                    class ShapeTracker:                                                                                                                                                                       
                      @staticmethod                                                                                                                                                                      #     shape/shapetracker.py         :    52: 
                      def from_shape(shape:Tuple[sint, ...]) -> ShapeTracker: return ShapeTracker((View.create(shape),))                                                                                      

                        @dataclass(frozen=True)                                                                                                                                                          #     shape/view.py                 :    85: 
                        class View:                                                                                                                                                                           
                          @staticmethod                                                                                                                                                                  #     shape/view.py                 :   101: 
                          @functools.lru_cache(maxsize=None)                                                                                                                                                  
                          def create(shape:Tuple[sint, ...], strides:Optional[Tuple[sint, ...]]=None, offset:sint=0, mask:Optional[Tuple[Tuple[sint, sint], ...]]=None):                                      
                            if not all(s >= 0 for s in shape): raise ValueError(f"Trying to create View with negative dimension: {shape=}")                                                              #     shape/view.py                 :   102: G: {'View': <class 'tinygrad.shape.view.View'>}    L: {'shape': (3,), 'offset': 0}    
                            strides = canonicalize_strides(shape, strides) if strides else strides_for_shape(shape)                                                                                      #     shape/view.py                 :   103: 

                              @functools.lru_cache(maxsize=None)                                                                                                                                         #     shape/view.py                 :    13: 
                              def strides_for_shape(shape:Tuple[sint, ...]) -> Tuple[sint, ...]:                                                                                                              
                                if not shape: return ()                                                                                                                                                  #     shape/view.py                 :    14: 
                                strides = tuple(itertools.accumulate(reversed(shape[1:]), operator.mul, initial=1))[::-1]                                                                                #     shape/view.py                 :    15: 
                                return canonicalize_strides(shape, strides)                                                                                                                              #     shape/view.py                 :    16: L: {'strides': (1,)}    

                                  @functools.lru_cache(maxsize=None)                                                                                                                                     #     shape/view.py                 :     9: 
                                  def canonicalize_strides(shape:Tuple[sint, ...], strides:Tuple[sint, ...]) -> Tuple[sint, ...]:                                                                             
                                    return tuple(0 if s == 1 else st for s, st in zip(shape, strides))                                                                                                   #     shape/view.py                 :    10: 

                            # canonicalize 0 in shape
                            if 0 in shape: return View(shape, (0,) * len(shape), offset=0, mask=None, contiguous=True)                                                                                   #     shape/view.py                 :   105: L: {'offset': 0, 'strides': (1,)}    
                            # canonicalize empty mask
                            if mask is not None and all(m == (0,s) for m,s in zip(mask, shape)): mask = None                                                                                             #     shape/view.py                 :   107: 
                            # if any dimension has size >1, but is masked such that only one index in the dimension is unmasked
                            # then its stride can also be set to 0, albeit with a corresponding adjustment required to the offset
                            # TODO: assert comparison with LtNode to avoid mis-using symbolic
                            if mask and any(elim := [not (b+1 < e) for b,e in mask]):                                                                                                                    #     shape/view.py                 :   111: 
                            contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)                                                                                            #     shape/view.py                 :   116: 
                            return View(shape, strides, offset, mask, contiguous)                                                                                                                        #     shape/view.py                 :   117: L: {'contiguous': True}    

                    def create_lazybuffer(device:str, st:ShapeTracker, dtype:DTypeLike, op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),                                              #     lazy.py                       :    12: 
                                          base:Optional[LazyBuffer]=None, enable_cache=bool(getenv("LAZYCACHE", 1))):                                                                                         
                      if st.size == 0: op, arg, srcs, base = MetaOps.CONST, 0, (), None                                                                                                                  #     lazy.py                       :    14: L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'srcs': ()}    

                        @dataclass(frozen=True)                                                                                                                                                          #     shape/shapetracker.py         :    36: 
                        class ShapeTracker:                                                                                                                                                                   
                          @property                                                                                                                                                                      #     shape/shapetracker.py         :    64: 
                          def size(self) -> int: return self.views[-1].size()                                                                                                                                 

                            @dataclass(frozen=True)                                                                                                                                                      #     shape/view.py                 :    85: 
                            class View:                                                                                                                                                                       
                              @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                                           #     shape/view.py                 :    93: 
                              def size(self) -> int:                                                                                                                                                          
                                # NOTE: Variable and the Node derived from it in symbolic shapes can only have int as max.
                                ret = prod([x.max if isinstance(x, Node) else x for x in self.shape])                                                                                                    #     shape/view.py                 :    95: L: {'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}    

                                  # NOTE: it returns int 1 if x is empty regardless of the type of x                                                                                                     #     helpers.py                    :    13: 
                                  def prod(x:Iterable[T]) -> Union[T,int]: return functools.reduce(operator.mul, x, 1)                                                                                        

                                assert isinstance(ret, int), f"{ret=} is not int"                                                                                                                        #     shape/view.py                 :    96: L: {'ret': 3}    
                                return ret                                                                                                                                                               #     shape/view.py                 :    97: 

                      dtype = to_dtype(dtype)                                                                                                                                                            #     lazy.py                       :    15: 

                        def to_dtype(dtype:DTypeLike) -> DType: return dtype if isinstance(dtype, DType) else getattr(dtypes, dtype)                                                                     #     dtype.py                      :   108: 

                      if op is MetaOps.CONST: arg, enable_cache = dtypes.as_const(arg, dtype) if not isinstance(arg, Variable) else arg, True                                                            #     lazy.py                       :    16: 
                    
                      cache_key = (device, st, dtype, op, arg, tuple(ref(x) for x in srcs)) if base is None else (st, ref(base))                                                                         #     lazy.py                       :    18: 
                      if enable_cache and (rret := lazycache.get(cache_key, None)): return rret                                                                                                          #     lazy.py                       :    19: L: {'cache_key': ('NPY', ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), dtypes.int, <MetaOps.EMPTY: 1>, None, ())}    
                    
                      ret = LazyBuffer(device, st, dtype, op, arg, srcs, base=base, metadata=_METADATA.get())                                                                                            #     lazy.py                       :    21: 

                        class LazyBuffer:                                                                                                                                                                #     lazy.py                       :    26: 
                          def __init__(self, device:str, st:ShapeTracker, dtype:DTypeLike,                                                                                                               #     lazy.py                       :    27: 
                                       op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),                                                                                                    
                                       base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):                                                                                                     
                            self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, to_dtype(dtype), st.shape, st.size, metadata                                            #     lazy.py                       :    30: 

                              @dataclass(frozen=True)                                                                                                                                                    #     shape/shapetracker.py         :    36: 
                              class ShapeTracker:                                                                                                                                                             
                                @property                                                                                                                                                                #     shape/shapetracker.py         :    61: 
                                def shape(self) -> Tuple[sint, ...]: return self.views[-1].shape                                                                                                              

                            self._base: Optional[LazyBuffer] = None                                                                                                                                      #     lazy.py                       :    31: 
                            if base is None:                                                                                                                                                             #     lazy.py                       :    32: 
                              # properties on base
                              self.op, self.arg, self.srcs = op, arg, srcs  # this is a UOp, except the src is LazyBuffers and not UOps                                                                  #     lazy.py                       :    34: 
                              assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"                                                                #     lazy.py                       :    35: L: {'self': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, None)>}    
                        
                              if self.op is MetaOps.VIEW:                                                                                                                                                #     lazy.py                       :    37: 
                              else:                                                                                                                                                                      #     lazy.py                       :    40: 
                                self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, self.dtype)                                                                #     lazy.py                       :    41: 

                                  class Buffer:                                                                                                                                                          #     device.py                     :    52: 
                                    def __init__(self, device:str, size:int, dtype:DType, opaque:Any=None, options:Optional[BufferOptions]=None,                                                         #     device.py                     :    53: 
                                                 initial_value:Optional[bytes]=None, lb_refcount=0, base:Optional[Buffer]=None, offset:int=0, preallocate=False):                                             
                                      assert isinstance(dtype, DType)                                                                                                                                    #     device.py                     :    55: G: {'HCQCompiled': <class 'tinygrad.device.HCQCompiled'>, 'HCQBuffer': <class 'tinygrad.device.HCQBuffer'>, 'HCQAllocator': <class 'tinygrad.device.HCQAllocator'>}    L: {'device': 'NPY', 'size': 3, 'dtype': dtypes.int, 'lb_refcount': 0, 'offset': 0, 'preallocate': False}    
                                      if isinstance(dtype, ImageDType): options = BufferOptions(image=dtype) # TODO: image hack shouldn't be here. where should it be?                                   #     device.py                     :    56: 
                                      self.device, self.size, self.dtype, self.options, self.offset = device, size, dtype, options, offset                                                               #     device.py                     :    57: 
                                      if base is None:                                                                                                                                                   #     device.py                     :    58: L: {'self': <buf real:False device:NPY size:3 dtype:dtypes.int>}    
                                        assert offset == 0, "base buffers can't have offset"                                                                                                             #     device.py                     :    59: 
                                        self._base = None                                                                                                                                                #     device.py                     :    60: 
                                        self._lb_refcount = lb_refcount                                                                                                                                  #     device.py                     :    61: 
                                        if opaque is not None: self.allocate(opaque)                                                                                                                     #     device.py                     :    62: 
                                        if initial_value is not None:                                                                                                                                    #     device.py                     :    63: 
                                      if preallocate: self.allocate()                                                                                                                                    #     device.py                     :    70: 

                              self.buffer.ref(1)                                                                                                                                                         #     lazy.py                       :    42: 

                                class Buffer:                                                                                                                                                            #     device.py                     :    52: 
                                  def ref(self, cnt): self.base._lb_refcount += cnt                                                                                                                      #     device.py                     :    75: 

                                    class Buffer:                                                                                                                                                        #     device.py                     :    52: 
                                      @property                                                                                                                                                          #     device.py                     :    72: 
                                      def base(self) -> Buffer: return self._base if self._base is not None else self                                                                                         

                              self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None                                                                                     #     lazy.py                       :    43: 
                              self.forced_realize = False                                                                                                                                                #     lazy.py                       :    44: 

                      if enable_cache: lazycache[cache_key] = ret                                                                                                                                        #     lazy.py                       :    22: L: {'enable_cache': False, 'cache_key': ('NPY', ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), dtypes.int, <MetaOps.EMPTY: 1>, None, ()), 'ret': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, None)>}    
                      return ret                                                                                                                                                                         #     lazy.py                       :    23: 

            # fake realize
            ret.buffer.allocate(x)                                                                                                                                                                       #     tensor.py                     :    54: L: {'ret': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, None)>}    

              class Buffer:                                                                                                                                                                              #     device.py                     :    52: 
                def allocate(self, opaque=None) -> Buffer:                                                                                                                                               #     device.py                     :    78: 
                  assert not hasattr(self, '_buf'), "can't allocate already allocated buffer"                                                                                                            #     device.py                     :    79: L: {'opaque': array([1, 2, 3], dtype=int32)}    
                  self.allocator = Device[self.device].allocator                                                                                                                                         #     device.py                     :    80: 

                    class _Device:                                                                                                                                                                       #     device.py                     :    13: 
                      def __getitem__(self, ix:str) -> Compiled: return self.__get_canonicalized_item(self.canonicalize(ix))                                                                             #     device.py                     :    19: 

                        class _Device:                                                                                                                                                                   #     device.py                     :    13: 
                          @functools.lru_cache(maxsize=None)  # this class is a singleton, pylint: disable=method-cache-max-size-none                                                                    #     device.py                     :    21: 
                          def __get_canonicalized_item(self, ix:str) -> Compiled:                                                                                                                             
                            cpn = multiprocessing.current_process().name                                                                                                                                 #     device.py                     :    22: L: {'ix': 'NPY', 'self': <tinygrad.device._Device object at 0x78972db5c9a0>}    
                            assert (cpn == "MainProcess") or ix.split(":")[0] in ["DISK", "NPY"], f"can only open device {ix} from parent, not {cpn}"                                                    #     device.py                     :    23: L: {'cpn': 'MainProcess'}    
                            x = ix.split(":")[0].upper()                                                                                                                                                 #     device.py                     :    24: 
                            ret = [cls for cname, cls in inspect.getmembers(importlib.import_module(f'tinygrad.runtime.ops_{x.lower()}')) if (cname.lower() == x.lower() + "device") and x in self._devices][0](ix)  # noqa: E501 #     device.py                     :    25: L: {'x': 'NPY'}    
                              import numpy as np                                                                                                                                                         #     runtime/ops_npy.py            :     1: G: {'__name__': 'tinygrad.runtime.ops_npy', '__doc__': None, '__package__': 'tinygrad.runtime', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f216920>, '__spec__': ModuleSpec(name='tinygrad.runtime.ops_npy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f216920>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_npy.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_npy.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/__pycache__/ops_npy.cpython-310.pyc'}    
                              from tinygrad.helpers import flat_mv                                                                                                                                       #     runtime/ops_npy.py            :     2: G: {'np': <module 'numpy' from '/home/lorinbaum/.local/lib/python3.10/site-packages/numpy/__init__.py'>}    
                              from tinygrad.device import Compiled, Allocator                                                                                                                            #     runtime/ops_npy.py            :     3: G: {'flat_mv': <function flat_mv at 0x789730f5aa70>}    

                              class NpyDevice(Compiled):                                                                                                                                                 #     runtime/ops_npy.py            :     8: 
                                def __init__(self, device:str): super().__init__(device, NpyAllocator(), None, None, None)                                                                               #     runtime/ops_npy.py            :     9: 

                                  class Compiled:                                                                                                                                                        #     device.py                     :   186: 
                                    def __init__(self, device:str, allocator:Allocator, renderer:Optional[Renderer], compiler:Optional[Compiler], runtime, graph=None):                                  #     device.py                     :   187: 
                                      self.dname, self.allocator, self.compiler, self.runtime, self.graph = device, allocator, compiler or Compiler(), runtime, graph                                    #     device.py                     :   188: L: {'self': <tinygrad.runtime.ops_npy.NpyDevice object at 0x78971f217790>, 'device': 'NPY', 'allocator': <tinygrad.runtime.ops_npy.NpyAllocator object at 0x78971f216ec0>}    

                                        class Compiler:                                                                                                                                                  #     device.py                     :   176: 
                                          def __init__(self, cachekey:Optional[str]=None): self.cachekey = None if getenv("DISABLE_COMPILER_CACHE") else cachekey                                        #     device.py                     :   177: 

                                      self.renderer = renderer or Renderer()                                                                                                                             #     device.py                     :   189: 

                            if DEBUG >= 1: print(f"opened device {ix} from pid:{os.getpid()}")                                                                                                           #     device.py                     :    26: L: {'ix': 'NPY', 'self': <tinygrad.device._Device object at 0x78972db5c9a0>, 'cpn': 'MainProcess', 'x': 'NPY', 'ret': <tinygrad.runtime.ops_npy.NpyDevice object at 0x78971f217790>}    
                            return ret                                                                                                                                                                   #     device.py                     :    27: 

                  if self._base is not None:                                                                                                                                                             #     device.py                     :    81: L: {'self': <buf real:False device:NPY size:3 dtype:dtypes.int offset:0>, 'opaque': array([1, 2, 3], dtype=int32)}    
                  else:                                                                                                                                                                                  #     device.py                     :    85: 
                    self._buf = opaque if opaque is not None else self.allocator.alloc(self.nbytes, self.options)                                                                                        #     device.py                     :    86: 
                    if not self.device.startswith("DISK"): GlobalCounters.mem_used += self.nbytes                                                                                                        #     device.py                     :    87: 

                      class Buffer:                                                                                                                                                                      #     device.py                     :    52: 
                        @property                                                                                                                                                                        #     device.py                     :    99: 
                        def nbytes(self): return self.size*self.dtype.itemsize                                                                                                                                

                  return self                                                                                                                                                                            #     device.py                     :    88: 

            del ret.srcs                                                                                                                                                                                 #     tensor.py                     :    55: 
            return ret                                                                                                                                                                                   #     tensor.py                     :    56: 

  
      # by this point, it has to be a LazyBuffer
      if not isinstance(data, (LazyBuffer, MultiLazyBuffer)):                                                                                                                                            #     tensor.py                     :   145: L: {'data': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>, 'device': 'CLANG', 'dtype': dtypes.int, 'd': [1, 2, 3]}    
  
      # data is a LazyBuffer, but it might be on the wrong device
      if isinstance(device, tuple):                                                                                                                                                                      #     tensor.py                     :   149: 
      else:                                                                                                                                                                                              #     tensor.py                     :   156: 
        self.lazydata = data if data.device == device else data.copy_to_device(device)                                                                                                                   #     tensor.py                     :   157: 

          class LazyBuffer:                                                                                                                                                                              #     lazy.py                       :    26: 
            def copy_to_device(self, device:str, force: bool = False) -> LazyBuffer:                                                                                                                     #     lazy.py                       :   119: 
              # no COPY
              if self.device == device: return self                                                                                                                                                      #     lazy.py                       :   121: L: {'self': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>, 'device': 'CLANG', 'force': False}    
          
              # double COPY = one COPY
              if not force and self.st.contiguous and self.size == self.base.size and not self.base.realized and self.base.op is MetaOps.COPY:                                                           #     lazy.py                       :   124: 

                @dataclass(frozen=True)                                                                                                                                                                  #     shape/shapetracker.py         :    36: 
                class ShapeTracker:                                                                                                                                                                           
                  @property                                                                                                                                                                              #     shape/shapetracker.py         :    55: 
                  def contiguous(self) -> bool: return len(self.views) == 1 and self.views[0].contiguous                                                                                                      

                class LazyBuffer:                                                                                                                                                                        #     lazy.py                       :    26: 
                  # NOTE: this has to be a function to prevent self reference                                                                                                                            #     lazy.py                       :    63: 
                  @property                                                                                                                                                                                   
                  def base(self) -> LazyBuffer: return self._base if self._base is not None else self                                                                                                         

                class LazyBuffer:                                                                                                                                                                        #     lazy.py                       :    26: 
                  @property                                                                                                                                                                              #     lazy.py                       :    57: 
                  def realized(self) -> Optional[Buffer]:                                                                                                                                                     
                    # NOTE: we check for a lack of srcs instead of an allocated buffer to make unrealized assigns return None here
                    return self.buffer if self._base is None and not hasattr(self, 'srcs') else None                                                                                                     #     lazy.py                       :    59: 

          
              # const doesn't have to be copied (issues with disk tensor)
              if self.is_unrealized_const():                                                                                                                                                             #     lazy.py                       :   128: L: {'device': 'CLANG', 'force': False}    

                class LazyBuffer:                                                                                                                                                                        #     lazy.py                       :    26: 
                  def is_unrealized_const(self): return self.base.realized is None and self.base.op is MetaOps.CONST and not isinstance(self.base.arg, Variable)                                         #     lazy.py                       :   113: 

          
              # if it's a shrink, do the shrink before the copy with CONTIGUOUS
              if prod(self.st.shape) < prod(self.base.st.shape): return self.contiguous()._copy(device)                                                                                                  #     lazy.py                       :   132: 
          
              # copy the base and apply the shapetracker on the new device
              return self.base._copy(device)._view(self.st)                                                                                                                                              #     lazy.py                       :   135: 

                class LazyBuffer:                                                                                                                                                                        #     lazy.py                       :    26: 
                  def _copy(self, device:str) -> LazyBuffer:                                                                                                                                             #     lazy.py                       :   116: 
                    return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, MetaOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)                                     #     lazy.py                       :   117: 

                class LazyBuffer:                                                                                                                                                                        #     lazy.py                       :    26: 
                  def _view(self, new_st:ShapeTracker) -> LazyBuffer:                                                                                                                                    #     lazy.py                       :   205: 
                    if self.st.size == 0 or (new_st.views[-1].mask is not None and any((x[1]-x[0]) == 0 for x in new_st.views[-1].mask)):                                                                #     lazy.py                       :   206: L: {'self': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'new_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                    if new_st.contiguous and self.base.shape == new_st.shape: return self.base                                                                                                           #     lazy.py                       :   208: 

  def _metadata_wrapper(fn):                                                                                                                                                                             #     tensor.py                     :  3341: 
    def _wrapper(*args, **kwargs):                                                                                                                                                                       #     tensor.py                     :  3342: 
      if _METADATA.get() is not None: return fn(*args, **kwargs)                                                                                                                                         #     tensor.py                     :  3343: L: {'args': (<Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 2), 'kwargs': {}, 'fn': <function Tensor.__add__ at 0x78971f1f13f0>}    
  
      if TRACEMETA >= 2:                                                                                                                                                                                 #     tensor.py                     :  3345: 
      else: caller = ""                                                                                                                                                                                  #     tensor.py                     :  3365: 
  
      token = _METADATA.set(Metadata(name=fn.__name__, caller=caller))                                                                                                                                   #     tensor.py                     :  3367: L: {'caller': ''}    
      ret = fn(*args, **kwargs)                                                                                                                                                                          #     tensor.py                     :  3368: L: {'token': <Token var=<ContextVar name='_METADATA' default=None at 0x789730f470b0> at 0x78972db4eb80>}    

        class Tensor:                                                                                                                                                                                    #     tensor.py                     :    92: 
          def __add__(self, x) -> Tensor: return self.add(x)                                                                                                                                             #     tensor.py                     :  2820: 

            class Tensor:                                                                                                                                                                                #     tensor.py                     :    92: 
              def add(self, x:Union[Tensor, ConstType], reverse=False) -> Tensor:                                                                                                                        #     tensor.py                     :  2570: 
                """                                                                                                                                                                                           
                Adds `self` and `x`.                                                                                                                                                                          
                Equivalent to `self + x`.                                                                                                                                                                     
                Supports broadcasting to a common shape, type promotion, and integer, float, boolean inputs.                                                                                                  
                                                                                                                                                                                                              
                ```python exec="true" source="above" session="tensor" result="python"                                                                                                                         
                Tensor.manual_seed(42)                                                                                                                                                                        
                t = Tensor.randn(4)                                                                                                                                                                           
                print(t.numpy())                                                                                                                                                                              
                ```                                                                                                                                                                                           
                ```python exec="true" source="above" session="tensor" result="python"                                                                                                                         
                print(t.add(20).numpy())                                                                                                                                                                      
                ```                                                                                                                                                                                           
                ```python exec="true" source="above" session="tensor" result="python"                                                                                                                         
                print(t.add(Tensor([[2.0], [3.5]])).numpy())                                                                                                                                                  
                ```                                                                                                                                                                                           
                """                                                                                                                                                                                           
                return F.Add.apply(*self._broadcasted(x, reverse))                                                                                                                                       #     tensor.py                     :  2588: L: {'self': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'x': 2, 'reverse': False}    

                  class Tensor:                                                                                                                                                                          #     tensor.py                     :    92: 
                    def _broadcasted(self, y:Union[Tensor, Node, ConstType], reverse:bool=False, match_dtype:bool=True) -> Tuple[Tensor, Tensor]:                                                        #     tensor.py                     :  2546: 
                      x: Tensor = self                                                                                                                                                                   #     tensor.py                     :  2547: L: {'y': 2, 'match_dtype': True}    
                      if not isinstance(y, Tensor):                                                                                                                                                      #     tensor.py                     :  2548: L: {'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>}    
                        # make y a Tensor
                        assert isinstance(y, (*get_args(ConstType), Node)), f"{type(y)=}, {y=}"                                                                                                          #     tensor.py                     :  2550: 
                        if isinstance(x.dtype, ImageDType) or dtypes.is_float(x.dtype) or (dtypes.is_int(x.dtype) and isinstance(y, int)): y_dtype = x.dtype                                             #     tensor.py                     :  2551: 

                          class Tensor:                                                                                                                                                                  #     tensor.py                     :    92: 
                            @property                                                                                                                                                                    #     tensor.py                     :   188: 
                            def dtype(self) -> DType: return self.lazydata.dtype                                                                                                                              

                        if isinstance(y, Node): y = Tensor.from_node(y, device=x.device)                                                                                                                 #     tensor.py                     :  2553: L: {'y_dtype': dtypes.int}    
                        else: y = Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)                                                                                            #     tensor.py                     :  2554: 

                          class Tensor:                                                                                                                                                                  #     tensor.py                     :    92: 
                            @property                                                                                                                                                                    #     tensor.py                     :   182: 
                            def device(self) -> Union[str, Tuple[str, ...]]: return self.lazydata.device                                                                                                      

                          def _metaop(op, shape:Tuple[sint,...], dtype:DType, device:Union[str, Tuple[str, ...]], arg=None, src:Tuple[LazyBuffer, ...]=()):                                              #     tensor.py                     :    44: 
                            if isinstance(device, str): return LazyBuffer.metaop(op, shape, dtype, device, arg, src)                                                                                     #     tensor.py                     :    45: L: {'device': 'CLANG', 'arg': 2, 'dtype': dtypes.int, 'op': <MetaOps.CONST: 2>, 'shape': (), 'src': ()}    

                  
                      if match_dtype and x.dtype != y.dtype:                                                                                                                                             #     tensor.py                     :  2556: L: {'self': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>, 'reverse': False, 'match_dtype': True, 'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y_dtype': dtypes.int}    
                  
                      if reverse: x, y = y, x                                                                                                                                                            #     tensor.py                     :  2560: 
                  
                      # broadcast
                      out_shape = _broadcast_shape(x.shape, y.shape)                                                                                                                                     #     tensor.py                     :  2563: 

                        class Tensor:                                                                                                                                                                    #     tensor.py                     :    92: 
                          @property                                                                                                                                                                      #     tensor.py                     :   185: 
                          def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape                                                                                                                     

                        def _broadcast_shape(*shapes:Tuple[sint, ...]) -> Tuple[sint, ...]:                                                                                                              #     tensor.py                     :    89: 
                          return tuple(0 if 0 in nth_dim_sizes else max(nth_dim_sizes) for nth_dim_sizes in zip(*_pad_left(*shapes)))                                                                    #     tensor.py                     :    90: L: {'shapes': ((3,), ())}    

                            def _pad_left(*shapes:Tuple[sint, ...]) -> Tuple[Tuple[sint, ...], ...]:                                                                                                     #     tensor.py                     :    86: 
                              max_dim = max(len(shape) for shape in shapes)                                                                                                                              #     tensor.py                     :    87: L: {'shapes': ((3,), ())}    
                              return tuple((1,) * (max_dim - len(shape)) + shape for shape in shapes)                                                                                                    #     tensor.py                     :    88: L: {'max_dim': 1}    

                      return x._broadcast_to(out_shape), y._broadcast_to(out_shape)                                                                                                                      #     tensor.py                     :  2564: L: {'self': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>, 'reverse': False, 'match_dtype': True, 'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y_dtype': dtypes.int, 'out_shape': (3,)}    

                        class Tensor:                                                                                                                                                                    #     tensor.py                     :    92: 
                          # ***** broadcasted elementwise ops *****                                                                                                                                      #     tensor.py                     :  2537: 
                          def _broadcast_to(self, shape:Tuple[sint, ...]) -> Tensor:                                                                                                                          
                            if self.shape == shape: return self                                                                                                                                          #     tensor.py                     :  2538: L: {'shape': (3,)}    

                        class Tensor:                                                                                                                                                                    #     tensor.py                     :    92: 
                          # ***** broadcasted elementwise ops *****                                                                                                                                      #     tensor.py                     :  2537: 
                          def _broadcast_to(self, shape:Tuple[sint, ...]) -> Tensor:                                                                                                                          
                            if self.shape == shape: return self                                                                                                                                          # OLD tensor.py                     :  2538: 
                            if self.ndim > len(shape): raise ValueError(f"cannot broadcast tensor to fewer dimensions. shape={self.shape} to {shape=}")                                                  #     tensor.py                     :  2539: L: {'self': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>}    

                              class Tensor:                                                                                                                                                              #     tensor.py                     :    92: 
                                @property                                                                                                                                                                #     tensor.py                     :  3067: 
                                def ndim(self) -> int:                                                                                                                                                        
                                  """                                                                                                                                                                         
                                  Returns the number of dimensions in the tensor.                                                                                                                             
                                                                                                                                                                                                              
                                  ```python exec="true" source="above" session="tensor" result="python"                                                                                                       
                                  t = Tensor([[1, 2], [3, 4]])                                                                                                                                                
                                  print(t.ndim)                                                                                                                                                               
                                  ```                                                                                                                                                                         
                                  """                                                                                                                                                                         
                                  return len(self.shape)                                                                                                                                                 #     tensor.py                     :  3076: 

                            # first pad left with 1s https://data-apis.org/array-api/latest/API_specification/broadcasting.html
                            padded, _ = _pad_left(self.shape, shape)                                                                                                                                     #     tensor.py                     :  2541: L: {'shape': (3,)}    
                            # for each dimension, check either from_ is 1, or it does not change
                            if any(from_ != 1 and from_ != to for from_,to in zip(padded, shape)): raise ValueError(f"cannot broadcast from shape={self.shape} to {shape=}")                             #     tensor.py                     :  2543: L: {'padded': (1,), '_': (3,)}    
                            return F.Expand.apply(self.reshape(padded), shape=shape)                                                                                                                     #     tensor.py                     :  2544: 

                              class Tensor:                                                                                                                                                              #     tensor.py                     :    92: 
                                def reshape(self, shape, *args) -> Tensor:                                                                                                                               #     tensor.py                     :   804: 
                                  """                                                                                                                                                                         
                                  Returns a tensor with the same data as the original tensor but with a different shape.                                                                                      
                                  `shape` can be passed as a tuple or as separate arguments.                                                                                                                  
                                                                                                                                                                                                              
                                  ```python exec="true" source="above" session="tensor" result="python"                                                                                                       
                                  t = Tensor.arange(6)                                                                                                                                                        
                                  print(t.reshape(2, 3).numpy())                                                                                                                                              
                                  ```                                                                                                                                                                         
                                  """                                                                                                                                                                         
                                  # resolve None and args
                                  new_shape = tuple([s if s is not None else self.shape[i] for i,s in enumerate(argfix(shape, *args))])                                                                  #     tensor.py                     :   815: L: {'shape': (1,), 'args': ()}    

                                    def argfix(*x):                                                                                                                                                      #     helpers.py                    :    20: 
                                      if x and x[0].__class__ in (tuple, list):                                                                                                                          #     helpers.py                    :    21: G: {'_db_tables': set(), 'diskcache_put': <function diskcache_put at 0x789730f5a3b0>, 'diskcache': <function diskcache at 0x789730f5a440>, 'fetch': <function fetch at 0x789730f5a4d0>, 'cpu_time_execution': <function cpu_time_execution at 0x789730f5a560>, 'cpu_objdump': <function cpu_objdump at 0x789730f5a5f0>, 'from_mv': <function from_mv at 0x789730f5a680>, 'to_mv': <function to_mv at 0x789730f5a710>, 'mv_address': <function mv_address at 0x789730f5a7a0>, 'to_char_p_p': <function to_char_p_p at 0x789730f5a830>, 'init_c_struct_t': <functools._lru_cache_wrapper object at 0x789730f51590>, 'init_c_var': <function init_c_var at 0x789730f5a8c0>, 'flat_mv': <function flat_mv at 0x789730f5aa70>, 'tqdm': <class 'tinygrad.helpers.tqdm'>, 'trange': <class 'tinygrad.helpers.trange'>, 'pretty_print': <function pretty_print at 0x789730f5ae60>}    L: {'x': ((1,),)}    
                                        if len(x) != 1: raise ValueError(f"bad arg {x}")                                                                                                                 #     helpers.py                    :    22: 
                                        return tuple(x[0])                                                                                                                                               #     helpers.py                    :    23: 

                                  # resolve -1
                                  if (c := new_shape.count(-1)) > 1: raise RuntimeError(f"only one dimension can be inferred using -1, getting {new_shape}")                                             #     tensor.py                     :   817: L: {'new_shape': (1,)}    
                                  if c: new_shape = tuple([-prod(self.shape) // prod(new_shape) if s == -1 else s for s in new_shape])                                                                   #     tensor.py                     :   818: L: {'c': 0}    
                                  return F.Reshape.apply(self, shape=new_shape) if new_shape != self.shape else self                                                                                     #     tensor.py                     :   819: 

                                    class Function:                                                                                                                                                      #     tensor.py                     :    23: 
                                      @classmethod                                                                                                                                                       #     tensor.py                     :    35: 
                                      def apply(fxn:Type[Function], *x:Tensor, **kwargs) -> Tensor:                                                                                                           
                                        ctx = fxn(x[0].device, *x, metadata=_METADATA.get())                                                                                                             #     tensor.py                     :    36: L: {'fxn': <class 'tinygrad.function.Reshape'>, 'x': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,), 'kwargs': {'shape': (1,)}}    

                                          class Function:                                                                                                                                                #     tensor.py                     :    23: 
                                            def __init__(self, device:Union[str, Tuple[str, ...]], *tensors:Tensor, metadata:Optional[Metadata]=None):                                                   #     tensor.py                     :    24: 
                                              self.device = device                                                                                                                                       #     tensor.py                     :    25: L: {'self': <tinygrad.function.Reshape object at 0x78971f2b0490>, 'device': 'CLANG', 'metadata': __add__, 'tensors': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,)}    
                                              self.needs_input_grad = [t.requires_grad for t in tensors]                                                                                                 #     tensor.py                     :    26: 
                                              self.requires_grad = True if any(self.needs_input_grad) else None if None in self.needs_input_grad else False                                              #     tensor.py                     :    27: 
                                              if self.requires_grad: self.parents = tensors                                                                                                              #     tensor.py                     :    28: 
                                              self.metadata = metadata                                                                                                                                   #     tensor.py                     :    29: 

                                        ret = Tensor.__new__(Tensor)                                                                                                                                     #     tensor.py                     :    37: L: {'fxn': <class 'tinygrad.function.Reshape'>, 'x': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,), 'kwargs': {'shape': (1,)}, 'ctx': <tinygrad.function.Reshape object at 0x78971f2b0490>}    
                                        ret.lazydata, ret.requires_grad, ret.grad = ctx.forward(*[t.lazydata for t in x], **kwargs), ctx.requires_grad, None                                             #     tensor.py                     :    38: 

                                          class Reshape(Function):                                                                                                                                       #     function.py                   :   176: 
                                            def forward(self, x:LazyBuffer, shape:Tuple[int, ...]) -> LazyBuffer:                                                                                        #     function.py                   :   177: 
                                              self.input_shape = x.shape                                                                                                                                 #     function.py                   :   178: G: {'sint': typing.Union[int, tinygrad.shape.symbolic.Variable, tinygrad.shape.symbolic.MulNode, tinygrad.shape.symbolic.SumNode], 'Contiguous': <class 'tinygrad.function.Contiguous'>, 'ContiguousBackward': <class 'tinygrad.function.ContiguousBackward'>, 'Cast': <class 'tinygrad.function.Cast'>, 'Reciprocal': <class 'tinygrad.function.Reciprocal'>, 'Sin': <class 'tinygrad.function.Sin'>, 'Relu': <class 'tinygrad.function.Relu'>, 'Log': <class 'tinygrad.function.Log'>, 'Exp': <class 'tinygrad.function.Exp'>, 'Sqrt': <class 'tinygrad.function.Sqrt'>, 'Sigmoid': <class 'tinygrad.function.Sigmoid'>, 'Sign': <class 'tinygrad.function.Sign'>, 'Less': <class 'tinygrad.function.Less'>, 'Neq': <class 'tinygrad.function.Neq'>, 'Xor': <class 'tinygrad.function.Xor'>, 'BitwiseAnd': <class 'tinygrad.function.BitwiseAnd'>, 'BitwiseOr': <class 'tinygrad.function.BitwiseOr'>, 'Threefry': <class 'tinygrad.function.Threefry'>, 'Add': <class 'tinygrad.function.Add'>, 'Mul': <class 'tinygrad.function.Mul'>, 'IDiv': <class 'tinygrad.function.IDiv'>, 'Where': <class 'tinygrad.function.Where'>, 'Sum': <class 'tinygrad.function.Sum'>, 'Max': <class 'tinygrad.function.Max'>, 'Expand': <class 'tinygrad.function.Expand'>, 'Reshape': <class 'tinygrad.function.Reshape'>, 'Permute': <class 'tinygrad.function.Permute'>, 'Pad': <class 'tinygrad.function.Pad'>, 'Shrink': <class 'tinygrad.function.Shrink'>, 'Flip': <class 'tinygrad.function.Flip'>}    L: {'self': <tinygrad.function.Reshape object at 0x78971f2b0490>, 'x': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'shape': (1,)}    
                                              return x.reshape(shape)                                                                                                                                    #     function.py                   :   179: 

                                                class LazyBuffer:                                                                                                                                        #     lazy.py                       :    26: 
                                                  def reshape(self, arg:Tuple[sint, ...]): return self._view(self.st.reshape(arg))                                                                       #     lazy.py                       :   211: 

                                                    @dataclass(frozen=True)                                                                                                                              #     shape/shapetracker.py         :    36: 
                                                    class ShapeTracker:                                                                                                                                       
                                                      def reshape(self, new_shape: Tuple[sint, ...]) -> ShapeTracker:                                                                                    #     shape/shapetracker.py         :   151: 
                                                        if getenv("MERGE_VIEW", 1) and (new_view := self.views[-1].reshape(new_shape)) is not None: return ShapeTracker(self.views[0:-1] + (new_view,))  #     shape/shapetracker.py         :   152: G: {'ShapeTracker': <class 'tinygrad.shape.shapetracker.ShapeTracker'>}    L: {'self': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), 'new_shape': (1,)}    

                                                          @dataclass(frozen=True)                                                                                                                        #     shape/view.py                 :    85: 
                                                          class View:                                                                                                                                         
                                                            @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                             #     shape/view.py                 :   267: 
                                                            def reshape(self, new_shape: Tuple[sint, ...]) -> Optional[View]:                                                                                 
                                                              if self.shape == new_shape: return self                                                                                                    #     shape/view.py                 :   268: L: {'self': View(shape=(), strides=(), offset=0, mask=None, contiguous=True), 'new_shape': (1,)}    
                                                          
                                                              assert all(x >= 0 for x in new_shape), f"shape can't contain negative numbers {new_shape}"                                                 #     shape/view.py                 :   270: 
                                                              if 0 in self.shape:                                                                                                                        #     shape/view.py                 :   271: 
                                                              # check for the same size
                                                              if (self_all_int := all_int(self.shape)):                                                                                                  #     shape/view.py                 :   275: 
                                                                assert all(isinstance(s, (int, Variable)) for s in new_shape), f"{self.shape=} -> {new_shape=} contains non (int, Variable) dim"         #     shape/view.py                 :   276: L: {'self_all_int': True}    
                                                                if prod(self.shape) != prod([s if isinstance(s, int) else cast(Variable,s).val for s in new_shape]):                                     #     shape/view.py                 :   277: 
                                                          
                                                              if new_shape == () and self.mask and any(mx==my for (mx,my) in self.mask): return None                                                     #     shape/view.py                 :   280: 
                                                          
                                                              # after the asserts, it's okay to check contiguous
                                                              if self.contiguous: return View.create(new_shape)                                                                                          #     shape/view.py                 :   283: 

                                                    class LazyBuffer:                                                                                                                                    #     lazy.py                       :    26: 
                                                      def _view(self, new_st:ShapeTracker) -> LazyBuffer:                                                                                                #     lazy.py                       :   205: 
                                                        if self.st.size == 0 or (new_st.views[-1].mask is not None and any((x[1]-x[0]) == 0 for x in new_st.views[-1].mask)):                            # OLD lazy.py                       :   206: 
                                                        if new_st.contiguous and self.base.shape == new_st.shape: return self.base                                                                       # OLD lazy.py                       :   208: 
                                                        return create_lazybuffer(self.device, new_st, self.dtype, base=self.base)                                                                        #     lazy.py                       :   209: L: {'self': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'new_st': ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))}    

                                                          class LazyBuffer:                                                                                                                              #     lazy.py                       :    26: 
                                                            def __init__(self, device:str, st:ShapeTracker, dtype:DTypeLike,                                                                             #     lazy.py                       :    27: 
                                                                         op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),                                                                  
                                                                         base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):                                                                   
                                                              self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, to_dtype(dtype), st.shape, st.size, metadata          # OLD lazy.py                       :    30: 
                                                              self._base: Optional[LazyBuffer] = None                                                                                                    # OLD lazy.py                       :    31: 
                                                              if base is None:                                                                                                                           # OLD lazy.py                       :    32: 
                                                                # properties on base
                                                                self.op, self.arg, self.srcs = op, arg, srcs  # this is a UOp, except the src is LazyBuffers and not UOps                                # OLD lazy.py                       :    34: 
                                                                assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"                              # OLD lazy.py                       :    35: 
                                                          
                                                                if self.op is MetaOps.VIEW:                                                                                                              # OLD lazy.py                       :    37: 
                                                                  self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, self.dtype)                              # OLD lazy.py                       :    41: 
                                                                self.buffer.ref(1)                                                                                                                       # OLD lazy.py                       :    42: 
                                                                self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None                                                   # OLD lazy.py                       :    43: 
                                                                self.forced_realize = False                                                                                                              # OLD lazy.py                       :    44: 
                                                              else:                                                                                                                                      #     lazy.py                       :    45: 
                                                                # properties on view
                                                                assert base.base == base, "base must be a base itself"                                                                                   #     lazy.py                       :    47: L: {'device': 'CLANG', 'st': ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),)), 'dtype': dtypes.int, 'srcs': (), 'base': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'metadata': __add__}    
                                                                self._base = base                                                                                                                        #     lazy.py                       :    48: 

                                        ret._ctx = ctx if ctx.requires_grad and not Tensor.no_grad else None  # used by autograd engine                                                                  #     tensor.py                     :    39: L: {'ret': <Tensor <LB CLANG (1,) int ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))> on CLANG with grad None>}    
                                        return ret                                                                                                                                                       #     tensor.py                     :    40: 

                              # NOTE: this is sum in reverse                                                                                                                                             #     function.py                   :   168: 
                              class Expand(Function):                                                                                                                                                         
                                def forward(self, x:LazyBuffer, shape:Tuple[int, ...]) -> LazyBuffer:                                                                                                    #     function.py                   :   169: 
                                  self.expanded_axis = tuple(i for i, (si, so) in enumerate(zip(x.shape, shape)) if si != so)                                                                            #     function.py                   :   170: L: {'self': <tinygrad.function.Expand object at 0x789735f922f0>, 'x': <LB CLANG (1,) int ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))>, 'shape': (3,)}    
                                  return x.expand(shape)                                                                                                                                                 #     function.py                   :   171: 

                                    class LazyBuffer:                                                                                                                                                    #     lazy.py                       :    26: 
                                      def expand(self, arg:Tuple[sint, ...]): return self._view(self.st.expand(arg))                                                                                     #     lazy.py                       :   213: 

                                        @dataclass(frozen=True)                                                                                                                                          #     shape/shapetracker.py         :    36: 
                                        class ShapeTracker:                                                                                                                                                   
                                          def expand(self, new_shape: Tuple[sint, ...]) -> ShapeTracker: return ShapeTracker(self.views[0:-1] + (self.views[-1].expand(new_shape), ))                    #     shape/shapetracker.py         :   147: 

                                            @dataclass(frozen=True)                                                                                                                                      #     shape/view.py                 :    85: 
                                            class View:                                                                                                                                                       
                                              @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                           #     shape/view.py                 :   239: 
                                              def expand(self, new_shape: Tuple[sint, ...]) -> View:                                                                                                          
                                                if len(new_shape) != len(self.shape): raise ValueError(f"expand arg {new_shape=} must have same number of dimensions as shape {self.shape=}")            #     shape/view.py                 :   240: L: {'self': View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True), 'new_shape': (3,)}    
                                                if 0 in self.shape:                                                                                                                                      #     shape/view.py                 :   241: 
                                                assert all((s == x or (s == 1 and st == 0)) for s,x,st in zip(self.shape, new_shape, self.strides)), f"can't expand {self.shape} into {new_shape}"       #     shape/view.py                 :   244: 
                                                # NOTE: can the mask ever be (0,0)?
                                                mask = tuple([(((0,0) if m != (0,1) else (0,ns)) if s != ns else m) for m,s,ns in zip(self.mask, self.shape, new_shape)]) if self.mask else None         #     shape/view.py                 :   246: 
                                                return View.create(new_shape, self.strides, self.offset, mask)                                                                                           #     shape/view.py                 :   247: 

                  class Add(Function):                                                                                                                                                                   #     function.py                   :   114: 
                    def forward(self, x:LazyBuffer, y:LazyBuffer) -> LazyBuffer: return x.e(BinaryOps.ADD, y)                                                                                            #     function.py                   :   115: 

                      class LazyBuffer:                                                                                                                                                                  #     lazy.py                       :    26: 
                        def e(self, op:Union[MetaOps, UnaryOps, BinaryOps, TernaryOps], *in_srcs:LazyBuffer, arg:Optional[Any]=None) -> LazyBuffer:                                                      #     lazy.py                       :   137: 
                          srcs: List[LazyBuffer] = []                                                                                                                                                    #     lazy.py                       :   138: L: {'self': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'op': <BinaryOps.ADD: 1>, 'in_srcs': (<LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>,)}    
                          for s in (self,)+in_srcs:                                                                                                                                                      #     lazy.py                       :   139: L: {'srcs': []}    
                            if s == s.base and s.base.contiguous_child and (root:=s.base.contiguous_child[0]()) is not None:                                                                             #     lazy.py                       :   140: L: {'s': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}    
                            else:                                                                                                                                                                        #     lazy.py                       :   142: 
                              srcs.append(s)                                                                                                                                                             #     lazy.py                       :   143: 
                          if not all_same(dts:=[x.dtype.scalar() for x in (srcs[1:] if op is TernaryOps.WHERE else srcs)]):                                                                              #     lazy.py                       :   144: L: {'s': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}    
                          assert all_same([x.shape for x in srcs]), f"all shapes must be the same {[x.shape for x in srcs]}"                                                                             #     lazy.py                       :   146: L: {'dts': [dtypes.int, dtypes.int]}    
                          if op is TernaryOps.WHERE: assert srcs[0].dtype == dtypes.bool, "TernaryOps.WHERE must have the first arg be bool"                                                             #     lazy.py                       :   147: 
                      
                          out_dtype = dtypes.bool if op in (BinaryOps.CMPLT, BinaryOps.CMPNE) else srcs[-1].dtype                                                                                        #     lazy.py                       :   149: 
                      
                          # const folding
                          if op in python_alu and all(s.is_unrealized_unmasked_const() for s in srcs):                                                                                                   #     lazy.py                       :   152: L: {'out_dtype': dtypes.int}    

                            class LazyBuffer:                                                                                                                                                            #     lazy.py                       :    26: 
                              def is_unrealized_unmasked_const(self): return self.is_unrealized_const() and all(v.mask is None for v in self.st.views)                                                   #     lazy.py                       :   114: 

                          if op in BinaryOps:                                                                                                                                                            #     lazy.py                       :   154: 
                            x, y = self, in_srcs[0]                                                                                                                                                      #     lazy.py                       :   155: 
                            if op is BinaryOps.ADD:                                                                                                                                                      #     lazy.py                       :   156: L: {'x': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'y': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}    
                              if y.is_unrealized_unmasked_const() and y.base.arg == 0: return x                                                                                                          #     lazy.py                       :   157: 
                              if x.is_unrealized_unmasked_const() and x.base.arg == 0: return y                                                                                                          #     lazy.py                       :   158: 
                            if op is BinaryOps.MUL:                                                                                                                                                      #     lazy.py                       :   159: 
                      
                          return create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))                                                                    #     lazy.py                       :   163: 

      _METADATA.reset(token)                                                                                                                                                                             #     tensor.py                     :  3369: L: {'args': (<Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 2), 'kwargs': {}, 'fn': <function Tensor.__add__ at 0x78971f1f13f0>, 'caller': '', 'token': <Token var=<ContextVar name='_METADATA' default=None at 0x789730f470b0> at 0x78972db4eb80>, 'ret': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}    
      return ret                                                                                                                                                                                         #     tensor.py                     :  3370: 

a.tolist()                                                                                                                                                                                               #     ...grad.tensor.tolist_CLANG.py:     3: G: {'a': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}    

  class Tensor:                                                                                                                                                                                          #     tensor.py                     :    92: 
    # TODO: should be Tensor.tolist() -> Union[List[ConstType], ConstType]. The List is Sequence because mypy expects memoryview.tolist() -> list[int]                                                   #     tensor.py                     :   282: 
    # src: https://github.com/python/mypy/blob/release-1.6/mypy/typeshed/stdlib/builtins.pyi#L803                                                                                                             
    def tolist(self) -> Union[Sequence[ConstType], ConstType]:                                                                                                                                                
      """                                                                                                                                                                                                     
      Returns the value of this tensor as a nested list.                                                                                                                                                      
                                                                                                                                                                                                              
      ```python exec="true" source="above" session="tensor" result="python"                                                                                                                                   
      t = Tensor([1, 2, 3, 4])                                                                                                                                                                                
      print(t.tolist())                                                                                                                                                                                       
      ```                                                                                                                                                                                                     
      """                                                                                                                                                                                                     
      return self.data().tolist()                                                                                                                                                                        #     tensor.py                     :   291: L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}    

        class Tensor:                                                                                                                                                                                    #     tensor.py                     :    92: 
          def data(self) -> memoryview:                                                                                                                                                                  #     tensor.py                     :   254: 
            """                                                                                                                                                                                               
            Returns the data of this tensor as a memoryview.                                                                                                                                                  
                                                                                                                                                                                                              
            ```python exec="true" source="above" session="tensor" result="python"                                                                                                                             
            t = Tensor([1, 2, 3, 4])                                                                                                                                                                          
            print(np.frombuffer(t.data(), dtype=np.int32))                                                                                                                                                    
            ```                                                                                                                                                                                               
            """                                                                                                                                                                                               
            assert self.dtype.fmt is not None, f"no fmt dtype for {self.dtype}"                                                                                                                          #     tensor.py                     :   263: 
            assert all_int(self.shape), f"no data if shape is symbolic, {self.shape=}"                                                                                                                   #     tensor.py                     :   264: 
            return self._data().cast(self.dtype.fmt, self.shape)                                                                                                                                         #     tensor.py                     :   265: 

              class Tensor:                                                                                                                                                                              #     tensor.py                     :    92: 
                def _data(self) -> memoryview:                                                                                                                                                           #     tensor.py                     :   246: 
                  if 0 in self.shape: return memoryview(bytearray(0))                                                                                                                                    #     tensor.py                     :   247: 
                  # NOTE: this realizes on the object from as_buffer being a Python object
                  cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()                                                                                                                #     tensor.py                     :   249: 

                    class Tensor:                                                                                                                                                                        #     tensor.py                     :    92: 
                      def cast(self, dtype:DTypeLike) -> Tensor:                                                                                                                                         #     tensor.py                     :  3144: 
                        """                                                                                                                                                                                   
                        Casts `self` to the given `dtype`.                                                                                                                                                    
                                                                                                                                                                                                              
                        ```python exec="true" source="above" session="tensor" result="python"                                                                                                                 
                        t = Tensor([-1, 2.5, 3], dtype=dtypes.float)                                                                                                                                          
                        print(t.dtype, t.numpy())                                                                                                                                                             
                        ```                                                                                                                                                                                   
                        ```python exec="true" source="above" session="tensor" result="python"                                                                                                                 
                        t = t.cast(dtypes.int32)                                                                                                                                                              
                        print(t.dtype, t.numpy())                                                                                                                                                             
                        ```                                                                                                                                                                                   
                        """                                                                                                                                                                                   
                        return self if self.dtype == (dt:=to_dtype(dtype)) else F.Cast.apply(self, dtype=dt)                                                                                             #     tensor.py                     :  3157: L: {'dtype': dtypes.int}    

                    class Tensor:                                                                                                                                                                        #     tensor.py                     :    92: 
                      def contiguous(self):                                                                                                                                                              #     tensor.py                     :  2070: 
                        """                                                                                                                                                                                   
                        Returns a contiguous tensor.                                                                                                                                                          
                        """                                                                                                                                                                                   
                        return F.Contiguous.apply(self)                                                                                                                                                  #     tensor.py                     :  2074: 

                          class Contiguous(Function):                                                                                                                                                    #     function.py                   :    11: 
                            def forward(self, x:LazyBuffer) -> LazyBuffer: return x.contiguous()                                                                                                         #     function.py                   :    12: 

                              class LazyBuffer:                                                                                                                                                          #     lazy.py                       :    26: 
                                def contiguous(self, allow_buffer_view=True):                                                                                                                            #     lazy.py                       :    87: 
                                  if not self.st.contiguous or self.size != self.base.size or self.is_unrealized_const():                                                                                #     lazy.py                       :    88: L: {'self': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'allow_buffer_view': True}    
                                  self.base.forced_realize = True                                                                                                                                        #     lazy.py                       :    92: 
                                  return self                                                                                                                                                            #     lazy.py                       :    93: 

                    class Tensor:                                                                                                                                                                        #     tensor.py                     :    92: 
                      def to(self, device:Optional[Union[str, Tuple[str, ...]]]) -> Tensor:                                                                                                              #     tensor.py                     :   307: 
                        """                                                                                                                                                                                   
                        Moves the tensor to the given device.                                                                                                                                                 
                        """                                                                                                                                                                                   
                        device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)                                                     #     tensor.py                     :   311: L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>, 'device': 'CLANG'}    
                        if device == self.device: return self                                                                                                                                            #     tensor.py                     :   312: L: {'device': 'CLANG'}    

                    class Tensor:                                                                                                                                                                        #     tensor.py                     :    92: 
                      def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:                                                                                                                    #     tensor.py                     :   206: 
                        """Triggers the computation needed to create these Tensor(s)."""                                                                                                                      
                        run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)                                                                                                    #     tensor.py                     :   208: L: {'do_update_stats': True, 'lst': ()}    

                          class Tensor:                                                                                                                                                                  #     tensor.py                     :    92: 
                            def schedule_with_vars(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:                                            #     tensor.py                     :   192: 
                              """Creates the schedule needed to realize these Tensor(s), with Variables."""                                                                                                   
                              if getenv("FUZZ_SCHEDULE"):                                                                                                                                                #     tensor.py                     :   194: 
                              schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)                                                                       #     tensor.py                     :   197: 

                                class LazyBuffer:                                                                                                                                                        #     lazy.py                       :    26: 
                                  # same API as multi                                                                                                                                                    #     lazy.py                       :    67: 
                                  @property                                                                                                                                                                   
                                  def lbs(self) -> List[LazyBuffer]: return [self]                                                                                                                            

                                def flatten(l:Iterable[Iterable[T]]): return [item for sublist in l for item in sublist]                                                                                 #     helpers.py                    :    34: 

                                def create_schedule_with_vars(outs:List[LazyBuffer], seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:                             #     engine/schedule.py            :   384: 
                                  if seen is None: seen = set()                                                                                                                                          #     engine/schedule.py            :   385: G: {'SCHEDULES': [], '_graph_schedule': <function _graph_schedule at 0x78971f3a7130>, 'create_schedule_with_vars': <function create_schedule_with_vars at 0x78971f3a71c0>, 'create_schedule': <function create_schedule at 0x78971f3a7250>}    L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>]}    
                                  graph, in_degree = _graph_schedule(outs, seen)                                                                                                                         #     engine/schedule.py            :   386: L: {'seen': set()}    

                                    def _graph_schedule(outs:List[LazyBuffer], seen:Set[LazyBuffer]) -> \                                                                                                #     engine/schedule.py            :   263: 
                                      Tuple[DefaultDict[LBScheduleItem, List[LBScheduleItem]],  # this is the graph                                                                                           
                                            DefaultDict[LBScheduleItem, int]]:                  # this is the in-degree of the graph                                                                          
                                      """create a graph for realizing the outputs"""                                                                                                                          
                                      # start by just realizing the buffers passed in
                                      realizes: Dict[LazyBuffer, None] = {x.base:None for x in outs if x.base.realized is None}                                                                          #     engine/schedule.py            :   268: 
                                      allbufs: Dict[LazyBuffer, None] = {}                                                                                                                               #     engine/schedule.py            :   269: L: {'realizes': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}}    
                                      simple_pads: Dict[LazyBuffer, None] = {}                                                                                                                           #     engine/schedule.py            :   270: L: {'allbufs': {}}    
                                      children: DefaultDict[LazyBuffer, Dict[LazyBuffer, None]] = defaultdict(dict)                                                                                      #     engine/schedule.py            :   271: L: {'simple_pads': {}}    
                                      assign_targets: Dict[LazyBuffer, LazyBuffer] = {}                                                                                                                  #     engine/schedule.py            :   272: L: {'children': defaultdict(<class 'dict'>, {})}    
                                      double_reduces: Dict[LazyBuffer, None] = {}                                                                                                                        #     engine/schedule.py            :   273: L: {'assign_targets': {}}    
                                      for out in outs: _recurse_lb(out.base, realizes, allbufs, simple_pads, children, assign_targets, double_reduces, scheduled=True)                                   #     engine/schedule.py            :   274: L: {'double_reduces': {}}    

                                        def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                             #     engine/schedule.py            :   188: 
                                                        children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],                                                
                                                        double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:                                                                                      
                                          """recursively search the entire graph for all LazyBuffers, insert realizes after expands"""                                                                        
                                          if buf in allbufs or buf.base.realized is not None: return                                                                                                     #     engine/schedule.py            :   192: L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'scheduled': True}    
                                          if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                       #     engine/schedule.py            :   193: 
                                          # check if we need to realize views
                                          if buf is not buf.base:                                                                                                                                        #     engine/schedule.py            :   195: 
                                          if buf.op in ReduceOps and buf.srcs[0].base.op is buf.op and buf.srcs[0] is not buf.srcs[0].base: double_reduces[buf] = None                                   #     engine/schedule.py            :   209: 
                                          allbufs[buf] = None                                                                                                                                            #     engine/schedule.py            :   210: 
                                          if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None                                                                                               #     engine/schedule.py            :   211: 
                                          if buf.op is MetaOps.ASSIGN:                                                                                                                                   #     engine/schedule.py            :   212: 
                                          if buf.op is MetaOps.COPY:                                                                                                                                     #     engine/schedule.py            :   216: 
                                          if buf.op is MetaOps.VIEW: realizes[buf.srcs[0].base] = None                                                                                                   #     engine/schedule.py            :   219: 
                                          for x in buf.srcs:                                                                                                                                             #     engine/schedule.py            :   220: 
                                            if x.base.realized is None: children[x.base][buf] = None                                                                                                     #     engine/schedule.py            :   221: L: {'x': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}    
                                            _recurse_lb(x, realizes, allbufs, simple_pads, children, assign_targets, double_reduces)                                                                     #     engine/schedule.py            :   222: 

                                              def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                       #     engine/schedule.py            :   188: 
                                                              children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],                                          
                                                              double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:                                                                                
                                                if buf in allbufs or buf.base.realized is not None: return                                                                                               # OLD engine/schedule.py            :   192: 
                                                if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                 # OLD engine/schedule.py            :   193: 
                                                # check if we need to realize views
                                                if buf is not buf.base:                                                                                                                                  # OLD engine/schedule.py            :   195: 
                                                if buf.op in ReduceOps and buf.srcs[0].base.op is buf.op and buf.srcs[0] is not buf.srcs[0].base: double_reduces[buf] = None                             # OLD engine/schedule.py            :   209: 
                                                allbufs[buf] = None                                                                                                                                      # OLD engine/schedule.py            :   210: 
                                                if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None                                                                                         # OLD engine/schedule.py            :   211: 
                                                if buf.op is MetaOps.ASSIGN:                                                                                                                             # OLD engine/schedule.py            :   212: 
                                                if buf.op is MetaOps.COPY:                                                                                                                               # OLD engine/schedule.py            :   216: 
                                                  assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"                                                 #     engine/schedule.py            :   217: L: {'buf': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'scheduled': False}    
                                                  realizes[buf.srcs[0].base] = None                                                                                                                      #     engine/schedule.py            :   218: 

                                              def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                       #     engine/schedule.py            :   188: 
                                                              children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],                                          
                                                              double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:                                                                                
                                                if buf in allbufs or buf.base.realized is not None: return                                                                                               # OLD engine/schedule.py            :   192: 
                                                if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                 # OLD engine/schedule.py            :   193: 
                                                # check if we need to realize views
                                                if buf is not buf.base:                                                                                                                                  # OLD engine/schedule.py            :   195: 
                                                  # fuse some pads
                                                  if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \                                                   #     engine/schedule.py            :   197: L: {'buf': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}    
                                                      prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):                                                                                
                                                  # realize all expands
                                                  elif prod(buf.base.st.shape) < prod(buf.st.shape):                                                                                                     #     engine/schedule.py            :   201: 
                                                    # this was causing "test_lil_model" to fail
                                                    if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):                         #     engine/schedule.py            :   203: 
                                                    else: realizes[buf.base] = None                                                                                                                      #     engine/schedule.py            :   205: 
                                                  return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children, assign_targets, double_reduces)                                                 #     engine/schedule.py            :   208: 

                                    
                                      # check if we have to realize pads
                                      for p in simple_pads:                                                                                                                                              #     engine/schedule.py            :   277: L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>}    
                                    
                                      # find all reduces, and pair them to a elementwise op. if they can't be cleanly paired, force realize the reduce (or a contig child)
                                      reduce_for_op: Dict[LazyBuffer, LazyBuffer] = {}                                                                                                                   #     engine/schedule.py            :   282: 
                                      reduce_of_const: List[LazyBuffer] = []                                                                                                                             #     engine/schedule.py            :   283: L: {'reduce_for_op': {}}    
                                      for r in allbufs:                                                                                                                                                  #     engine/schedule.py            :   284: L: {'reduce_of_const': []}    
                                        if r.op not in ReduceOps or r in realizes: continue                                                                                                              #     engine/schedule.py            :   285: L: {'r': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>}    
                                    
                                      # fuse double reduces with no other child
                                      if FUSE_CONV_BW:                                                                                                                                                   #     engine/schedule.py            :   325: L: {'r': <LB CLANG () int (<MetaOps.CONST: 2>, None)>}    
                                    
                                      for r in reduce_of_const:                                                                                                                                          #     engine/schedule.py            :   330: 
                                    
                                      output_groups: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)                                                                                       #     engine/schedule.py            :   339: 
                                      for buf in realizes:                                                                                                                                               #     engine/schedule.py            :   340: L: {'output_groups': defaultdict(<class 'list'>, {})}    
                                        if buf.realized is not None or buf.op is MetaOps.CONST or buf in seen: continue                                                                                  #     engine/schedule.py            :   341: L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>}    
                                        output_groups[reduce_for_op[buf] if buf in reduce_for_op and MULTIOUTPUT else buf].append(buf)                                                                   #     engine/schedule.py            :   342: 
                                    
                                        # make things that can't be images not images
                                        if isinstance(buf.dtype, ImageDType) and (prod(buf.shape) != prod(buf.dtype.shape) or                                                                            #     engine/schedule.py            :   345: 
                                                                                  not any(buf.shape[x]%4 == 0 for x in buf.st.unit_stride_axes())):                                                           
                                    
                                      # preschedule all buffers in realizes
                                      prescheduled = [_lower_lazybuffer(group, realizes) for group in output_groups.values()]                                                                            #     engine/schedule.py            :   356: L: {'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>}    

                                        def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None]) -> LBScheduleItem:                                                                 #     engine/schedule.py            :   146: 
                                          """describe the computation for a LazyBuffer with UOp + inputs + var_vals"""                                                                                        
                                          if (out:=outs[0]).op is MetaOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:                           #     engine/schedule.py            :   148: L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>]}    
                                          if out.op in {MetaOps.CUSTOM, MetaOps.COPY, MetaOps.EMPTY, MetaOps.VIEW}:                                                                                      #     engine/schedule.py            :   153: L: {'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>}    
                                          # push through all movementops between reduceops
                                          reduce_info: Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]] = {}                                                                  #     engine/schedule.py            :   156: 
                                          seen_ops: Dict[Tuple[LazyBuffer, ShapeTracker], Optional[Tuple[LazyBuffer, ShapeTracker]]] = {}                                                                #     engine/schedule.py            :   157: L: {'reduce_info': {}}    
                                          for out in outs: _recurse_reduceops(out, out.st, realizes, outs, reduce_info, seen_ops)                                                                        #     engine/schedule.py            :   158: L: {'seen_ops': {}}    

                                            def _recurse_reduceops(buf:LazyBuffer, st:ShapeTracker, realizes:Dict[LazyBuffer, None], outs:List[LazyBuffer],                                              #     engine/schedule.py            :   106: 
                                                                   reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],                                                   
                                                                   cache:Dict[Tuple[LazyBuffer, ShapeTracker], Optional[Tuple[LazyBuffer, ShapeTracker]]]) -> \                                               
                                                                     Optional[Tuple[LazyBuffer, ShapeTracker]]:                                                                                               
                                              if (buf, st) in cache: return cache[(buf, st)]                                                                                                             #     engine/schedule.py            :   110: L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'cache': {}}    
                                              if buf.base.realized is not None or (buf.base in realizes and buf.base not in outs): return None                                                           #     engine/schedule.py            :   111: 
                                              if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                      #     engine/schedule.py            :   112: 
                                              input_st = ShapeTracker.from_shape(buf.srcs[0].shape) if buf.op in ReduceOps else st                                                                       #     engine/schedule.py            :   113: 
                                              reduce_srcs = [r for x in buf.srcs if (r:=_recurse_reduceops(x, input_st, realizes, outs, reduce_info, cache)) is not None]                                #     engine/schedule.py            :   114: L: {'input_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                              top_reduce = reduce_srcs[-1] if len(reduce_srcs) != 0 else None                                                                                            #     engine/schedule.py            :   115: L: {'reduce_srcs': []}    
                                              if buf.op in ReduceOps:                                                                                                                                    #     engine/schedule.py            :   116: 
                                              return cache.setdefault((buf, st), top_reduce)                                                                                                             #     engine/schedule.py            :   144: 

                                          # pad all reduceops to the max of each dimension
                                          shape_dims = [sorted(dedup(dims)) for dims in zip(*[input_st.shape for input_st,_ in reduce_info.values()])]                                                   #     engine/schedule.py            :   160: L: {'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'seen_ops': {(<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))): None}}    
                                          for i,dims in enumerate(shape_dims):                                                                                                                           #     engine/schedule.py            :   161: L: {'shape_dims': []}    
                                          # create the stores
                                          var_vals = merge_dicts([out.st.var_vals.copy() for out in outs])                                                                                               #     engine/schedule.py            :   168: 

                                            @dataclass(frozen=True)                                                                                                                                      #     shape/shapetracker.py         :    36: 
                                            class ShapeTracker:                                                                                                                                               
                                              @property                                                                                                                                                  #     shape/shapetracker.py         :    94: 
                                              def var_vals(self) -> Dict[Variable, int]: return merge_dicts([dict([v.unbind()]) for v in self.vars()])                                                        

                                                @dataclass(frozen=True)                                                                                                                                  #     shape/shapetracker.py         :    36: 
                                                class ShapeTracker:                                                                                                                                           
                                                  def vars(self) -> Set[Variable]: return set().union(*[v.vars() for v in self.views])                                                                   #     shape/shapetracker.py         :    91: 

                                                    @dataclass(frozen=True)                                                                                                                              #     shape/view.py                 :    85: 
                                                    class View:                                                                                                                                               
                                                      @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                           #     shape/view.py                 :   120: 
                                                      def vars(self) -> Set[Variable]:                                                                                                                        
                                                        flatten_mask = tuple(x for m in self.mask for x in m) if self.mask is not None else tuple()                                                      #     shape/view.py                 :   121: L: {'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}    
                                                        return functools.reduce(operator.or_, [x.vars() for x in self.shape+self.strides+(self.offset,)+flatten_mask if isinstance(x, Node)], set())     #     shape/view.py                 :   122: L: {'flatten_mask': ()}    

                                                def merge_dicts(ds:Iterable[Dict[T,U]]) -> Dict[T,U]:                                                                                                    #     helpers.py                    :    41: 
                                                  kvs = set([(k,v) for d in ds for k,v in d.items()])                                                                                                    #     helpers.py                    :    42: L: {'ds': []}    
                                                  assert len(kvs) == len(set(kv[0] for kv in kvs)), f"cannot merge, {kvs} contains different values for the same key"                                    #     helpers.py                    :    43: L: {'kvs': set()}    
                                                  return {k:v for d in ds for k,v in d.items()}                                                                                                          #     helpers.py                    :    44: 

                                          assign_targets = {x.srcs[1]:x for x in outs if x.op is MetaOps.ASSIGN}                                                                                         #     engine/schedule.py            :   169: L: {'var_vals': {}}    
                                          cache: Dict[Tuple[LazyBuffer, ShapeTracker], UOp] = {}                                                                                                         #     engine/schedule.py            :   170: L: {'assign_targets': {}}    
                                          ast: List[UOp] = []                                                                                                                                            #     engine/schedule.py            :   171: L: {'cache': {}}    
                                          inputs: Dict[LazyBuffer, int] = {}                                                                                                                             #     engine/schedule.py            :   172: L: {'ast': []}    
                                          for i, out in enumerate(outs):                                                                                                                                 #     engine/schedule.py            :   173: L: {'inputs': {}}    
                                            output_st = ShapeTracker.from_shape(ShapeTracker.reduce(*deque(reduce_info.values(), 1).pop()) if reduce_info else out.shape)                                #     engine/schedule.py            :   174: L: {'i': 0}    
                                            src = _recursive_uop(out, output_st, tuple(outs), var_vals, inputs, realizes, assign_targets, reduce_info, cache=cache)                                      #     engine/schedule.py            :   175: L: {'output_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    

                                              def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],            #     engine/schedule.py            :    51: 
                                                                    realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],                                                             
                                                                    reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],                                                  
                                                                    cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:                                                                                 
                                                """recursively create a UOp"""                                                                                                                                
                                                if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                    #     engine/schedule.py            :    56: L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'outputs': (<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>,), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                if (buf, st) in cache: return cache[(buf, st)]                                                                                                           #     engine/schedule.py            :    57: 
                                                assert buf.op is not None, "base must be a base itself"                                                                                                  #     engine/schedule.py            :    58: 
                                                dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                               #     engine/schedule.py            :    59: 
                                              
                                                # buffer ops define ShapeTracker
                                                if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                                 #     engine/schedule.py            :    62: L: {'dtype': dtypes.int}    
                                              
                                                # reduce ops change ShapeTracker
                                                if buf.op in ReduceOps:                                                                                                                                  #     engine/schedule.py            :    83: 
                                              
                                                # elementwise ops pass shapetracker
                                                in_uops = tuple(_recursive_uop(x, st, outputs, var_vals, inputs, realizes, assign_targets, reduce_info, cache) for x in buf.srcs)                        #     engine/schedule.py            :    93: 

                                                  def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],        #     engine/schedule.py            :    51: 
                                                                        realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],                                                         
                                                                        reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],                                              
                                                                        cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:                                                                             
                                                    if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                # OLD engine/schedule.py            :    56: 
                                                    if (buf, st) in cache: return cache[(buf, st)]                                                                                                       # OLD engine/schedule.py            :    57: 
                                                    assert buf.op is not None, "base must be a base itself"                                                                                              # OLD engine/schedule.py            :    58: 
                                                    dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                           # OLD engine/schedule.py            :    59: 
                                                  
                                                    # buffer ops define ShapeTracker
                                                    if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                             # OLD engine/schedule.py            :    62: 
                                                      unbound_st, st_var_vals = st.simplify().unbind()                                                                                                   #     engine/schedule.py            :    63: L: {'buf': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}    

                                                        @dataclass(frozen=True)                                                                                                                          #     shape/shapetracker.py         :    36: 
                                                        class ShapeTracker:                                                                                                                                   
                                                          def simplify(self) -> ShapeTracker:                                                                                                            #     shape/shapetracker.py         :   138: 
                                                            if len(self.views) >= 2 and (new_view := self.views[-2] + self.views[-1]) is not None:                                                       #     shape/shapetracker.py         :   139: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                            return self                                                                                                                                  #     shape/shapetracker.py         :   141: 

                                                        @dataclass(frozen=True)                                                                                                                          #     shape/shapetracker.py         :    36: 
                                                        class ShapeTracker:                                                                                                                                   
                                                          def unbind(self) -> Tuple[ShapeTracker, Dict[Variable, int]]:                                                                                  #     shape/shapetracker.py         :    96: 
                                                            unbound_views, var_vals = zip(*[v.unbind() for v in self.views])                                                                             #     shape/shapetracker.py         :    97: 

                                                              @dataclass(frozen=True)                                                                                                                    #     shape/view.py                 :    85: 
                                                              class View:                                                                                                                                     
                                                                @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                 #     shape/view.py                 :   125: 
                                                                def unbind(self) -> Tuple[View, Dict[Variable, int]]:                                                                                         
                                                                  var_unboundvar_val = [(v, v.unbind()) for v in self.vars()]                                                                            #     shape/view.py                 :   126: 
                                                                  unbound_vars = {v:uv for v,(uv,_) in var_unboundvar_val}                                                                               #     shape/view.py                 :   127: L: {'var_unboundvar_val': []}    
                                                                  new_shape = tuple(map(substitute, self.shape))                                                                                         #     shape/view.py                 :   129: L: {'unbound_vars': {}, 'substitute': <function View.unbind.<locals>.substitute at 0x78971f220280>}    

                                                                    @dataclass(frozen=True)                                                                                                              #     shape/view.py                 :    85: 
                                                                    class View:                                                                                                                               
                                                                      @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                           #     shape/view.py                 :   125: 
                                                                      def unbind(self) -> Tuple[View, Dict[Variable, int]]:                                                                                   
                                                                        def substitute(x): return x if isinstance(x, int) else x.substitute(unbound_vars)                                                #     shape/view.py                 :   128: 

                                                                  new_strides = tuple(map(substitute, self.strides))                                                                                     #     shape/view.py                 :   130: L: {'new_shape': (3,)}    
                                                                  new_offset = substitute(self.offset)                                                                                                   #     shape/view.py                 :   131: L: {'new_strides': (1,)}    
                                                                  new_mask = tuple((substitute(x[0]), substitute(x[1])) for x in self.mask) if self.mask is not None else None                           #     shape/view.py                 :   132: L: {'new_offset': 0}    
                                                                  return View.create(new_shape, new_strides, new_offset, new_mask), dict(x[1] for x in var_unboundvar_val)                               #     shape/view.py                 :   133: 

                                                            return ShapeTracker(tuple(unbound_views)), merge_dicts(var_vals)                                                                             #     shape/shapetracker.py         :    98: L: {'unbound_views': (View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),), 'var_vals': ({},)}    

                                                      var_vals.update(st_var_vals)                                                                                                                       #     engine/schedule.py            :    64: L: {'unbound_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'st_var_vals': {}}    
                                                      # if it's a const, we generate it
                                                      if buf.op is MetaOps.CONST:                                                                                                                        #     engine/schedule.py            :    66: 
                                                      # otherwise, it's a load and we add it to the inputs
                                                      if buf in assign_targets and not (unbound_st.contiguous or (len(unbound_st.views) == 1 and unbound_st.views[0].mask is not None and \              #     engine/schedule.py            :    73: 
                                                          ShapeTracker.from_shape(unbound_st.shape).shrink(unbound_st.views[0].mask) == unbound_st.shrink(unbound_st.views[0].mask))):                        
                                                      ubuf = UOp(UOps.DEFINE_GLOBAL, buf.dtype if isinstance(buf.dtype, ImageDType) else PtrDType(buf.dtype), (),                                        #     engine/schedule.py            :    78: 
                                                                 outputs.index(assign_targets[buf]) if buf in assign_targets else len(outputs)+inputs.setdefault(buf, len(inputs)))                           

                                                        # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                      #     dtype.py                      :    31: 
                                                        class PtrDType(DType):                                                                                                                                
                                                          def __init__(self, dt:DType): super().__init__(dt.priority, dt.itemsize, dt.name, dt.fmt, dt.count)                                            #     dtype.py                      :    32: 

                                                      return UOp(UOps.LOAD, dtype, (ubuf, unbound_st.to_uop()))                                                                                          #     engine/schedule.py            :    80: L: {'ubuf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=())}    

                                                        @dataclass(frozen=True)                                                                                                                          #     shape/shapetracker.py         :    36: 
                                                        class ShapeTracker:                                                                                                                                   
                                                          def to_uop(self) -> UOp: return UOp(UOps.SHAPETRACKER, None, (), self)                                                                         #     shape/shapetracker.py         :    68: 

                                                  @dataclass(frozen=True)                                                                                                                                #     shape/shapetracker.py         :    36: 
                                                  class ShapeTracker:                                                                                                                                         
                                                    def __add__(self, st:ShapeTracker) -> ShapeTracker:                                                                                                  #     shape/shapetracker.py         :    39: 
                                                      ret = self                                                                                                                                         #     shape/shapetracker.py         :    40: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                      for v in st.views: ret = ShapeTracker(ret.views + (v,)).simplify() # one view at a time = better simplification                                    #     shape/shapetracker.py         :    41: L: {'ret': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}    

                                                        @dataclass(frozen=True)                                                                                                                          #     shape/view.py                 :    85: 
                                                        class View:                                                                                                                                           
                                                          @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                               #     shape/view.py                 :   136: 
                                                          def __add__(self, vm1:View) -> Optional[View]:                                                                                                      
                                                            vm2 = self                                                                                                                                   #     shape/view.py                 :   137: L: {'self': View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False), 'vm1': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}    
                                                            if vm2.contiguous: return vm1                                                                                                                #     shape/view.py                 :   138: L: {'vm2': View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False)}    
                                                            if vm1.contiguous and vm1.shape == vm2.shape: return vm2                                                                                     #     shape/view.py                 :   139: 

                                                        @dataclass(frozen=True)                                                                                                                          #     shape/shapetracker.py         :    36: 
                                                        class ShapeTracker:                                                                                                                                   
                                                          def simplify(self) -> ShapeTracker:                                                                                                            #     shape/shapetracker.py         :   138: 
                                                            if len(self.views) >= 2 and (new_view := self.views[-2] + self.views[-1]) is not None:                                                       # OLD shape/shapetracker.py         :   139: 
                                                              return ShapeTracker(self.views[:-2] + (new_view,)).simplify()                                                                              #     shape/shapetracker.py         :   140: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False), View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True))), 'new_view': View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False)}    

                                                      return ret                                                                                                                                         #     shape/shapetracker.py         :    42: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'ret': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'v': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}    

                                                  def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],        #     engine/schedule.py            :    51: 
                                                                        realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],                                                         
                                                                        reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],                                              
                                                                        cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:                                                                             
                                                    if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                # OLD engine/schedule.py            :    56: 
                                                    if (buf, st) in cache: return cache[(buf, st)]                                                                                                       # OLD engine/schedule.py            :    57: 
                                                    assert buf.op is not None, "base must be a base itself"                                                                                              # OLD engine/schedule.py            :    58: 
                                                    dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                           # OLD engine/schedule.py            :    59: 
                                                  
                                                    # buffer ops define ShapeTracker
                                                    if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                             # OLD engine/schedule.py            :    62: 
                                                      unbound_st, st_var_vals = st.simplify().unbind()                                                                                                   # OLD engine/schedule.py            :    63: 
                                                      var_vals.update(st_var_vals)                                                                                                                       # OLD engine/schedule.py            :    64: 
                                                      # if it's a const, we generate it
                                                      if buf.op is MetaOps.CONST:                                                                                                                        # OLD engine/schedule.py            :    66: 
                                                        if isinstance(val:=buf.arg, Variable):                                                                                                           #     engine/schedule.py            :    67: L: {'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'unbound_st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st_var_vals': {}}    
                                                        else: assert isinstance(val, get_args(ConstType)), f"cannot create ConstBuffer with value {val}"                                                 #     engine/schedule.py            :    70: L: {'val': 2}    
                                                        return UOp(UOps.CONST, dtype, (unbound_st.to_uop(),), val)                                                                                       #     engine/schedule.py            :    71: 

                                                if buf.op in {MetaOps.CONTIGUOUS, MetaOps.ASSIGN}:                                                                                                       #     engine/schedule.py            :    94: L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'in_uops': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)))}    
                                                if buf.op is UnaryOps.CAST: return cache.setdefault((buf, st), UOp(UOps.CAST, dtype, in_uops))                                                           #     engine/schedule.py            :    97: 
                                                if buf.op is UnaryOps.BITCAST: return cache.setdefault((buf, st), UOp(UOps.BITCAST, dtype, in_uops))                                                     #     engine/schedule.py            :    98: 
                                                return cache.setdefault((buf, st), UOp(UOps.ALU, dtype, in_uops, buf.op))                                                                                #     engine/schedule.py            :    99: 

                                            if out.op is MetaOps.ASSIGN and out.arg:                                                                                                                     #     engine/schedule.py            :   176: L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'seen_ops': {(<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))): None}, 'shape_dims': [], 'ast': [], 'i': 0, 'output_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'src': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),))}    
                                            output_st, vv = output_st.simplify().unbind()                                                                                                                #     engine/schedule.py            :   179: 
                                            if vv: var_vals.update(vv)                                                                                                                                   #     engine/schedule.py            :   180: L: {'output_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'vv': {}}    
                                            ubuf = UOp(UOps.DEFINE_GLOBAL, out.dtype if isinstance(out.dtype, ImageDType) else PtrDType(out.dtype), (), i)                                               #     engine/schedule.py            :   181: 
                                            ast.append(UOp(UOps.STORE, None, (ubuf, output_st.to_uop(), src)))                                                                                           #     engine/schedule.py            :   182: L: {'ubuf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                          sink = UOp(UOps.SINK, None, tuple(ast))                                                                                                                        #     engine/schedule.py            :   183: 
                                          return LBScheduleItem(sink, outs, list(inputs), var_vals, dedup([x[0].metadata for x in cache if x[0].metadata and x[0] not in inputs]))                       #     engine/schedule.py            :   184: L: {'sink': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    

                                            def dedup(x:Iterable[T]): return list(dict.fromkeys(x))   # retains list order                                                                               #     helpers.py                    :    19: 

                                              @dataclass(frozen=True)                                                                                                                                    #     helpers.py                    :   116: 
                                              class Metadata:                                                                                                                                                 
                                                def __hash__(self): return hash(self.name)                                                                                                               #     helpers.py                    :   120: 

                                        def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None]) -> LBScheduleItem:                                                                 #     engine/schedule.py            :   146: 
                                          if (out:=outs[0]).op is MetaOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:                           # OLD engine/schedule.py            :   148: 
                                          if out.op in {MetaOps.CUSTOM, MetaOps.COPY, MetaOps.EMPTY, MetaOps.VIEW}:                                                                                      # OLD engine/schedule.py            :   153: 
                                            return LBScheduleItem(UOp(UOps.EXT, out.dtype, (), (out.op, out.arg)), outs, [x.base for x in out.srcs])                                                     #     engine/schedule.py            :   154: L: {'outs': [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], 'out': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}    

                                      schedule_targets = {out:lsi for lsi in prescheduled for out in lsi.outputs}                                                                                        #     engine/schedule.py            :   357: L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'allbufs': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'simple_pads': {}, 'children': defaultdict(<class 'dict'>, {<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}}), 'assign_targets': {}, 'double_reduces': {}, 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'reduce_for_op': {}, 'reduce_of_const': [], 'r': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'output_groups': defaultdict(<class 'list'>, {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>]}), 'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'prescheduled': [LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])]}    
                                    
                                      graph: DefaultDict[LBScheduleItem, List[LBScheduleItem]] = defaultdict(list)                                                                                       #     engine/schedule.py            :   359: L: {'schedule_targets': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])}}    
                                      in_degree: DefaultDict[LBScheduleItem, int] = defaultdict(int)                                                                                                     #     engine/schedule.py            :   360: L: {'graph': defaultdict(<class 'list'>, {})}    
                                      for lsi in prescheduled:                                                                                                                                           #     engine/schedule.py            :   361: L: {'in_degree': defaultdict(<class 'int'>, {})}    
                                        if lsi not in in_degree: in_degree[lsi] = 0                                                                                                                      #     engine/schedule.py            :   362: L: {'lsi': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__])}    

                                          @dataclass(frozen=True)                                                                                                                                        #     engine/schedule.py            :    39: 
                                          class LBScheduleItem:                                                                                                                                               
                                            def __hash__(self):                                                                                                                                          #     engine/schedule.py            :    45: 
                                              """The unique identifier of a schedule item in the toposort."""                                                                                                 
                                              return hash(self.outputs[0])                                                                                                                               #     engine/schedule.py            :    47: L: {'self': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__])}    

                                        # realize outputs after all parents are realized
                                        scheduled_parents = dedup(schedule_targets[x] for x in lsi.inputs if x in schedule_targets)                                                                      #     engine/schedule.py            :   364: L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'realizes': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'allbufs': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'simple_pads': {}, 'children': defaultdict(<class 'dict'>, {<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}}), 'assign_targets': {}, 'double_reduces': {}, 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'reduce_for_op': {}, 'reduce_of_const': [], 'r': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'output_groups': defaultdict(<class 'list'>, {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>]}), 'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'prescheduled': [LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])], 'schedule_targets': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])}, 'graph': defaultdict(<class 'list'>, {}), 'in_degree': defaultdict(<class 'int'>, {LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]): 0}), 'lsi': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__])}    
                                        for x in scheduled_parents:                                                                                                                                      #     engine/schedule.py            :   365: L: {'scheduled_parents': [LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])]}    
                                          graph[x].append(lsi)                                                                                                                                           #     engine/schedule.py            :   366: L: {'x': LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])}    
                                          in_degree[lsi] += 1                                                                                                                                            #     engine/schedule.py            :   367: 
                                        # realize outputs before a parent is assigned to
                                        parents_assigns = dedup(schedule_targets[assign_targets[x]] for x in lsi.inputs if x in assign_targets)                                                          #     engine/schedule.py            :   369: 
                                        for assign in parents_assigns:                                                                                                                                   #     engine/schedule.py            :   370: L: {'parents_assigns': []}    
                                    
                                      if SAVE_SCHEDULE:                                                                                                                                                  #     engine/schedule.py            :   374: L: {'lsi': LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[]), 'scheduled_parents': [], 'parents_assigns': []}    
                                      return graph, in_degree                                                                                                                                            #     engine/schedule.py            :   380: 

                                  if getenv("RUN_PROCESS_REPLAY") and getenv("COMPARE_SCHEDULE", 1):                                                                                                     #     engine/schedule.py            :   387: 
                                
                                  queue = deque(lsi for lsi,deg in in_degree.items() if deg == 0)                                                                                                        #     engine/schedule.py            :   391: 
                                  schedule: List[ScheduleItem] = []                                                                                                                                      #     engine/schedule.py            :   392: L: {'queue': deque([LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])])}    
                                  var_vals: Dict[Variable, int] = {}                                                                                                                                     #     engine/schedule.py            :   393: L: {'schedule': []}    
                                  kernel_number = GlobalCounters.kernel_count                                                                                                                            #     engine/schedule.py            :   394: L: {'var_vals': {}}    
                                  while queue:                                                                                                                                                           #     engine/schedule.py            :   395: L: {'kernel_number': 0}    
                                    lsi = queue.popleft()                                                                                                                                                #     engine/schedule.py            :   396: 
                                    for buf in lsi.outputs: seen.add(buf)                                                                                                                                #     engine/schedule.py            :   397: L: {'lsi': LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])}    
                                    if GRAPH:                                                                                                                                                            #     engine/schedule.py            :   398: L: {'buf': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}    
                                    var_vals = merge_dicts([var_vals, lsi.var_vals])                                                                                                                     #     engine/schedule.py            :   401: 
                                    for out in lsi.outputs: del out.srcs  # can only schedule once                                                                                                       #     engine/schedule.py            :   402: L: {'var_vals': {}}    
                                    schedule.append(si:=ScheduleItem(lsi.ast, tuple(x.buffer for x in lsi.outputs+lsi.inputs if x.size != 0), lsi.metadata))                                             #     engine/schedule.py            :   403: L: {'out': <LB CLANG (3,) int (<MetaOps.COPY: 3>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>}    
                                    if logops and si.ast.op is UOps.SINK and not any(i.device.startswith("DISK:") for i in si.inputs):                                                                   #     engine/schedule.py            :   404: L: {'si': ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[])}    
                                    for x in graph[lsi]:                                                                                                                                                 #     engine/schedule.py            :   406: 
                                      in_degree[x] -= 1                                                                                                                                                  #     engine/schedule.py            :   407: L: {'x': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[__add__])}    
                                      if in_degree[x] == 0: queue.append(x)                                                                                                                              #     engine/schedule.py            :   408: 

                                        class LazyBuffer:                                                                                                                                                #     lazy.py                       :    26: 
                                          def __del__(self):                                                                                                                                             #     lazy.py                       :    50: 
                                            if hasattr(self, 'buffer'): self.buffer.ref(-1)                                                                                                              #     lazy.py                       :    51: L: {'self': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}    

                                
                                  # confirm everything was scheduled correctly
                                  if any(degree != 0 for degree in in_degree.values()) or len(in_degree) != len(schedule):                                                                               #     engine/schedule.py            :   411: L: {'var_vals': {}, 'lsi': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[__add__]), 'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>, 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)>, 'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])}    
                                  if DEBUG >= 1 and len(schedule) >= 10: print(f"scheduled {len(schedule)} kernels")                                                                                     #     engine/schedule.py            :   413: 
                                  return schedule, var_vals                                                                                                                                              #     engine/schedule.py            :   414: 

                              return memory_planner(schedule), var_vals                                                                                                                                  #     tensor.py                     :   198: L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])], 'var_vals': {}}    

                                def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:                                                                                                   #     engine/realize.py             :   264: 
                                  # Exclude buffers involved in load ops (e.g transfers) to preserve parallelism in graphs.
                                  assigned = _internal_memory_planner([si.bufs for si in schedule],                                                                                                      #     engine/realize.py             :   266: G: {'capturing': [], 'run_schedule': <function run_schedule at 0x78971f1c81f0>, '_internal_memory_planner': <function _internal_memory_planner at 0x78971f1c8280>, 'memory_planner': <function memory_planner at 0x78971f1c8310>}    L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])]}    
                                                                      noopt_buffers={b for si in schedule if si.ast.op is not UOps.SINK for b in si.bufs})                                                    

                                    def _internal_memory_planner(buffers:List[Union[List[Buffer], Tuple[Buffer, ...]]], noopt_buffers=None, debug_prefix="") -> Dict[Buffer, Buffer]:                    #     engine/realize.py             :   227: 
                                      if getenv("NO_MEMORY_PLANNER"): return {}                                                                                                                          #     engine/realize.py             :   228: L: {'noopt_buffers': {<buf real:True device:NPY size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>}, 'debug_prefix': '', 'buffers': [(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)]}    
                                      first_appearance, last_appearance = {}, {}                                                                                                                         #     engine/realize.py             :   229: 
                                      for i,u in enumerate(buffers):                                                                                                                                     #     engine/realize.py             :   230: L: {'first_appearance': {}, 'last_appearance': {}}    
                                        for buf in u:                                                                                                                                                    #     engine/realize.py             :   231: L: {'i': 0, 'u': (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)}    
                                          if buf.is_allocated() or buf.lb_refcount > 0 or (noopt_buffers is not None and buf.base in noopt_buffers): continue                                            #     engine/realize.py             :   232: L: {'buf': <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>}    

                                            class Buffer:                                                                                                                                                #     device.py                     :    52: 
                                              def is_allocated(self) -> bool: return hasattr(self, '_buf')                                                                                               #     device.py                     :    76: 

                                            class Buffer:                                                                                                                                                #     device.py                     :    52: 
                                              @property                                                                                                                                                  #     device.py                     :    74: 
                                              def lb_refcount(self): return self.base._lb_refcount                                                                                                            

                                    
                                      # Sort buffers by size in descending order, prioritizing largest buffers for allocation first.
                                      # Track free segments, each containing (start, stop, and buffer that could be reused on this segment).
                                      free_segs: Dict[Tuple, List[Tuple[int, int, Buffer]]] = defaultdict(list) # Dict[buffer key, Tuple[start, end, buffer to reuse on the seg]]                        #     engine/realize.py             :   238: L: {'i': 1, 'u': (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)}    
                                    
                                      buffer_requests = sorted([(first_appearance[buf], last_appearance[buf], buf) for buf in first_appearance.keys()], key=lambda x: -x[2].nbytes)                      #     engine/realize.py             :   250: L: {'free_segs': defaultdict(<class 'list'>, {}), 'find_replace_buffer': <function _internal_memory_planner.<locals>.find_replace_buffer at 0x789735ee5900>}    
                                      assigned = {buf:find_replace_buffer(buf, st, en) for st, en, buf in buffer_requests}                                                                               #     engine/realize.py             :   251: L: {'buffer_requests': []}    
                                    
                                      for i,u in enumerate(buffers):                                                                                                                                     #     engine/realize.py             :   253: L: {'assigned': {}}    
                                        for buf in u:                                                                                                                                                    #     engine/realize.py             :   254: L: {'i': 0, 'u': (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)}    
                                          if buf.is_allocated() or buf.lb_refcount > 0 or (noopt_buffers is not None and buf.base in noopt_buffers): continue                                            #     engine/realize.py             :   255: 
                                    
                                      if DEBUG >= 1 and len(ak:=dedup(x for x in assigned.keys() if x._base is None)) != len(av:=dedup(x for x in assigned.values() if x._base is None)):                #     engine/realize.py             :   259: L: {'i': 1, 'u': (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)}    
                                      return assigned                                                                                                                                                    #     engine/realize.py             :   262: 

                                  return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]                                                              #     engine/realize.py             :   268: L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])]}    

                          def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):                                                              #     engine/realize.py             :   220: 
                            for ei in lower_schedule(schedule):                                                                                                                                          #     engine/realize.py             :   221: L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])], 'var_vals': {}, 'do_update_stats': True}    

                              def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:                                                                                        #     engine/realize.py             :   205: 
                                while len(schedule):                                                                                                                                                     #     engine/realize.py             :   206: 
                                  si = schedule.pop(0)                                                                                                                                                   #     engine/realize.py             :   207: 
                                  try: yield lower_schedule_item(si)                                                                                                                                     #     engine/realize.py             :   208: L: {'si': ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[])}    

                                    def lower_schedule_item(si:ScheduleItem) -> ExecItem:                                                                                                                #     engine/realize.py             :   189: 
                                      assert len(set(x.device for x in si.bufs)) == 1 or (si.ast.op is UOps.EXT and si.ast.arg[0] is MetaOps.COPY)                                                       #     engine/realize.py             :   190: 
                                      if si.ast.op is UOps.SINK:                                                                                                                                         #     engine/realize.py             :   191: 
                                      out, (op, arg) = si.outputs[0], si.ast.arg                                                                                                                         #     engine/realize.py             :   194: 

                                        @dataclass(frozen=True)                                                                                                                                          #     engine/schedule.py            :    25: 
                                        class ScheduleItem:                                                                                                                                                   
                                          @property                                                                                                                                                      #     engine/schedule.py            :    30: 
                                          def outputs(self) -> Tuple[Buffer, ...]:                                                                                                                            
                                            """Read/write or write only buffers in the schedule."""                                                                                                           
                                            return self.bufs[:len(self.ast.src)] if self.ast.op is UOps.SINK else self.bufs[0:1]                                                                         #     engine/schedule.py            :    32: L: {'self': ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[])}    

                                      if op is MetaOps.COPY:                                                                                                                                             #     engine/realize.py             :   195: L: {'out': <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, 'op': <MetaOps.COPY: 3>, 'arg': 12}    
                                        kernel_type = BufferCopy                                                                                                                                         #     engine/realize.py             :   196: 
                                        if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:                                          #     engine/realize.py             :   197: L: {'kernel_type': <class 'tinygrad.engine.realize.BufferCopy'>}    
                                          import ctypes, subprocess, pathlib, tempfile                                                                                                                   #     runtime/ops_clang.py          :     1: G: {'__name__': 'tinygrad.runtime.ops_clang', '__doc__': None, '__package__': 'tinygrad.runtime', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f2b3970>, '__spec__': ModuleSpec(name='tinygrad.runtime.ops_clang', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f2b3970>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_clang.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_clang.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/__pycache__/ops_clang.cpython-310.pyc'}    
                                          from tinygrad.device import Compiled, Compiler, MallocAllocator                                                                                                #     runtime/ops_clang.py          :     2: G: {'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>, 'subprocess': <module 'subprocess' from '/usr/lib/python3.10/subprocess.py'>, 'pathlib': <module 'pathlib' from '/usr/lib/python3.10/pathlib.py'>, 'tempfile': <module 'tempfile' from '/usr/lib/python3.10/tempfile.py'>}    
                                          from tinygrad.helpers import cpu_time_execution, DEBUG, cpu_objdump                                                                                            #     runtime/ops_clang.py          :     3: G: {'Compiled': <class 'tinygrad.device.Compiled'>, 'Compiler': <class 'tinygrad.device.Compiler'>, 'MallocAllocator': <tinygrad.device._MallocAllocator object at 0x78972db5ff10>}    
                                          from tinygrad.renderer.cstyle import ClangRenderer                                                                                                             #     runtime/ops_clang.py          :     4: G: {'cpu_time_execution': <function cpu_time_execution at 0x789730f5a560>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>, 'cpu_objdump': <function cpu_objdump at 0x789730f5a5f0>}    
                                            from typing import Dict, List, Optional, Tuple, Union, DefaultDict, cast, Literal, Callable                                                                  #     renderer/cstyle.py            :     1: G: {'__name__': 'tinygrad.renderer.cstyle', '__doc__': None, '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, '__spec__': ModuleSpec(name='tinygrad.renderer.cstyle', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/cstyle.cpython-310.pyc'}    
                                            import os, math                                                                                                                                              #     renderer/cstyle.py            :     2: G: {'Dict': typing.Dict, 'List': typing.List, 'Optional': typing.Optional, 'Tuple': typing.Tuple, 'Union': typing.Union, 'DefaultDict': typing.DefaultDict, 'cast': <function cast at 0x789735eb17e0>, 'Literal': typing.Literal, 'Callable': typing.Callable}    
                                            from collections import defaultdict, Counter                                                                                                                 #     renderer/cstyle.py            :     3: G: {'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'math': <module 'math' (built-in)>}    
                                            from tinygrad.ops import UnaryOps, BinaryOps, TernaryOps, UOps, UOp                                                                                          #     renderer/cstyle.py            :     4: G: {'defaultdict': <class 'collections.defaultdict'>, 'Counter': <class 'collections.Counter'>}    
                                            from tinygrad.helpers import strip_parens, getenv, prod, dedup                                                                                               #     renderer/cstyle.py            :     5: G: {'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>}    
                                            from tinygrad.dtype import ImageDType, dtypes, DType, PtrDType, ConstType                                                                                    #     renderer/cstyle.py            :     6: G: {'strip_parens': <function strip_parens at 0x789730f58a60>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'prod': <function prod at 0x7897327a7520>, 'dedup': <function dedup at 0x789730f581f0>}    
                                            from tinygrad.renderer import Renderer, TensorCore                                                                                                           #     renderer/cstyle.py            :     7: G: {'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'ConstType': typing.Union[float, int, bool]}    

                                              class CStyleLanguage(Renderer):                                                                                                                            #     renderer/cstyle.py            :     9: 
                                                kernel_prefix: str = ""                                                                                                                                  #     renderer/cstyle.py            :    10: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>, 'TensorCore': <class 'tinygrad.renderer.TensorCore'>}    L: {'__module__': 'tinygrad.renderer.cstyle', '__qualname__': 'CStyleLanguage', '__annotations__': {}}    
                                                buffer_prefix: str = ""                                                                                                                                  #     renderer/cstyle.py            :    11: L: {'kernel_prefix': ''}    
                                                buffer_suffix: str = ""                                                                                                                                  #     renderer/cstyle.py            :    12: L: {'buffer_prefix': ''}    
                                                smem_align: str = ""                                                                                                                                     #     renderer/cstyle.py            :    13: L: {'buffer_suffix': ''}    
                                                smem_prefix: str = ""                                                                                                                                    #     renderer/cstyle.py            :    14: L: {'smem_align': ''}    
                                                smem_prefix_for_cast: bool = True                                                                                                                        #     renderer/cstyle.py            :    15: L: {'smem_prefix': ''}    
                                                arg_int_prefix: str = "const int"                                                                                                                        #     renderer/cstyle.py            :    16: L: {'smem_prefix_for_cast': True}    
                                                barrier: str = ""                                                                                                                                        #     renderer/cstyle.py            :    17: L: {'arg_int_prefix': 'const int'}    
                                                code_for_workitem: Dict[Union[Literal["g"], Literal["l"], Literal["i"]], Callable] = {}                                                                  #     renderer/cstyle.py            :    18: L: {'barrier': ''}    
                                                extra_args: List[str] = []                                                                                                                               #     renderer/cstyle.py            :    19: L: {'code_for_workitem': {}}    
                                                float4: Optional[str] = None                                                                                                                             #     renderer/cstyle.py            :    20: L: {'extra_args': []}    
                                                uses_vload: bool = False                                                                                                                                 #     renderer/cstyle.py            :    21: 
                                                uses_ptr_arithmetic: bool = False                                                                                                                        #     renderer/cstyle.py            :    22: L: {'uses_vload': False}    
                                                type_map: Dict[DType, str] = {}                                                                                                                          #     renderer/cstyle.py            :    23: L: {'uses_ptr_arithmetic': False}    
                                                infinity: str = "INFINITY"                                                                                                                               #     renderer/cstyle.py            :    24: L: {'type_map': {}}    
                                                nan: str = "NAN"                                                                                                                                         #     renderer/cstyle.py            :    25: L: {'infinity': 'INFINITY'}    
                                                code_for_op: Dict = {                                                                                                                                    #     renderer/cstyle.py            :    26: L: {'nan': 'NAN'}    
                                                  UnaryOps.SQRT: lambda x,dtype: f"sqrt({x})",                                                                                                                
                                                  UnaryOps.RECIP: lambda x,dtype: f"(1/{x})",                                                                                                                 
                                                  UnaryOps.EXP2: lambda x,dtype: f"exp2({x})", UnaryOps.LOG2: lambda x,dtype: f"log2({x})", UnaryOps.SIN: lambda x,dtype: f"sin({x})",                        
                                                  BinaryOps.ADD: lambda a,b,dtype: f"({a}+{b})", BinaryOps.MAX: lambda a,b,dtype: f"max({a},{b})",                                                            
                                                  BinaryOps.IDIV: lambda a,b,dtype: f"({a}/{b})", BinaryOps.MUL: lambda a,b,dtype: f"({a}*{b})", BinaryOps.MOD: lambda a,b,dtype: f"({a}%{b})",               
                                                  BinaryOps.CMPLT: lambda a,b,dtype: f"({a}<{b})", BinaryOps.CMPNE: lambda a,b,dtype: f"({a}!={b})", BinaryOps.XOR: lambda a,b,dtype: f"({a}^{b})",           
                                                  BinaryOps.AND: lambda a,b,dtype: f"({a}&{b})", BinaryOps.OR: lambda a,b,dtype: f"({a}|{b})",                                                                
                                                  TernaryOps.WHERE: lambda a,b,c,dtype: f"({a}?{b}:{c})"}                                                                                                     

                                              class ClangRenderer(CStyleLanguage):                                                                                                                       #     renderer/cstyle.py            :   194: 
                                                device = "CLANG"                                                                                                                                         #     renderer/cstyle.py            :   195: G: {'CStyleLanguage': <class 'tinygrad.renderer.cstyle.CStyleLanguage'>, '_make_clang_dtype': <function _make_clang_dtype at 0x78971f0d27a0>}    L: {'__qualname__': 'ClangRenderer'}    
                                                float4 = "(float4)"                                                                                                                                      #     renderer/cstyle.py            :   196: L: {'device': 'CLANG'}    
                                                has_local = False                                                                                                                                        #     renderer/cstyle.py            :   197: L: {'float4': '(float4)'}    
                                                global_max = None                                                                                                                                        #     renderer/cstyle.py            :   198: L: {'has_local': False}    
                                                infinity = "__builtin_inff()"                                                                                                                            #     renderer/cstyle.py            :   199: 
                                                nan = '__builtin_nanf("")'                                                                                                                               #     renderer/cstyle.py            :   200: L: {'infinity': '__builtin_inff()'}    
                                              
                                                # language options
                                                buffer_suffix = " restrict"                                                                                                                              #     renderer/cstyle.py            :   203: L: {'nan': '__builtin_nanf("")'}    
                                                type_map = {dtypes.bool:"_Bool", dtypes.half:"__fp16"}                                                                                                   #     renderer/cstyle.py            :   204: L: {'buffer_suffix': ' restrict'}    
                                                code_for_op = {**({k:v for k,v in CStyleLanguage().code_for_op.items() if k not in [UnaryOps.EXP2, UnaryOps.SIN, UnaryOps.LOG2]}),                       #     renderer/cstyle.py            :   205: L: {'type_map': {dtypes.bool: '_Bool', dtypes.half: '__fp16'}}    
                                                               UnaryOps.SQRT: lambda x,dtype: f"__builtin_sqrtl({x})" if dtype == dtypes.float64 else f"__builtin_sqrtf({x})",                                
                                                               BinaryOps.MAX: lambda a,b,dtype: f"(({a}>{b})?{a}:{b})"}                                                                                       

                                              class OpenCLRenderer(CStyleLanguage):                                                                                                                      #     renderer/cstyle.py            :   213: 
                                                device = "GPU"                                                                                                                                           #     renderer/cstyle.py            :   214: G: {'ClangRenderer': <class 'tinygrad.renderer.cstyle.ClangRenderer'>}    L: {'__qualname__': 'OpenCLRenderer'}    
                                              
                                                # language options
                                                kernel_prefix = "__kernel "                                                                                                                              #     renderer/cstyle.py            :   217: L: {'device': 'GPU'}    
                                                buffer_prefix = "__global "                                                                                                                              #     renderer/cstyle.py            :   218: L: {'kernel_prefix': '__kernel '}    
                                                smem_align = "__attribute__ ((aligned (16))) "                                                                                                           #     renderer/cstyle.py            :   219: L: {'buffer_prefix': '__global '}    
                                                smem_prefix = "__local "                                                                                                                                 #     renderer/cstyle.py            :   220: L: {'smem_align': '__attribute__ ((aligned (16))) '}    
                                                barrier = "barrier(CLK_LOCAL_MEM_FENCE);"                                                                                                                #     renderer/cstyle.py            :   221: L: {'smem_prefix': '__local '}    
                                                float4 = "(float4)"                                                                                                                                      #     renderer/cstyle.py            :   222: L: {'barrier': 'barrier(CLK_LOCAL_MEM_FENCE);'}    
                                                code_for_workitem = {"g": lambda x: f"get_group_id({x})", "l": lambda x: f"get_local_id({x})", "i": lambda x: f"get_global_id({x})"}                     #     renderer/cstyle.py            :   223: L: {'float4': '(float4)'}    
                                                uses_vload = True                                                                                                                                        #     renderer/cstyle.py            :   224: L: {'code_for_workitem': {'g': <function OpenCLRenderer.<lambda> at 0x78971f0d05e0>, 'l': <function OpenCLRenderer.<lambda> at 0x78971f0d0670>, 'i': <function OpenCLRenderer.<lambda> at 0x78971f0d0700>}}    
                                                type_map = { dtypes.uint8: "uchar", dtypes.uint32: "uint", dtypes.uint16: "ushort", dtypes.uint64: "ulong", dtypes.bfloat16: "ushort" }                  #     renderer/cstyle.py            :   225: L: {'uses_vload': True}    

                                              class IntelRenderer(OpenCLRenderer):                                                                                                                       #     renderer/cstyle.py            :   233: 
                                                device, suffix, kernel_prefix = "GPU", "INTEL", "__attribute__((intel_reqd_sub_group_size(8)))\n" + "__kernel "                                          #     renderer/cstyle.py            :   234: G: {'OpenCLRenderer': <class 'tinygrad.renderer.cstyle.OpenCLRenderer'>}    L: {'__qualname__': 'IntelRenderer'}    
                                                tensor_cores = [TensorCore(dims=(8,8,16), threads=[(0,8)], dtype_in=di, dtype_out=do) for di, do in [(dtypes.half, dtypes.float), (dtypes.bfloat16, dtypes.float)]]  # noqa: E501 #     renderer/cstyle.py            :   235: L: {'device': 'GPU', 'suffix': 'INTEL', 'kernel_prefix': '__attribute__((intel_reqd_sub_group_size(8)))\n__kernel '}    

                                              class MetalRenderer(CStyleLanguage):                                                                                                                       #     renderer/cstyle.py            :   251: 
                                                device = "METAL"                                                                                                                                         #     renderer/cstyle.py            :   252: G: {'IntelRenderer': <class 'tinygrad.renderer.cstyle.IntelRenderer'>}    L: {'__qualname__': 'MetalRenderer'}    
                                                shared_max = 32768                                                                                                                                       #     renderer/cstyle.py            :   253: L: {'device': 'METAL'}    
                                                tensor_cores = [TensorCore(dims=(8,8,8), threads=[(0,2),(1,4),(0,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in [(dtypes.float, dtypes.float), (dtypes.half, dtypes.float), (dtypes.half, dtypes.half)]] # noqa: E501 #     renderer/cstyle.py            :   254: L: {'shared_max': 32768}    
                                              
                                                # language options
                                                kernel_prefix = "kernel "                                                                                                                                #     renderer/cstyle.py            :   258: L: {'tensor_cores': [TensorCore(dims=(8, 8, 8), dtype_in=dtypes.float, dtype_out=dtypes.float, threads=[(0, 2), (1, 4), (0, 2), (1, 2)]), TensorCore(dims=(8, 8, 8), dtype_in=dtypes.half, dtype_out=dtypes.float, threads=[(0, 2), (1, 4), (0, 2), (1, 2)]), TensorCore(dims=(8, 8, 8), dtype_in=dtypes.half, dtype_out=dtypes.half, threads=[(0, 2), (1, 4), (0, 2), (1, 2)])], '__init__': <function MetalRenderer.__init__ at 0x78971f0d0af0>}    
                                                buffer_prefix = "device "                                                                                                                                #     renderer/cstyle.py            :   259: L: {'kernel_prefix': 'kernel '}    
                                                smem_prefix = "threadgroup "                                                                                                                             #     renderer/cstyle.py            :   260: L: {'buffer_prefix': 'device '}    
                                                arg_int_prefix = "constant int&"                                                                                                                         #     renderer/cstyle.py            :   261: L: {'smem_prefix': 'threadgroup '}    
                                                barrier = "threadgroup_barrier(mem_flags::mem_threadgroup);"                                                                                             #     renderer/cstyle.py            :   262: L: {'arg_int_prefix': 'constant int&'}    
                                                float4 = "float4"                                                                                                                                        #     renderer/cstyle.py            :   263: L: {'barrier': 'threadgroup_barrier(mem_flags::mem_threadgroup);'}    
                                                uses_ptr_arithmetic = True                                                                                                                               #     renderer/cstyle.py            :   264: L: {'float4': 'float4'}    
                                                code_for_workitem = {"g": lambda x: f"gid.{chr(120+int(x))}", "l": lambda x: f"lid.{chr(120+int(x))}"}                                                   #     renderer/cstyle.py            :   265: L: {'uses_ptr_arithmetic': True}    
                                                # uint3 used for gid/lid - TODO: this should probably be `ushort3 lid [[thread_position_in_threadgroup]]`
                                                extra_args = ['uint3 gid [[threadgroup_position_in_grid]]', 'uint3 lid [[thread_position_in_threadgroup]]']                                              #     renderer/cstyle.py            :   267: L: {'code_for_workitem': {'g': <function MetalRenderer.<lambda> at 0x78971f0d0b80>, 'l': <function MetalRenderer.<lambda> at 0x78971f0d0c10>}}    
                                                type_map = {dtypes.bfloat16: "bfloat"}                                                                                                                   #     renderer/cstyle.py            :   268: L: {'extra_args': ['uint3 gid [[threadgroup_position_in_grid]]', 'uint3 lid [[thread_position_in_threadgroup]]']}    
                                                code_for_op = {**CStyleLanguage().code_for_op,                                                                                                           #     renderer/cstyle.py            :   269: L: {'type_map': {dtypes.bfloat16: 'bfloat'}}    
                                                  BinaryOps.MAX: lambda a,b,dtype: f"(bfloat)max((float){a},(float){b})" if dtype == dtypes.bfloat16 else f"max({a},{b})",                                    
                                                  UnaryOps.SQRT: lambda x,dtype: f"(bfloat)sqrt({x})" if dtype == dtypes.bfloat16 else f"sqrt({x})",                                                          
                                                  UnaryOps.EXP2: lambda x,dtype: f"(bfloat)exp2({x})" if dtype == dtypes.bfloat16 else f"exp2({x})",                                                          
                                                  UnaryOps.LOG2: lambda x,dtype: f"(bfloat)log2({x})" if dtype == dtypes.bfloat16 else f"log2({x})",                                                          
                                                  UnaryOps.SIN: lambda x,dtype: f"(bfloat)precise::sin({x})" if dtype == dtypes.bfloat16 else f"precise::sin({x})",}                                          

                                            
                                            code_for_op_half = {UnaryOps.RECIP: lambda x,dtype: f"hrcp({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"1/{x}",                                   #     renderer/cstyle.py            :   287: G: {'MetalRenderer': <class 'tinygrad.renderer.cstyle.MetalRenderer'>}    L: {'__name__': 'tinygrad.renderer.cstyle', '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, '__spec__': ModuleSpec(name='tinygrad.renderer.cstyle', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/cstyle.cpython-310.pyc', 'Dict': typing.Dict, 'List': typing.List, 'Optional': typing.Optional, 'Tuple': typing.Tuple, 'Union': typing.Union, 'DefaultDict': typing.DefaultDict, 'cast': <function cast at 0x789735eb17e0>, 'Literal': typing.Literal, 'Callable': typing.Callable, 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'math': <module 'math' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Counter': <class 'collections.Counter'>, 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'prod': <function prod at 0x7897327a7520>, 'dedup': <function dedup at 0x789730f581f0>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'ConstType': typing.Union[float, int, bool], 'Renderer': <class 'tinygrad.renderer.Renderer'>, 'TensorCore': <class 'tinygrad.renderer.TensorCore'>, 'CStyleLanguage': <class 'tinygrad.renderer.cstyle.CStyleLanguage'>, '_make_clang_dtype': <function _make_clang_dtype at 0x78971f0d27a0>, 'ClangRenderer': <class 'tinygrad.renderer.cstyle.ClangRenderer'>, 'OpenCLRenderer': <class 'tinygrad.renderer.cstyle.OpenCLRenderer'>, 'IntelRenderer': <class 'tinygrad.renderer.cstyle.IntelRenderer'>}    
                                                                BinaryOps.MAX: lambda a,b,dtype: f"__hmax({a},{b})" if dtype in (dtypes.half, dtypes.bfloat16) else f"max({a},{b})",                          
                                                                UnaryOps.SQRT: lambda x,dtype: f"hsqrt({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"sqrt({x})",                                    
                                                                UnaryOps.SIN: lambda x,dtype: f"hsin({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"sin({x})",                                       
                                                                UnaryOps.LOG2: lambda x,dtype: f"hlog2({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"log2({x})",                                    
                                                                UnaryOps.EXP2: lambda x,dtype: f"hexp2({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"exp2({x})",}                                   
                                            
                                            _nms = "xyzwabcdefghijkl"                                                                                                                                    #     renderer/cstyle.py            :   294: G: {'code_for_op_half': {<UnaryOps.RECIP: 7>: <function <lambda> at 0x78971f0d03a0>, <BinaryOps.MAX: 4>: <function <lambda> at 0x78971f0d1090>, <UnaryOps.SQRT: 6>: <function <lambda> at 0x78971f0d1240>, <UnaryOps.SIN: 5>: <function <lambda> at 0x78971f0d12d0>, <UnaryOps.LOG2: 2>: <function <lambda> at 0x78971f0d1360>, <UnaryOps.EXP2: 1>: <function <lambda> at 0x78971f0d13f0>}}    

                                              class CUDARenderer(CStyleLanguage):                                                                                                                        #     renderer/cstyle.py            :   300: 
                                                device = "CUDA"                                                                                                                                          #     renderer/cstyle.py            :   301: G: {'_nms': 'xyzwabcdefghijkl', '_make_cuda_dtype': <function _make_cuda_dtype at 0x78971f0d1480>}    L: {'__module__': 'tinygrad.renderer.cstyle', '__qualname__': 'CUDARenderer'}    
                                                global_max = (2147483647, 65535, 65535)                                                                                                                  #     renderer/cstyle.py            :   302: L: {'device': 'CUDA'}    
                                                local_max = (1024, 1024, 64)                                                                                                                             #     renderer/cstyle.py            :   303: L: {'global_max': (2147483647, 65535, 65535)}    
                                                shared_max = 49152                                                                                                                                       #     renderer/cstyle.py            :   304: L: {'local_max': (1024, 1024, 64)}    
                                                tensor_cores = [TensorCore(dims=(8,16,16), threads=[(0,2),(0,2),(1,2),(1,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in ([(dtypes.half, dtypes.float), (dtypes.bfloat16, dtypes.float)])]  # noqa: E501 #     renderer/cstyle.py            :   305: L: {'shared_max': 49152}    
                                              
                                                # language options
                                                kernel_prefix = "extern \"C\" __global__ "                                                                                                               #     renderer/cstyle.py            :   309: L: {'tensor_cores': [TensorCore(dims=(8, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.float, threads=[(0, 2), (0, 2), (1, 2), (1, 2), (1, 2)]), TensorCore(dims=(8, 16, 16), dtype_in=dtypes.bfloat16, dtype_out=dtypes.float, threads=[(0, 2), (0, 2), (1, 2), (1, 2), (1, 2)])], '__init__': <function CUDARenderer.__init__ at 0x78971f0d15a0>}    
                                                smem_prefix = "__shared__ "                                                                                                                              #     renderer/cstyle.py            :   310: L: {'kernel_prefix': 'extern "C" __global__ '}    
                                                smem_prefix_for_cast = False                                                                                                                             #     renderer/cstyle.py            :   311: L: {'smem_prefix': '__shared__ '}    
                                                barrier = "__syncthreads();"                                                                                                                             #     renderer/cstyle.py            :   312: L: {'smem_prefix_for_cast': False}    
                                                float4 = "make_float4"                                                                                                                                   #     renderer/cstyle.py            :   313: L: {'barrier': '__syncthreads();'}    
                                                code_for_workitem = {"g": lambda x: f"blockIdx.{chr(120+int(x))}", "l": lambda x: f"threadIdx.{chr(120+int(x))}",                                        #     renderer/cstyle.py            :   314: L: {'float4': 'make_float4'}    
                                                                     "i": lambda x: f"(blockIdx.{chr(120+int(x))}*blockDim.{chr(120+x)}+threadIdx.{chr(120+int(x))})"}                                        
                                                code_for_op = {**CStyleLanguage().code_for_op, **code_for_op_half}                                                                                       #     renderer/cstyle.py            :   316: L: {'code_for_workitem': {'g': <function CUDARenderer.<lambda> at 0x78971f0d1630>, 'l': <function CUDARenderer.<lambda> at 0x78971f0d1750>, 'i': <function CUDARenderer.<lambda> at 0x78971f0d17e0>}}    
                                                type_map = {dtypes.bfloat16: "nv_bfloat16"}                                                                                                              #     renderer/cstyle.py            :   317: L: {'code_for_op': {<UnaryOps.SQRT: 6>: <function <lambda> at 0x78971f0d1240>, <UnaryOps.RECIP: 7>: <function <lambda> at 0x78971f0d03a0>, <UnaryOps.EXP2: 1>: <function <lambda> at 0x78971f0d13f0>, <UnaryOps.LOG2: 2>: <function <lambda> at 0x78971f0d1360>, <UnaryOps.SIN: 5>: <function <lambda> at 0x78971f0d12d0>, <BinaryOps.ADD: 1>: <function CStyleLanguage.<lambda> at 0x78971f0d23b0>, <BinaryOps.MAX: 4>: <function <lambda> at 0x78971f0d1090>, <BinaryOps.IDIV: 3>: <function CStyleLanguage.<lambda> at 0x78971f0d2290>, <BinaryOps.MUL: 2>: <function CStyleLanguage.<lambda> at 0x78971f0d2200>, <BinaryOps.MOD: 5>: <function CStyleLanguage.<lambda> at 0x78971f0d2170>, <BinaryOps.CMPLT: 6>: <function CStyleLanguage.<lambda> at 0x78971f0d20e0>, <BinaryOps.CMPNE: 7>: <function CStyleLanguage.<lambda> at 0x78971f0d2050>, <BinaryOps.XOR: 8>: <function CStyleLanguage.<lambda> at 0x78971f0d1fc0>, <BinaryOps.AND: 12>: <function CStyleLanguage.<lambda> at 0x78971f0d1f30>, <BinaryOps.OR: 11>: <function CStyleLanguage.<lambda> at 0x78971f0d1ea0>, <TernaryOps.WHERE: 1>: <function CStyleLanguage.<lambda> at 0x78971f0d1e10>}}    

                                            
                                            code_for_op_hip = { UnaryOps.SQRT: lambda x,dtype: f"__ocml_sqrt_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",                                #     renderer/cstyle.py            :   342: G: {'CUDARenderer': <class 'tinygrad.renderer.cstyle.CUDARenderer'>}    L: {'__name__': 'tinygrad.renderer.cstyle', '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, '__spec__': ModuleSpec(name='tinygrad.renderer.cstyle', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f2b17b0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/cstyle.cpython-310.pyc', 'Dict': typing.Dict, 'List': typing.List, 'Optional': typing.Optional, 'Tuple': typing.Tuple, 'Union': typing.Union, 'DefaultDict': typing.DefaultDict, 'cast': <function cast at 0x789735eb17e0>, 'Literal': typing.Literal, 'Callable': typing.Callable, 'os': <module 'os' from '/usr/lib/python3.10/os.py'>, 'math': <module 'math' (built-in)>, 'defaultdict': <class 'collections.defaultdict'>, 'Counter': <class 'collections.Counter'>, 'UnaryOps': <enum 'UnaryOps'>, 'BinaryOps': <enum 'BinaryOps'>, 'TernaryOps': <enum 'TernaryOps'>, 'UOps': <enum 'UOps'>, 'UOp': <class 'tinygrad.ops.UOp'>, 'strip_parens': <function strip_parens at 0x789730f58a60>, 'getenv': <functools._lru_cache_wrapper object at 0x789730f50720>, 'prod': <function prod at 0x7897327a7520>, 'dedup': <function dedup at 0x789730f581f0>, 'ImageDType': <class 'tinygrad.dtype.ImageDType'>, 'dtypes': <class 'tinygrad.dtype.dtypes'>, 'DType': <class 'tinygrad.dtype.DType'>, 'PtrDType': <class 'tinygrad.dtype.PtrDType'>, 'ConstType': typing.Union[float, int, bool], 'Renderer': <class 'tinygrad.renderer.Renderer'>, 'TensorCore': <class 'tinygrad.renderer.TensorCore'>, 'CStyleLanguage': <class 'tinygrad.renderer.cstyle.CStyleLanguage'>, '_make_clang_dtype': <function _make_clang_dtype at 0x78971f0d27a0>, 'ClangRenderer': <class 'tinygrad.renderer.cstyle.ClangRenderer'>, 'OpenCLRenderer': <class 'tinygrad.renderer.cstyle.OpenCLRenderer'>, 'IntelRenderer': <class 'tinygrad.renderer.cstyle.IntelRenderer'>, 'MetalRenderer': <class 'tinygrad.renderer.cstyle.MetalRenderer'>, 'code_for_op_half': {<UnaryOps.RECIP: 7>: <function <lambda> at 0x78971f0d03a0>, <BinaryOps.MAX: 4>: <function <lambda> at 0x78971f0d1090>, <UnaryOps.SQRT: 6>: <function <lambda> at 0x78971f0d1240>, <UnaryOps.SIN: 5>: <function <lambda> at 0x78971f0d12d0>, <UnaryOps.LOG2: 2>: <function <lambda> at 0x78971f0d1360>, <UnaryOps.EXP2: 1>: <function <lambda> at 0x78971f0d13f0>}, '_nms': 'xyzwabcdefghijkl', '_make_cuda_dtype': <function _make_cuda_dtype at 0x78971f0d1480>}    
                                                                UnaryOps.SIN: lambda x,dtype: f"__ocml_sin_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",                                       
                                                                UnaryOps.LOG2: lambda x,dtype: f"__ocml_log2_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",                                     
                                                                UnaryOps.EXP2: lambda x,dtype: f"__ocml_exp2_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",                                     
                                                                # TODO: MAX with int uses fmax_f32?                                                                                                           
                                                                BinaryOps.MAX: lambda a,b,dtype: f"__ocml_fmax_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32) }({a},{b})",}                             

                                              class AMDRenderer(CStyleLanguage):                                                                                                                         #     renderer/cstyle.py            :   364: 
                                                device = "AMD"                                                                                                                                           #     renderer/cstyle.py            :   365: G: {'code_for_op_hip': {<UnaryOps.SQRT: 6>: <function <lambda> at 0x78971f0d1510>, <UnaryOps.SIN: 5>: <function <lambda> at 0x78971f0d1b40>, <UnaryOps.LOG2: 2>: <function <lambda> at 0x78971f0d1120>, <UnaryOps.EXP2: 1>: <function <lambda> at 0x78971f0d3b50>, <BinaryOps.MAX: 4>: <function <lambda> at 0x78971f0d3d00>}, '_make_hip_code_for_op': <function _make_hip_code_for_op at 0x78971f0d3d90>, '_make_hip_dtype': <function _make_hip_dtype at 0x78971f0d3e20>}    L: {'__module__': 'tinygrad.renderer.cstyle', '__qualname__': 'AMDRenderer'}    
                                                shared_max = 65536                                                                                                                                       #     renderer/cstyle.py            :   366: L: {'device': 'AMD'}    
                                                tensor_cores = [TensorCore(dims=(16,16,16), threads=[(0,8),(0,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in [(dtypes.half, dtypes.float), (dtypes.half, dtypes.half)]] # noqa: E501 #     renderer/cstyle.py            :   367: L: {'shared_max': 65536}    
                                              
                                                # language options
                                                ockl = [(f"__ockl_get_{name}", "unsigned int", "size_t", "const") for name in ["local_id", "group_id", "local_size"]]                                    #     renderer/cstyle.py            :   370: L: {'tensor_cores': [TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.float, threads=[(0, 8), (0, 2), (1, 2)]), TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.half, threads=[(0, 8), (0, 2), (1, 2)])]}    
                                                ocml = [(f"__ocml_{name}_f{n}", f"{dt}, {dt}" if "fmax" == name else dt, dt, atr)                                                                        #     renderer/cstyle.py            :   371: L: {'ockl': [('__ockl_get_local_id', 'unsigned int', 'size_t', 'const'), ('__ockl_get_group_id', 'unsigned int', 'size_t', 'const'), ('__ockl_get_local_size', 'unsigned int', 'size_t', 'const')]}    
                                                          for dt, n in [("float", 32), ("double", 64), ("_Float16", 16)]                                                                                      
                                                          for name, atr in [("fmax", "const"), ("exp2", "pure"), ("log2", "pure"), ("sqrt", "const"), ("sin", "")]]                                           
                                              
                                                kernel_prefix = "\n".join(f'extern "C" __attribute__((device{f", {atr}" if atr else ""})) {dto} {meth}({dti});' for meth,dti,dto,atr in ockl+ocml)       #     renderer/cstyle.py            :   375: L: {'ocml': [('__ocml_fmax_f32', 'float, float', 'float', 'const'), ('__ocml_exp2_f32', 'float', 'float', 'pure'), ('__ocml_log2_f32', 'float', 'float', 'pure'), ('__ocml_sqrt_f32', 'float', 'float', 'const'), ('__ocml_sin_f32', 'float', 'float', ''), ('__ocml_fmax_f64', 'double, double', 'double', 'const'), ('__ocml_exp2_f64', 'double', 'double', 'pure'), ('__ocml_log2_f64', 'double', 'double', 'pure'), ('__ocml_sqrt_f64', 'double', 'double', 'const'), ('__ocml_sin_f64', 'double', 'double', ''), ('__ocml_fmax_f16', '_Float16, _Float16', '_Float16', 'const'), ('__ocml_exp2_f16', '_Float16', '_Float16', 'pure'), ('__ocml_log2_f16', '_Float16', '_Float16', 'pure'), ('__ocml_sqrt_f16', '_Float16', '_Float16', 'const'), ('__ocml_sin_f16', '_Float16', '_Float16', '')]}    
                                                kernel_prefix += '\nextern "C" __attribute__((global))'                                                                                                  #     renderer/cstyle.py            :   376: L: {'kernel_prefix': 'extern "C" __attribute__((device, const)) size_t __ockl_get_local_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_group_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_local_size(unsigned int);\nextern "C" __attribute__((device, const)) float __ocml_fmax_f32(float, float);\nextern "C" __attribute__((device, pure)) float __ocml_exp2_f32(float);\nextern "C" __attribute__((device, pure)) float __ocml_log2_f32(float);\nextern "C" __attribute__((device, const)) float __ocml_sqrt_f32(float);\nextern "C" __attribute__((device)) float __ocml_sin_f32(float);\nextern "C" __attribute__((device, const)) double __ocml_fmax_f64(double, double);\nextern "C" __attribute__((device, pure)) double __ocml_exp2_f64(double);\nextern "C" __attribute__((device, pure)) double __ocml_log2_f64(double);\nextern "C" __attribute__((device, const)) double __ocml_sqrt_f64(double);\nextern "C" __attribute__((device)) double __ocml_sin_f64(double);\nextern "C" __attribute__((device, const)) _Float16 __ocml_fmax_f16(_Float16, _Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_exp2_f16(_Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_log2_f16(_Float16);\nextern "C" __attribute__((device, const)) _Float16 __ocml_sqrt_f16(_Float16);\nextern "C" __attribute__((device)) _Float16 __ocml_sin_f16(_Float16);'}    
                                                code_for_workitem = {"g": lambda x: f"__ockl_get_group_id({x})", "l": lambda x: f"__ockl_get_local_id({x})",                                             #     renderer/cstyle.py            :   377: L: {'kernel_prefix': 'extern "C" __attribute__((device, const)) size_t __ockl_get_local_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_group_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_local_size(unsigned int);\nextern "C" __attribute__((device, const)) float __ocml_fmax_f32(float, float);\nextern "C" __attribute__((device, pure)) float __ocml_exp2_f32(float);\nextern "C" __attribute__((device, pure)) float __ocml_log2_f32(float);\nextern "C" __attribute__((device, const)) float __ocml_sqrt_f32(float);\nextern "C" __attribute__((device)) float __ocml_sin_f32(float);\nextern "C" __attribute__((device, const)) double __ocml_fmax_f64(double, double);\nextern "C" __attribute__((device, pure)) double __ocml_exp2_f64(double);\nextern "C" __attribute__((device, pure)) double __ocml_log2_f64(double);\nextern "C" __attribute__((device, const)) double __ocml_sqrt_f64(double);\nextern "C" __attribute__((device)) double __ocml_sin_f64(double);\nextern "C" __attribute__((device, const)) _Float16 __ocml_fmax_f16(_Float16, _Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_exp2_f16(_Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_log2_f16(_Float16);\nextern "C" __attribute__((device, const)) _Float16 __ocml_sqrt_f16(_Float16);\nextern "C" __attribute__((device)) _Float16 __ocml_sin_f16(_Float16);\nextern "C" __attribute__((global))'}    
                                                                     "i": lambda x: f"(__ockl_get_group_id({x})*__ockl_get_local_size({x})+__ockl_get_local_id({x}))"}                                        
                                                code_for_op = _make_hip_code_for_op()                                                                                                                    #     renderer/cstyle.py            :   379: L: {'code_for_workitem': {'g': <function AMDRenderer.<lambda> at 0x78971f0d1c60>, 'l': <function AMDRenderer.<lambda> at 0x78971f0d1990>, 'i': <function AMDRenderer.<lambda> at 0x78971f0dbac0>}}    

                                                  def _make_hip_code_for_op():                                                                                                                           #     renderer/cstyle.py            :   349: 
                                                    return { k:wrapper(k,v) for k,v in {**CStyleLanguage().code_for_op, **code_for_op_hip}.items() }                                                     #     renderer/cstyle.py            :   357: L: {'wrapper': <function _make_hip_code_for_op.<locals>.wrapper at 0x78971f0db9a0>}    

                                                      def _make_hip_code_for_op():                                                                                                                       #     renderer/cstyle.py            :   349: 
                                                        def wrapper(key, func):                                                                                                                          #     renderer/cstyle.py            :   350: 
                                                          return cast_bf16                                                                                                                               #     renderer/cstyle.py            :   356: L: {'func': <function <lambda> at 0x78971f0d1510>, 'key': <UnaryOps.SQRT: 6>, 'cast_bf16': <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db880>}    

                                                smem_prefix = "__attribute__((shared))"                                                                                                                  #     renderer/cstyle.py            :   380: L: {'__module__': 'tinygrad.renderer.cstyle', '__qualname__': 'AMDRenderer', 'device': 'AMD', 'shared_max': 65536, 'tensor_cores': [TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.float, threads=[(0, 8), (0, 2), (1, 2)]), TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.half, threads=[(0, 8), (0, 2), (1, 2)])], 'ockl': [('__ockl_get_local_id', 'unsigned int', 'size_t', 'const'), ('__ockl_get_group_id', 'unsigned int', 'size_t', 'const'), ('__ockl_get_local_size', 'unsigned int', 'size_t', 'const')], 'ocml': [('__ocml_fmax_f32', 'float, float', 'float', 'const'), ('__ocml_exp2_f32', 'float', 'float', 'pure'), ('__ocml_log2_f32', 'float', 'float', 'pure'), ('__ocml_sqrt_f32', 'float', 'float', 'const'), ('__ocml_sin_f32', 'float', 'float', ''), ('__ocml_fmax_f64', 'double, double', 'double', 'const'), ('__ocml_exp2_f64', 'double', 'double', 'pure'), ('__ocml_log2_f64', 'double', 'double', 'pure'), ('__ocml_sqrt_f64', 'double', 'double', 'const'), ('__ocml_sin_f64', 'double', 'double', ''), ('__ocml_fmax_f16', '_Float16, _Float16', '_Float16', 'const'), ('__ocml_exp2_f16', '_Float16', '_Float16', 'pure'), ('__ocml_log2_f16', '_Float16', '_Float16', 'pure'), ('__ocml_sqrt_f16', '_Float16', '_Float16', 'const'), ('__ocml_sin_f16', '_Float16', '_Float16', '')], 'kernel_prefix': 'extern "C" __attribute__((device, const)) size_t __ockl_get_local_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_group_id(unsigned int);\nextern "C" __attribute__((device, const)) size_t __ockl_get_local_size(unsigned int);\nextern "C" __attribute__((device, const)) float __ocml_fmax_f32(float, float);\nextern "C" __attribute__((device, pure)) float __ocml_exp2_f32(float);\nextern "C" __attribute__((device, pure)) float __ocml_log2_f32(float);\nextern "C" __attribute__((device, const)) float __ocml_sqrt_f32(float);\nextern "C" __attribute__((device)) float __ocml_sin_f32(float);\nextern "C" __attribute__((device, const)) double __ocml_fmax_f64(double, double);\nextern "C" __attribute__((device, pure)) double __ocml_exp2_f64(double);\nextern "C" __attribute__((device, pure)) double __ocml_log2_f64(double);\nextern "C" __attribute__((device, const)) double __ocml_sqrt_f64(double);\nextern "C" __attribute__((device)) double __ocml_sin_f64(double);\nextern "C" __attribute__((device, const)) _Float16 __ocml_fmax_f16(_Float16, _Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_exp2_f16(_Float16);\nextern "C" __attribute__((device, pure)) _Float16 __ocml_log2_f16(_Float16);\nextern "C" __attribute__((device, const)) _Float16 __ocml_sqrt_f16(_Float16);\nextern "C" __attribute__((device)) _Float16 __ocml_sin_f16(_Float16);\nextern "C" __attribute__((global))', 'code_for_workitem': {'g': <function AMDRenderer.<lambda> at 0x78971f0d1c60>, 'l': <function AMDRenderer.<lambda> at 0x78971f0d1990>, 'i': <function AMDRenderer.<lambda> at 0x78971f0dbac0>}, 'code_for_op': {<UnaryOps.SQRT: 6>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db880>, <UnaryOps.RECIP: 7>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db7f0>, <UnaryOps.EXP2: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db760>, <UnaryOps.LOG2: 2>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db6d0>, <UnaryOps.SIN: 5>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db640>, <BinaryOps.ADD: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db5b0>, <BinaryOps.MAX: 4>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db520>, <BinaryOps.IDIV: 3>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db490>, <BinaryOps.MUL: 2>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db400>, <BinaryOps.MOD: 5>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db370>, <BinaryOps.CMPLT: 6>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db2e0>, <BinaryOps.CMPNE: 7>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db250>, <BinaryOps.XOR: 8>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db1c0>, <BinaryOps.AND: 12>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db130>, <BinaryOps.OR: 11>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db0a0>, <TernaryOps.WHERE: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x78971f0db010>}}    
                                                barrier = '__builtin_amdgcn_fence(__ATOMIC_RELEASE, "workgroup");' + '__builtin_amdgcn_s_barrier();' + \                                                 #     renderer/cstyle.py            :   381: L: {'smem_prefix': '__attribute__((shared))'}    
                                                          '__builtin_amdgcn_fence(__ATOMIC_ACQUIRE, "workgroup");'                                                                                            
                                                float4 = "make_float4"                                                                                                                                   #     renderer/cstyle.py            :   383: L: {'barrier': '__builtin_amdgcn_fence(__ATOMIC_RELEASE, "workgroup");__builtin_amdgcn_s_barrier();__builtin_amdgcn_fence(__ATOMIC_ACQUIRE, "workgroup");'}    
                                                uses_ptr_arithmetic = False  # NOTE: this fixes TestLinearizerOverflowAlt                                                                                #     renderer/cstyle.py            :   384: L: {'float4': 'make_float4'}    
                                                type_map = {dtypes.bfloat16: "hip_bfloat16"}                                                                                                             #     renderer/cstyle.py            :   385: L: {'uses_ptr_arithmetic': False}    

                                              class NVRenderer(CUDARenderer): device = "NV"                                                                                                              #     renderer/cstyle.py            :   429: 

                                              class HIPRenderer(AMDRenderer): device = "HIP"                                                                                                             #     renderer/cstyle.py            :   430: 

                                          class ClangDevice(Compiled):                                                                                                                                   #     runtime/ops_clang.py          :    25: 
                                            def __init__(self, device:str):                                                                                                                              #     runtime/ops_clang.py          :    26: 
                                              from tinygrad.runtime.graph.clang import ClangGraph                                                                                                        #     runtime/ops_clang.py          :    27: G: {'ClangRenderer': <class 'tinygrad.renderer.cstyle.ClangRenderer'>, 'ClangCompiler': <class 'tinygrad.runtime.ops_clang.ClangCompiler'>, 'ClangProgram': <class 'tinygrad.runtime.ops_clang.ClangProgram'>, 'ClangDevice': <class 'tinygrad.runtime.ops_clang.ClangDevice'>}    L: {'self': <tinygrad.runtime.ops_clang.ClangDevice object at 0x789735e539d0>, 'device': 'CLANG', '__class__': <class 'tinygrad.runtime.ops_clang.ClangDevice'>}    
                                                from typing import List, Dict, cast                                                                                                                      #     runtime/graph/clang.py        :     1: G: {'__name__': 'tinygrad.runtime.graph.clang', '__doc__': None, '__package__': 'tinygrad.runtime.graph', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x78971f2b2b60>, '__spec__': ModuleSpec(name='tinygrad.runtime.graph.clang', loader=<_frozen_importlib_external.SourceFileLoader object at 0x78971f2b2b60>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/clang.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/clang.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/__pycache__/clang.cpython-310.pyc'}    
                                                import ctypes                                                                                                                                            #     runtime/graph/clang.py        :     2: G: {'List': typing.List, 'Dict': typing.Dict, 'cast': <function cast at 0x789735eb17e0>}    
                                                from tinygrad.helpers import dedup, cpu_time_execution, DEBUG                                                                                            #     runtime/graph/clang.py        :     3: G: {'ctypes': <module 'ctypes' from '/usr/lib/python3.10/ctypes/__init__.py'>}    
                                                from tinygrad.engine.jit import GraphRunner, GraphException                                                                                              #     runtime/graph/clang.py        :     4: G: {'dedup': <function dedup at 0x789730f581f0>, 'cpu_time_execution': <function cpu_time_execution at 0x789730f5a560>, 'DEBUG': <tinygrad.helpers.ContextVar object at 0x789730f3b7f0>}    
                                                from tinygrad.device import Buffer, Device                                                                                                               #     runtime/graph/clang.py        :     5: G: {'GraphRunner': <class 'tinygrad.engine.jit.GraphRunner'>, 'GraphException': <class 'tinygrad.engine.jit.GraphException'>}    
                                                from tinygrad.engine.realize import ExecItem, CompiledRunner                                                                                             #     runtime/graph/clang.py        :     6: G: {'Buffer': <class 'tinygrad.device.Buffer'>, 'Device': <tinygrad.device._Device object at 0x78972db5c9a0>}    
                                                from tinygrad.shape.symbolic import Variable                                                                                                             #     runtime/graph/clang.py        :     7: G: {'ExecItem': <class 'tinygrad.engine.realize.ExecItem'>, 'CompiledRunner': <class 'tinygrad.engine.realize.CompiledRunner'>}    
                                                from tinygrad.runtime.ops_clang import ClangProgram                                                                                                      #     runtime/graph/clang.py        :     8: G: {'Variable': <class 'tinygrad.shape.symbolic.Variable'>}    
                                                from tinygrad.renderer.cstyle import ClangRenderer                                                                                                       #     runtime/graph/clang.py        :     9: G: {'ClangProgram': <class 'tinygrad.runtime.ops_clang.ClangProgram'>}    
                                                render_dtype = ClangRenderer().render_dtype                                                                                                              #     runtime/graph/clang.py        :    10: G: {'ClangRenderer': <class 'tinygrad.renderer.cstyle.ClangRenderer'>}    

                                              super().__init__(device, MallocAllocator, ClangRenderer(), ClangCompiler("compile_clang"), ClangProgram, ClangGraph)                                       #     runtime/ops_clang.py          :    28: L: {'ClangGraph': <class 'tinygrad.runtime.graph.clang.ClangGraph'>}    

                                        return ExecItem(kernel_type(arg, out.device, si.inputs[0].device), list(si.bufs))                                                                                #     engine/realize.py             :   199: 

                                          @dataclass(frozen=True)                                                                                                                                        #     engine/schedule.py            :    25: 
                                          class ScheduleItem:                                                                                                                                                 
                                            @property                                                                                                                                                    #     engine/schedule.py            :    34: 
                                            def inputs(self) -> Tuple[Buffer, ...]:                                                                                                                           
                                              """Read only buffers in the schedule."""                                                                                                                        
                                              return self.bufs[len(self.ast.src):] if self.ast.op is UOps.SINK else self.bufs[1:]                                                                        #     engine/schedule.py            :    36: 

                                          class BufferCopy(Runner):                                                                                                                                      #     engine/realize.py             :   121: 
                                            def __init__(self, total_sz, dest_device, src_device):                                                                                                       #     engine/realize.py             :   122: 
                                              if total_sz >= 1e6: name = f"{type(self).__name__[6:].lower()} {total_sz/1e6:7.2f}M, {dest_device[:7]:>7s} <- {src_device[:7]:7s}"                         #     engine/realize.py             :   123: L: {'self': <tinygrad.engine.realize.BufferCopy object at 0x78971f24fb80>, 'total_sz': 12, 'dest_device': 'CLANG', 'src_device': 'NPY', '__class__': <class 'tinygrad.engine.realize.BufferCopy'>}    
                                              else: name = f"{type(self).__name__[6:].lower()} {total_sz:8d}, {dest_device[:7]:>7s} <- {src_device[:7]:7s}"                                              #     engine/realize.py             :   124: 
                                              super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)                                                                                        #     engine/realize.py             :   125: L: {'name': 'copy       12,   CLANG <- NPY    '}    

                                                def colored(st, color:Optional[str], background=False): return f"\u001b[{10*background+60*(color.upper() == color)+30+['black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'].index(color.lower())}m{st}\u001b[0m" if color is not None else st  # replace the termcolor library with one line  # noqa: E501 #     helpers.py                    :    28: 

                                                class Runner:                                                                                                                                            #     engine/realize.py             :    68: 
                                                  def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0, lds_estimate:Optional[sint]=None):                            #     engine/realize.py             :    69: 
                                                    self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate, self.lds_estimate = \                                            #     engine/realize.py             :    70: L: {'display_name': '\x1b[33mcopy       12,   CLANG <- NPY    \x1b[0m', 'dname': 'CLANG', 'op_estimate': 0, 'mem_estimate': 12}    
                                                      True, display_name, dname, op_estimate, mem_estimate, mem_estimate if lds_estimate is None else lds_estimate                                            

                              if len(capturing) and CAPTURING: capturing[0].add(ei)                                                                                                                      #     engine/realize.py             :   222: L: {'schedule': [ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])], 'var_vals': {}, 'do_update_stats': True, 'ei': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x78971f24fb80>, bufs=[<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None)}    
                              ei.run(var_vals, do_update_stats=do_update_stats)                                                                                                                          #     engine/realize.py             :   223: 

                                @dataclass(frozen=True)                                                                                                                                                  #     engine/realize.py             :   167: 
                                class ExecItem:                                                                                                                                                               
                                  def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:                                            #     engine/realize.py             :   171: 
                                    bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]                                                      #     engine/realize.py             :   172: L: {'self': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x78971f24fb80>, bufs=[<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None), 'wait': False, 'jit': False}    

                                      class Buffer:                                                                                                                                                      #     device.py                     :    52: 
                                        def ensure_allocated(self) -> Buffer: return self.allocate() if not hasattr(self, '_buf') else self                                                              #     device.py                     :    77: 

                                          class LRUAllocator(Allocator):  # pylint: disable=abstract-method                                                                                              #     device.py                     :   143: 
                                            """                                                                                                                                                               
                                            The LRU Allocator is responsible for caching buffers.                                                                                                             
                                            It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.                                                                   
                                            """                                                                                                                                                               
                                            def alloc(self, size:int, options:Optional[BufferOptions]=None):                                                                                             #     device.py                     :   149: 
                                              if len(c := self.cache[(size, options)]): return c.pop()                                                                                                   #     device.py                     :   150: L: {'self': <tinygrad.device._MallocAllocator object at 0x78972db5ff10>, 'size': 12, '__class__': <class 'tinygrad.device.LRUAllocator'>}    
                                              try: return super().alloc(size, options)                                                                                                                   #     device.py                     :   151: L: {'c': []}    

                                                # TODO: size, dest, src are the same type. can we enforce this?                                                                                          #     device.py                     :   132: 
                                                class Allocator:                                                                                                                                              
                                                  def alloc(self, size:int, options:Optional[BufferOptions]=None):                                                                                       #     device.py                     :   133: 
                                                    assert not isinstance(size, int) or size > 0, f"alloc size must be positve, getting {size}"                                                          #     device.py                     :   134: 
                                                    return self._alloc(size, options if options is not None else BufferOptions())                                                                        #     device.py                     :   135: 

                                                      class _MallocAllocator(LRUAllocator):                                                                                                              #     device.py                     :   163: 
                                                        def _alloc(self, size:int, options:BufferOptions): return (ctypes.c_uint8 * size)()                                                              #     device.py                     :   164: 

                                    et = self.prg(bufs, var_vals if var_vals is not None else {}, wait=wait or DEBUG >= 2)                                                                               #     engine/realize.py             :   173: L: {'bufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>]}    

                                      class BufferCopy(Runner):                                                                                                                                          #     engine/realize.py             :   121: 
                                        def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):                                                                              #     engine/realize.py             :   135: 
                                          dest, src = rawbufs[0:2]                                                                                                                                       #     engine/realize.py             :   136: L: {'self': <tinygrad.engine.realize.BufferCopy object at 0x78971f24fb80>, 'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>]}    
                                          assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, {dest.size} != {src.size}, {dest.dtype} != {src.dtype}"                      #     engine/realize.py             :   137: L: {'dest': <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, 'src': <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>}    
                                          st = time.perf_counter()                                                                                                                                       #     engine/realize.py             :   138: 
                                          self.copy(dest, src)                                                                                                                                           #     engine/realize.py             :   139: L: {'st': 24323.574753698}    

                                            class BufferCopy(Runner):                                                                                                                                    #     engine/realize.py             :   121: 
                                              def copy(self, dest, src):                                                                                                                                 #     engine/realize.py             :   126: 
                                                disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')         #     engine/realize.py             :   127: 
                                                if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:                    #     engine/realize.py             :   128: L: {'disk_supports_fast_copyout': False}    
                                                elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):                                                                             #     engine/realize.py             :   130: 
                                                else:                                                                                                                                                    #     engine/realize.py             :   133: 
                                                  dest.copyin(src.as_buffer(allow_zero_copy=True))  # may allocate a CPU buffer depending on allow_zero_copy                                             #     engine/realize.py             :   134: 

                                                    class Buffer:                                                                                                                                        #     device.py                     :    52: 
                                                      def as_buffer(self, allow_zero_copy=False, force_zero_copy=False) -> memoryview:                                                                   #     device.py                     :   109: 
                                                        # zero copy with as_buffer (disabled by default due to use after free)
                                                        if (force_zero_copy or allow_zero_copy) and hasattr(self.allocator, 'as_buffer'): return self.allocator.as_buffer(self._buf)                     #     device.py                     :   111: L: {'self': <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>, 'allow_zero_copy': True, 'force_zero_copy': False}    
                                                        assert not force_zero_copy, "force zero copy was passed, but copy is required"                                                                   #     device.py                     :   112: 
                                                        return self.copyout(memoryview(bytearray(self.nbytes)))                                                                                          #     device.py                     :   113: 

                                                          class Buffer:                                                                                                                                  #     device.py                     :    52: 
                                                            def copyout(self, mv:memoryview) -> memoryview:                                                                                              #     device.py                     :   120: 
                                                              mv = flat_mv(mv)                                                                                                                           #     device.py                     :   121: L: {'mv': <memory at 0x78971f276500>}    

                                                                def flat_mv(mv:memoryview): return mv if len(mv) == 0 else mv.cast("B", shape=(mv.nbytes,))                                              #     helpers.py                    :   313: 

                                                              assert len(mv) == self.nbytes, f"size mismatch, {len(mv)=} != {self.dtype=} {self.size=}"                                                  #     device.py                     :   122: L: {'mv': <memory at 0x78971f274ac0>}    
                                                              assert self.is_allocated(), "can't copyout unallocated buffer"                                                                             #     device.py                     :   123: 
                                                              self.allocator.copyout(mv, self._buf)                                                                                                      #     device.py                     :   124: 

                                                                class NpyAllocator(Allocator):  # pylint: disable=abstract-method                                                                        #     runtime/ops_npy.py            :     5: 
                                                                  def copyout(self, dest:memoryview, src:np.ndarray): dest[:] = flat_mv(np.require(src, requirements='C').data)                          #     runtime/ops_npy.py            :     6: 

                                                              return mv                                                                                                                                  #     device.py                     :   125: 

                                                    class Buffer:                                                                                                                                        #     device.py                     :    52: 
                                                      def copyin(self, mv:memoryview):                                                                                                                   #     device.py                     :   114: 
                                                        mv = flat_mv(mv)                                                                                                                                 #     device.py                     :   115: L: {'self': <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>}    
                                                        assert len(mv) == self.nbytes, f"size mismatch, {len(mv)=} != {self.dtype=} {self.size=}"                                                        #     device.py                     :   116: L: {'mv': <memory at 0x78971f276500>}    
                                                        assert self.is_allocated(), "can't copyin to unallocated buffer"                                                                                 #     device.py                     :   117: 
                                                        self.allocator.copyin(self._buf, mv)                                                                                                             #     device.py                     :   118: 

                                                          class _MallocAllocator(LRUAllocator):                                                                                                          #     device.py                     :   163: 
                                                            def copyin(self, dest, src:memoryview): ctypes.memmove(dest, from_mv(src), len(src))                                                         #     device.py                     :   166: 

                                                              # TODO: make this work with read only memoryviews (if possible)                                                                            #     helpers.py                    :   302: 
                                                              def from_mv(mv:memoryview, to_type=ctypes.c_char):                                                                                              
                                                                return ctypes.cast(ctypes.addressof(to_type.from_buffer(mv)), ctypes.POINTER(to_type * len(mv))).contents                                #     helpers.py                    :   303: L: {'mv': <memory at 0x78971f276500>, 'to_type': <class 'ctypes.c_char'>}    

                                                        return self                                                                                                                                      #     device.py                     :   119: 

                                          if wait:                                                                                                                                                       #     engine/realize.py             :   140: L: {'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], 'var_vals': {}, 'wait': False, 'st': 24323.574753698}    

                                    if do_update_stats:                                                                                                                                                  #     engine/realize.py             :   174: L: {'self': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x78971f24fb80>, bufs=[<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None), 'jit': False, 'do_update_stats': True, 'bufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>]}    
                                      GlobalCounters.kernel_count += 1                                                                                                                                   #     engine/realize.py             :   175: 
                                      GlobalCounters.global_ops += (op_est:=sym_infer(self.prg.op_estimate, var_vals))                                                                                   #     engine/realize.py             :   176: 

                                        def sym_infer(a: Union[Node, int], var_vals: Optional[Dict[Variable, int]]) -> int:                                                                              #     shape/symbolic.py             :   297: 
                                          if isinstance(a, (int, float)): return a                                                                                                                       #     shape/symbolic.py             :   298: G: {'render_python': {<class 'tinygrad.shape.symbolic.Variable'>: <function <lambda> at 0x789730fe71c0>, <class 'tinygrad.shape.symbolic.NumNode'>: <function <lambda> at 0x789730fe7250>, <class 'tinygrad.shape.symbolic.MulNode'>: <function render_mulnode at 0x789730fe7130>, <class 'tinygrad.shape.symbolic.DivNode'>: <function <lambda> at 0x789730fe72e0>, <class 'tinygrad.shape.symbolic.ModNode'>: <function <lambda> at 0x789730fe7370>, <class 'tinygrad.shape.symbolic.LtNode'>: <function <lambda> at 0x789730fe7400>, <class 'tinygrad.shape.symbolic.SumNode'>: <function <lambda> at 0x789730fe7490>, <class 'tinygrad.shape.symbolic.AndNode'>: <function <lambda> at 0x789730fe7520>}}    L: {'a': 0, 'var_vals': {}}    

                                      GlobalCounters.global_mem += (mem_est:=sym_infer(self.prg.mem_estimate, var_vals))                                                                                 #     engine/realize.py             :   177: L: {'op_est': 0}    
                                      if et is not None: GlobalCounters.time_sum_s += et                                                                                                                 #     engine/realize.py             :   178: L: {'mem_est': 12}    
                                      if DEBUG >= 2:                                                                                                                                                     #     engine/realize.py             :   179: 
                                      self.prg.first_run = False                                                                                                                                         #     engine/realize.py             :   186: 
                                    return et                                                                                                                                                            #     engine/realize.py             :   187: 

                                def lower_schedule_item(si:ScheduleItem) -> ExecItem:                                                                                                                    #     engine/realize.py             :   189: 
                                  assert len(set(x.device for x in si.bufs)) == 1 or (si.ast.op is UOps.EXT and si.ast.arg[0] is MetaOps.COPY)                                                           # OLD engine/realize.py             :   190: 
                                  if si.ast.op is UOps.SINK:                                                                                                                                             # OLD engine/realize.py             :   191: 
                                    runner = get_runner(si.outputs[0].device, si.ast)                                                                                                                    #     engine/realize.py             :   192: L: {'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])}    

                                      def get_runner(dname:str, ast:UOp) -> CompiledRunner:                                                                                                              #     engine/realize.py             :   150: 
                                        ckey = (dname, ast.key, BEAM.value, NOOPT.value, False)                                                                                                          #     engine/realize.py             :   151: L: {'dname': 'CLANG', 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    

                                          @dataclass(frozen=True, eq=False)                                                                                                                              #     ops.py                        :    58: 
                                          class UOp:                                                                                                                                                          
                                            @functools.cached_property                                                                                                                                   #     ops.py                        :    73: 
                                            def key(self) -> bytes:                                                                                                                                           
                                              return hashlib.sha256(functools.reduce(lambda x,y: x+y, [s.key for s in self.src], str((self.op, self.dtype, self.arg)).encode())).digest()                #     ops.py                        :    74: L: {'self': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    

                                                # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                              #     dtype.py                      :    31: 
                                                class PtrDType(DType):                                                                                                                                        
                                                  def __repr__(self): return f"PtrDType({super().__repr__()})"                                                                                           #     dtype.py                      :    36: 

                                                    @dataclass(frozen=True, order=True)                                                                                                                  #     dtype.py                      :     9: 
                                                    class DType:                                                                                                                                              
                                                      def __repr__(self): return f"dtypes.{'_'*(c:=self.count!=1)}{INVERSE_DTYPES_DICT[self.name if not c else self.scalar().name]}{str(self.count)*c}"  #     dtype.py                      :    15: 

                                        if cret:=method_cache.get(ckey): return cret                                                                                                                     #     engine/realize.py             :   152: L: {'ckey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, False)}    
                                        bkey = (dname.split(":")[0], ast.key, BEAM.value, NOOPT.value, True)                                                                                             #     engine/realize.py             :   153: 
                                        if bret:=method_cache.get(bkey):                                                                                                                                 #     engine/realize.py             :   154: L: {'bkey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, True)}    
                                        else:                                                                                                                                                            #     engine/realize.py             :   156: 
                                          prg: Program = get_kernel(Device[dname].renderer, ast).to_program()                                                                                            #     engine/realize.py             :   157: 

                                            def get_kernel(renderer:Renderer, ast:UOp) -> Kernel:                                                                                                        #     engine/realize.py             :    17: 
                                              if DEBUG >= 5:                                                                                                                                             #     engine/realize.py             :    18: L: {'renderer': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>}    
                                              k = Kernel(ast, opts=renderer).required_optimizations()                                                                                                    #     engine/realize.py             :    20: 

                                                class Kernel:                                                                                                                                            #     codegen/kernel.py             :    54: 
                                                  def __init__(self, ast:UOp, opts:Optional[Renderer]=None):                                                                                             #     codegen/kernel.py             :    55: 
                                                    if ast.op is UOps.SINK: self.ast = ast                                                                                                               #     codegen/kernel.py             :    56: G: {'Kernel': <class 'tinygrad.codegen.kernel.Kernel'>, '_assert_valid_uop': <function _assert_valid_uop at 0x78972dbabeb0>, 'verify_ast': <function verify_ast at 0x789727101a20>}    L: {'self': <tinygrad.codegen.kernel.Kernel object at 0x78971f0ceb60>, 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>}    
                                                
                                                    self.opts = opts if opts is not None else Device[Device.DEFAULT].renderer                                                                            #     codegen/kernel.py             :    62: 
                                                    try: uop_sts_map = verify_ast(self.ast)                                                                                                              #     codegen/kernel.py             :    63: 

                                                      def verify_ast(ast:UOp) -> Dict[UOp, ShapeTracker]:                                                                                                #     codegen/kernel.py             :   797: 
                                                        assert ast.op is UOps.SINK and all(x.op is UOps.STORE for x in ast.src), "must be SINK"                                                          #     codegen/kernel.py             :   798: 
                                                        assert len(set(x.st_arg.size for x in ast.src)) == 1, "outputs must be exactly the same size"                                                    #     codegen/kernel.py             :   799: 

                                                          @dataclass(frozen=True, eq=False)                                                                                                              #     ops.py                        :    58: 
                                                          class UOp:                                                                                                                                          
                                                            # *** uop syntactic sugar                                                                                                                    #     ops.py                        :    80: 
                                                            @property                                                                                                                                         
                                                            def st_arg(self) -> ShapeTracker:                                                                                                                 
                                                              assert self.op in BUFFER_UOPS, f"st_arg called on {self.op}"                                                                               #     ops.py                        :    81: L: {'self': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}    
                                                              ret = self.src[0 if self.op is UOps.CONST else 1]                                                                                          #     ops.py                        :    82: 
                                                              assert ret.op is UOps.SHAPETRACKER, f"st_arg trying to return {ret}"                                                                       #     ops.py                        :    83: L: {'ret': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}    
                                                              return ret.arg                                                                                                                             #     ops.py                        :    84: 

                                                        sts: Dict[UOp, ShapeTracker] = {}                                                                                                                #     codegen/kernel.py             :   800: 
                                                        for out in ast.src: _assert_valid_uop(out, out.st_arg, sts)                                                                                      #     codegen/kernel.py             :   801: L: {'sts': {}}    

                                                          def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:                                                          #     codegen/kernel.py             :   773: 
                                                            if uop in sts: return                                                                                                                        #     codegen/kernel.py             :   774: L: {'uop': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                            op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                        #     codegen/kernel.py             :   775: 
                                                            # NOTE: UOps.DEFINE_GLOBAL and UOps.DEFINE_LOCAL don't have shape
                                                            if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                     #     codegen/kernel.py             :   777: L: {'op': <UOps.STORE: 22>, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)))}    
                                                            # restore globals from the two stage reduce
                                                            if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                       #     codegen/kernel.py             :   779: 
                                                            for x in src: _assert_valid_uop(x, st, sts)                                                                                                  #     codegen/kernel.py             :   783: 

                                                              def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:                                                      #     codegen/kernel.py             :   773: 
                                                                if uop in sts: return                                                                                                                    # OLD codegen/kernel.py             :   774: 
                                                                op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                    # OLD codegen/kernel.py             :   775: 
                                                                # NOTE: UOps.DEFINE_GLOBAL and UOps.DEFINE_LOCAL don't have shape
                                                                if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                 # OLD codegen/kernel.py             :   777: 
                                                                # restore globals from the two stage reduce
                                                                if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                   # OLD codegen/kernel.py             :   779: 
                                                                for x in src: _assert_valid_uop(x, st, sts)                                                                                              # OLD codegen/kernel.py             :   783: 
                                                                # only reduceuop is allowed to change shape, limited to turning n to 1
                                                                if op is UOps.REDUCE_AXIS: st = ShapeTracker.from_shape(sts[src[0]].reduce(arg[1][-1] if arg[0] is ReduceOps.WMMA else arg[1]))          #     codegen/kernel.py             :   785: L: {'uop': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), 'op': <UOps.SHAPETRACKER: 5>, 'src': (), 'arg': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                                else:                                                                                                                                    #     codegen/kernel.py             :   786: 
                                                                  assert op in {UOps.SHAPETRACKER, UOps.ALU, UOps.CAST, UOps.BITCAST, *BUFFER_UOPS}, f"bad UOp in intermediate uops {uop}"               #     codegen/kernel.py             :   787: 
                                                                  # movementops are pushed to the edges with SHAPETRACKER
                                                                  # elementwise inherits shape
                                                                  st = arg if op is UOps.SHAPETRACKER else sts[src[-1]]                                                                                  #     codegen/kernel.py             :   790: 
                                                                  for x in (src[1:] if op in BUFFER_UOPS else src):                                                                                      #     codegen/kernel.py             :   791: 
                                                                sts[uop] = st                                                                                                                            #     codegen/kernel.py             :   795: 

                                                              def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:                                                      #     codegen/kernel.py             :   773: 
                                                                if uop in sts: return                                                                                                                    # OLD codegen/kernel.py             :   774: 
                                                                op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                    # OLD codegen/kernel.py             :   775: 
                                                                # NOTE: UOps.DEFINE_GLOBAL and UOps.DEFINE_LOCAL don't have shape
                                                                if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                 # OLD codegen/kernel.py             :   777: 
                                                                # restore globals from the two stage reduce
                                                                if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                   # OLD codegen/kernel.py             :   779: 
                                                                for x in src: _assert_valid_uop(x, st, sts)                                                                                              # OLD codegen/kernel.py             :   783: 
                                                                # only reduceuop is allowed to change shape, limited to turning n to 1
                                                                if op is UOps.REDUCE_AXIS: st = ShapeTracker.from_shape(sts[src[0]].reduce(arg[1][-1] if arg[0] is ReduceOps.WMMA else arg[1]))          # OLD codegen/kernel.py             :   785: 
                                                                  assert op in {UOps.SHAPETRACKER, UOps.ALU, UOps.CAST, UOps.BITCAST, *BUFFER_UOPS}, f"bad UOp in intermediate uops {uop}"               # OLD codegen/kernel.py             :   787: 
                                                                  # movementops are pushed to the edges with SHAPETRACKER
                                                                  # elementwise inherits shape
                                                                  st = arg if op is UOps.SHAPETRACKER else sts[src[-1]]                                                                                  # OLD codegen/kernel.py             :   790: 
                                                                  for x in (src[1:] if op in BUFFER_UOPS else src):                                                                                      # OLD codegen/kernel.py             :   791: 
                                                                    if sts[x].shape != st.shape:                                                                                                         #     codegen/kernel.py             :   792: L: {'uop': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'op': <UOps.LOAD: 21>, '_': dtypes.int, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())), 'x': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}    

                                                        shape_dims = [sorted(dedup(dims)) for dims in zip(*[x.shape for x in sts.values()])]                                                             #     codegen/kernel.py             :   802: L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'out': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}    
                                                        assert all(len(x) == 1 or (len(x) == 2 and x[0] == 1) for x in shape_dims), f"shapes must have either 1 or n in each dimension, {shape_dims}"    #     codegen/kernel.py             :   803: L: {'shape_dims': [[3]]}    
                                                        type_verify(list(sts))                                                                                                                           #     codegen/kernel.py             :   804: 

                                                          def type_verify(uops):                                                                                                                         #     ops.py                        :   381: 
                                                            for u in uops:                                                                                                                               #     ops.py                        :   382: L: {'uops': [UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))]}    
                                                              uop, arg, src, dtype = u.op, u.arg, u.src, u.dtype                                                                                         #     ops.py                        :   383: L: {'u': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}    
                                                              if uop is UOps.DEFINE_LOCAL: assert isinstance(dtype, PtrDType), f"invalid dtype for local buffer {dtype}"                                 #     ops.py                        :   384: L: {'uop': <UOps.SHAPETRACKER: 5>, 'arg': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'src': ()}    
                                                              if uop is UOps.DEFINE_GLOBAL: assert isinstance(dtype, (PtrDType, ImageDType)), f"invalid dtype for global buffer {dtype}"                 #     ops.py                        :   385: 
                                                              if isinstance(dtype, ImageDType): assert uop is UOps.DEFINE_GLOBAL, f"{uop} can't be image"                                                #     ops.py                        :   386: 
                                                              if uop is UOps.SHAPETRACKER: assert len(src) == 0, f"SHAPETRACKER must only define a ShapeTracker arg {uop}"                               #     ops.py                        :   387: 
                                                              if uop is UOps.REDUCE_AXIS: assert isinstance(arg, tuple) and len(arg) == 2 and arg[0] in ReduceOps, f"invalid arg for REDUCE_AXIS {arg}"  #     ops.py                        :   388: 
                                                              if uop in {UOps.CONST, UOps.DEFINE_ACC}:                                                                                                   #     ops.py                        :   389: 
                                                              if uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}: assert arg is None and dtype is not None # type is the output type, not an arg        #     ops.py                        :   395: 
                                                              if uop is UOps.CAST: assert dtype.count == 1 and len(src) == 1                                                                             #     ops.py                        :   396: 
                                                              if uop is UOps.VECTORIZE:                                                                                                                  #     ops.py                        :   397: 
                                                              if uop is UOps.LOAD and len(src) > 3 and src[3].op is UOps.ALU: assert src[3].dtype == dtypes.bool and src[2].dtype == dtype               #     ops.py                        :   400: 
                                                              if uop is UOps.GEP: assert dtype == src[0].dtype.scalar(), f"GEP of {src[0].dtype=} should be {src[0].dtype.scalar()} != {dtype}"          #     ops.py                        :   401: 
                                                              if uop is UOps.STORE:                                                                                                                      #     ops.py                        :   402: 
                                                              if uop is UOps.ALU:                                                                                                                        #     ops.py                        :   405: 
                                                            #for u in uops: # branches differently:                                                                                                      # OLD ops.py                        :   382: 
                                                              uop, arg, src, dtype = u.op, u.arg, u.src, u.dtype                                                                                         # OLD ops.py                        :   383: 
                                                              if uop is UOps.DEFINE_LOCAL: assert isinstance(dtype, PtrDType), f"invalid dtype for local buffer {dtype}"                                 # OLD ops.py                        :   384: 
                                                              if uop is UOps.DEFINE_GLOBAL: assert isinstance(dtype, (PtrDType, ImageDType)), f"invalid dtype for global buffer {dtype}"                 # OLD ops.py                        :   385: 
                                                              if isinstance(dtype, ImageDType): assert uop is UOps.DEFINE_GLOBAL, f"{uop} can't be image"                                                # OLD ops.py                        :   386: 
                                                              if uop is UOps.SHAPETRACKER: assert len(src) == 0, f"SHAPETRACKER must only define a ShapeTracker arg {uop}"                               # OLD ops.py                        :   387: 
                                                              if uop is UOps.REDUCE_AXIS: assert isinstance(arg, tuple) and len(arg) == 2 and arg[0] in ReduceOps, f"invalid arg for REDUCE_AXIS {arg}"  # OLD ops.py                        :   388: 
                                                              if uop in {UOps.CONST, UOps.DEFINE_ACC}:                                                                                                   # OLD ops.py                        :   389: 
                                                                if uop is UOps.CONST:                                                                                                                    #     ops.py                        :   390: L: {'u': UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)), 'uop': <UOps.CONST: 10>, 'arg': 2, 'dtype': dtypes.int, 'src': (UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)}    
                                                                  assert dtype is not None and dtype == dtype.scalar(), f"consts must be scalar, got {dtype}"                                            #     ops.py                        :   391: 
                                                                  # TODO: intermediate CONST of Variable is DEFINE_VAR
                                                                  assert (isinstance(arg, Variable) and u.src) or (type(arg) is type(dtypes.as_const(arg, dtype))), f"type of {arg=} does not match {dtype}" #     ops.py                        :   393: 
                                                                if uop is UOps.DEFINE_ACC: assert dtype is not None and src[0].dtype == dtype, f"dtype mismatch {src[0].dtype=} != {dtype=}"             #     ops.py                        :   394: 
                                                              if uop is UOps.ALU:                                                                                                                        #     ops.py                        :   405: 
                                                                if arg in UnaryOps: assert dtype == src[0].dtype, f"{arg} dtype mismatch {dtype=} != {src[0].dtype=}"                                    #     ops.py                        :   406: L: {'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)), 'uop': <UOps.ALU: 17>, 'arg': <BinaryOps.ADD: 1>, 'src': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)))}    
                                                                elif arg in {BinaryOps.CMPLT, BinaryOps.CMPNE}:                                                                                          #     ops.py                        :   407: 
                                                                elif arg is BinaryOps.IDIV:                                                                                                              #     ops.py                        :   411: 
                                                                elif arg in {BinaryOps.SHL, BinaryOps.SHR}:                                                                                              #     ops.py                        :   414: 
                                                                elif arg in BinaryOps: assert dtype == src[0].dtype == src[1].dtype, f"{arg} dtype mismatch {dtype=} != {src[0].dtype=} != {src[1].dtype=}" #     ops.py                        :   417: 
                                                            #for u in uops: # branches differently:                                                                                                      # OLD ops.py                        :   382: 
                                                              uop, arg, src, dtype = u.op, u.arg, u.src, u.dtype                                                                                         # OLD ops.py                        :   383: 
                                                              if uop is UOps.DEFINE_LOCAL: assert isinstance(dtype, PtrDType), f"invalid dtype for local buffer {dtype}"                                 # OLD ops.py                        :   384: 
                                                              if uop is UOps.DEFINE_GLOBAL: assert isinstance(dtype, (PtrDType, ImageDType)), f"invalid dtype for global buffer {dtype}"                 # OLD ops.py                        :   385: 
                                                              if isinstance(dtype, ImageDType): assert uop is UOps.DEFINE_GLOBAL, f"{uop} can't be image"                                                # OLD ops.py                        :   386: 
                                                              if uop is UOps.SHAPETRACKER: assert len(src) == 0, f"SHAPETRACKER must only define a ShapeTracker arg {uop}"                               # OLD ops.py                        :   387: 
                                                              if uop is UOps.REDUCE_AXIS: assert isinstance(arg, tuple) and len(arg) == 2 and arg[0] in ReduceOps, f"invalid arg for REDUCE_AXIS {arg}"  # OLD ops.py                        :   388: 
                                                              if uop in {UOps.CONST, UOps.DEFINE_ACC}:                                                                                                   # OLD ops.py                        :   389: 
                                                              if uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}: assert arg is None and dtype is not None # type is the output type, not an arg        # OLD ops.py                        :   395: 
                                                              if uop is UOps.CAST: assert dtype.count == 1 and len(src) == 1                                                                             # OLD ops.py                        :   396: 
                                                              if uop is UOps.VECTORIZE:                                                                                                                  # OLD ops.py                        :   397: 
                                                              if uop is UOps.LOAD and len(src) > 3 and src[3].op is UOps.ALU: assert src[3].dtype == dtypes.bool and src[2].dtype == dtype               # OLD ops.py                        :   400: 
                                                              if uop is UOps.GEP: assert dtype == src[0].dtype.scalar(), f"GEP of {src[0].dtype=} should be {src[0].dtype.scalar()} != {dtype}"          # OLD ops.py                        :   401: 
                                                              if uop is UOps.STORE:                                                                                                                      # OLD ops.py                        :   402: 
                                                                assert dtype is None, f"{uop} dtype must be None, got {dtype}"                                                                           #     ops.py                        :   403: L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), 'uop': <UOps.STORE: 22>, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)))}    
                                                                if len(src) == 4: assert src[3].dtype == dtypes.bool, f"gate dtype mismatch {src[3].dtype} != {dtypes.bool}"                             #     ops.py                        :   404: 

                                                        return sts                                                                                                                                       #     codegen/kernel.py             :   805: 

                                                    self.reduceops = dedup([x for x in ordered_parents(self.ast) if x.op is UOps.REDUCE_AXIS])                                                           #     codegen/kernel.py             :    71: L: {'self': <tinygrad.codegen.kernel.Kernel object at 0x78971f0ceb60>, 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x78971f103e20>}    

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        def __init__(self, ast:UOp, opts:Optional[Renderer]=None):                                                                                       #     codegen/kernel.py             :    55: 
                                                          @functools.lru_cache(None)                                                                                                                     #     codegen/kernel.py             :    70: 
                                                          def ordered_parents(op:UOp) -> List[UOp]: return dedup([item for x in op.src for item in ordered_parents(x)] + [op])                                

                                                
                                                    self.vars: List[Variable] = self.ast.variables()                                                                                                     #     codegen/kernel.py             :    73: 

                                                      @dataclass(frozen=True, eq=False)                                                                                                                  #     ops.py                        :    58: 
                                                      class UOp:                                                                                                                                              
                                                        def variables(self) -> List[Variable]:                                                                                                           #     ops.py                        :   137: 
                                                          st_vars: List[Set[Variable]] = [x.st_arg.vars() for x in self.sparents if x.op in BUFFER_UOPS]                                                 #     ops.py                        :   138: L: {'self': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    

                                                            @dataclass(frozen=True, eq=False)                                                                                                            #     ops.py                        :    58: 
                                                            class UOp:                                                                                                                                        
                                                              @property  # parents with self                                                                                                             #     ops.py                        :   130: 
                                                              def sparents(self) -> Dict[UOp, None]: return {**self.parents, self:None}                                                                       

                                                                @dataclass(frozen=True, eq=False)                                                                                                        #     ops.py                        :    58: 
                                                                class UOp:                                                                                                                                    
                                                                  @functools.cached_property                                                                                                             #     ops.py                        :   128: 
                                                                  def parents(self) -> Dict[UOp, None]: return {**{x:None for x in self.src}, **{k:None for x in self.src for k in x.parents.keys()}}         

                                                          return sorted(set.union(*st_vars, set([x.arg for x in self.sparents if x.op is UOps.DEFINE_VAR])), key=lambda v: v.expr)                       #     ops.py                        :   139: L: {'st_vars': [set(), set(), set()]}    

                                                    self.bufs: List[UOp] = [x for x in self.ast.parents if x.op in BUFFER_UOPS]                                                                          #     codegen/kernel.py             :    74: 
                                                
                                                    # get earlybufs, before any reduceops
                                                    earlybufs: List[UOp] = [x for reduceop in self.reduceops for x in reduceop.parents if x.op in BUFFER_UOPS]                                           #     codegen/kernel.py             :    77: 
                                                    self.full_buf_index: int = self.bufs.index(earlybufs[0]) if earlybufs else 0                                                                         #     codegen/kernel.py             :    78: L: {'earlybufs': []}    
                                                    # NOTE: full_shape can be wrong if there's a tree of reduces
                                                
                                                    # create new shapetrackers inside this kernel, we will permute them
                                                    self.sts: List[ShapeTracker] = [x.st_arg for x in self.bufs]                                                                                         #     codegen/kernel.py             :    82: 
                                                
                                                    # add the shapetrackers for each reduce
                                                    # we use this to track which axes are reduced in each reduce
                                                    for x in self.reduceops:                                                                                                                             #     codegen/kernel.py             :    86: 
                                                
                                                    # move all reduce axes to the end
                                                    reduce = list(enumerate(zip(self.full_shape, self.output_shape)))                                                                                    #     codegen/kernel.py             :    91: 

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        @property                                                                                                                                        #     codegen/kernel.py             :   156: 
                                                        def full_shape(self) -> Tuple[sint, ...]: return self.sts[self.full_buf_index].shape                                                                  

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        @property                                                                                                                                        #     codegen/kernel.py             :   153: 
                                                        def output_shape(self) -> Tuple[sint, ...]: return self.sts[0].shape                                                                                  

                                                    permute = tuple([i for i,(s,n) in reduce if s == n] + [i for i,(s,n) in reduce if s != n])                                                           #     codegen/kernel.py             :    92: L: {'reduce': [(0, (3, 3))]}    
                                                    self.reshape_and_permute(None, permute)                                                                                                              #     codegen/kernel.py             :    93: L: {'permute': (0,)}    

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        # apply reshape and permute to all shapetrackers                                                                                                 #     codegen/kernel.py             :   203: 
                                                        def reshape_and_permute(self, new_shape_fxn, axis):                                                                                                   
                                                          new_sts = []                                                                                                                                   #     codegen/kernel.py             :   204: L: {'axis': (0,)}    
                                                          for st in self.sts:                                                                                                                            #     codegen/kernel.py             :   205: L: {'new_sts': []}    
                                                            if new_shape_fxn is not None: st = st.reshape(tuple(new_shape_fxn(st.shape)))                                                                #     codegen/kernel.py             :   206: L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                            if axis is not None: st = st.permute(tuple(axis))                                                                                            #     codegen/kernel.py             :   207: 

                                                              @dataclass(frozen=True)                                                                                                                    #     shape/shapetracker.py         :    36: 
                                                              class ShapeTracker:                                                                                                                             
                                                                def permute(self, axis: Tuple[int, ...]) -> ShapeTracker: return ShapeTracker(self.views[0:-1] + (self.views[-1].permute(axis), ))       #     shape/shapetracker.py         :   148: 

                                                                  @dataclass(frozen=True)                                                                                                                #     shape/view.py                 :    85: 
                                                                  class View:                                                                                                                                 
                                                                    @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                     #     shape/view.py                 :   250: 
                                                                    def permute(self, axis: Tuple[int, ...]) -> View:                                                                                         
                                                                      assert sorted(axis) == list(range(len(self.shape))), f"invalid permutation {axis} of len {len(self.shape)}"                        #     shape/view.py                 :   251: L: {'axis': (0,), 'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}    
                                                                      return View.create(tuple(self.shape[a] for a in axis), tuple(self.strides[a] for a in axis), self.offset,                          #     shape/view.py                 :   252: 
                                                                                         tuple(self.mask[a] for a in axis) if self.mask is not None else None)                                                

                                                            new_sts.append(st)                                                                                                                           #     codegen/kernel.py             :   208: L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                          self.sts = new_sts                                                                                                                             #     codegen/kernel.py             :   209: L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}    

                                                
                                                    # parameters for optimization
                                                    self.applied_opts: List[Opt] = []                                                                                                                    #     codegen/kernel.py             :    96: L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x78971f103e20>, 'earlybufs': [], 'reduce': [(0, (3, 3))], 'permute': (0,)}    
                                                    self.group_for_reduces: int = 0                                                                                                                      #     codegen/kernel.py             :    97: 
                                                    self.upcasted: int = 0                                                                                                                               #     codegen/kernel.py             :    98: 
                                                    self.local_dims: int = 0                                                                                                                             #     codegen/kernel.py             :    99: 
                                                    self.tensor_core: Optional[TensorCore] = None                                                                                                        #     codegen/kernel.py             :   100: 
                                                    self.tensor_core_opts: Optional[TensorCoreOptions] = None                                                                                            #     codegen/kernel.py             :   101: 
                                                    self.use_tensor_cores: int = 0                                                                                                                       #     codegen/kernel.py             :   102: 
                                                    # the local aliased buffers for A and B
                                                    self.bufs_for_tensor_core: Dict[UOp, Tuple[int, int]] = {}                                                                                           #     codegen/kernel.py             :   104: 
                                                    self.dont_use_locals: bool = False                                                                                                                   #     codegen/kernel.py             :   105: 
                                                
                                                    # group simplifies
                                                    self.simplify_ones()                                                                                                                                 #     codegen/kernel.py             :   108: 

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        def simplify_ones(self) -> bool:                                                                                                                 #     codegen/kernel.py             :   230: 
                                                          # remove places where the shape is all ones
                                                          # TODO: this should be factored in to multi shape stride
                                                          if self.shape_len == 0: return False                                                                                                           #     codegen/kernel.py             :   233: 

                                                            class Kernel:                                                                                                                                #     codegen/kernel.py             :    54: 
                                                              @property                                                                                                                                  #     codegen/kernel.py             :   162: 
                                                              def shape_len(self) -> int: return len(self.sts[0].shape)                                                                                       

                                                          all_ones = [s==1 for s in self.full_shape]                                                                                                     #     codegen/kernel.py             :   234: 
                                                          self.local_dims -= sum(all_ones[self.first_reduce-self.local_dims:self.first_reduce])                                                          #     codegen/kernel.py             :   235: L: {'all_ones': [False]}    

                                                            class Kernel:                                                                                                                                #     codegen/kernel.py             :    54: 
                                                              @property                                                                                                                                  #     codegen/kernel.py             :   143: 
                                                              def first_reduce(self) -> int:                                                                                                                  
                                                                return [x!=y for x,y in zip(self.sts[0].shape[:self.first_upcast]+(0,), self.full_shape[:self.first_upcast]+(1,))].index(True)           #     codegen/kernel.py             :   144: 

                                                                  class Kernel:                                                                                                                          #     codegen/kernel.py             :    54: 
                                                                    @property                                                                                                                            #     codegen/kernel.py             :   147: 
                                                                    def first_upcast(self) -> int: return self.shape_len-self.upcasted                                                                        

                                                          self.upcasted -= sum(all_ones[self.first_upcast:]) # TODO: no necessary since upcasted axis can't be un-upcasted                               #     codegen/kernel.py             :   236: L: {'all_ones': [False]}    
                                                          self.reshape_and_permute(lambda shape: [x for i,x in enumerate(shape) if not all_ones[i]], None)                                               #     codegen/kernel.py             :   237: 
                                                          return any(all_ones)                                                                                                                           #     codegen/kernel.py             :   238: 

                                                    self.simplify_merge_adjacent()                                                                                                                       #     codegen/kernel.py             :   109: L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x78971f103e20>, 'earlybufs': [], 'reduce': [(0, (3, 3))], 'permute': (0,)}    

                                                      class Kernel:                                                                                                                                      #     codegen/kernel.py             :    54: 
                                                        def simplify_merge_adjacent(self):                                                                                                               #     codegen/kernel.py             :   240: 
                                                          if self.shape_len == 0: return                                                                                                                 #     codegen/kernel.py             :   241: 
                                                          shapes, strides = [x.shape for x in self.sts], [x.real_strides() for x in self.sts]                                                            #     codegen/kernel.py             :   242: 

                                                            @dataclass(frozen=True)                                                                                                                      #     shape/shapetracker.py         :    36: 
                                                            class ShapeTracker:                                                                                                                               
                                                              # NOTE: if a stride is not always valid, it will be None                                                                                   #     shape/shapetracker.py         :   101: 
                                                              def real_strides(self, ignore_valid=False) -> Tuple[Optional[sint], ...]:                                                                       
                                                                if len(self.views) == 1 and self.views[-1].mask is None: return self.views[-1].strides                                                   #     shape/shapetracker.py         :   102: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'ignore_valid': False}    

                                                      
                                                          # if it's an image, insert fake strides such that this fusion doesn't happen across image axes
                                                          if isinstance(self.membufs[0].dtype, ImageDType):                                                                                              #     codegen/kernel.py             :   245: L: {'shapes': [(3,), (3,), (3,)], 'strides': [(1,), (1,), (0,)]}    

                                                            class Kernel:                                                                                                                                #     codegen/kernel.py             :    54: 
                                                              @property                                                                                                                                  #     codegen/kernel.py             :   131: 
                                                              def membufs(self) -> List[UOp]: return list({x.src[0].key:x.src[0] for x in self.bufs if x.op in {UOps.LOAD, UOps.STORE}}.values())             

                                                      
                                                          # merge dimensions if we can, multi _merge_dims
                                                          # NOTE: this does not always preserve the reduce dimension
                                                          # TODO: move this into shapetracker, with tests!
                                                          # TODO: how does this work with multi-reduce?
                                                          rets = [[(s[0], st[0])] for s,st in zip(shapes, strides)]                                                                                      #     codegen/kernel.py             :   261: 
                                                          for i in range(1, len(shapes[0])):                                                                                                             #     codegen/kernel.py             :   262: L: {'rets': [[(3, 1)], [(3, 1)], [(3, 0)]]}    
                                                      
                                                          # do the reshapes
                                                          for i,x in enumerate(rets[:len(self.sts)]): self.sts[i] = self.sts[i].reshape(tuple([y[0] for y in x]))                                        #     codegen/kernel.py             :   275: 

                                                class Kernel:                                                                                                                                            #     codegen/kernel.py             :    54: 
                                                  def required_optimizations(self) -> Kernel:                                                                                                            #     codegen/kernel.py             :   482: 
                                                    if isinstance(self.membufs[0].dtype, ImageDType):                                                                                                    #     codegen/kernel.py             :   483: 
                                                    return self                                                                                                                                          #     codegen/kernel.py             :   488: 

                                              if not NOOPT:                                                                                                                                              #     engine/realize.py             :    21: L: {'k': <tinygrad.codegen.kernel.Kernel object at 0x78971f0ceb60>}    
                                                if not (used_tensor_cores:=k.apply_tensor_cores(getenv("TC", 1))): k.hand_coded_optimizations()                                                          #     engine/realize.py             :    22: 

                                                  class Kernel:                                                                                                                                          #     codegen/kernel.py             :    54: 
                                                    def apply_tensor_cores(self, use_tensor_cores=1, extra_opts:Optional[List[Opt]]=None, axis:int=0, tc_opt:Optional[int]=None) -> bool:                #     codegen/kernel.py             :   347: 
                                                      """ Attempts to apply a tensor core optimization to the kernel.  If one exists and applies properly, return true, otherwise return false.               
                                                      Tensor cores are optimized instructions that matrix multiply-accumulate across a wave of threads: D(M, N) = A(M, K) * B(K, N) + C(M, N).                
                                                                                                                                                                                                              
                                                      Keyword arguments:                                                                                                                                      
                                                      use_tensor_cores -- controls how tensor cores are applied (default 1)                                                                                   
                                                        0: will disable any tensor core matching                                                                                                              
                                                        1: enable tensor cores                                                                                                                                
                                                        2: apply tensor core shape but don't use UOp.WMMA                                                                                                     
                                                      extra_opts -- additional Opt's to apply after the tensor core instead of the hand-coded additional Opt's (default None)                                 
                                                      tc_opt -- controls which kinds of kernels may be eligible for tensor cores application (default 2 during BEAM, 0 otherwise)                             
                                                        0: applies to only kernels with a single reduce axis and direct UOps.LOAD into BinaryOps.MUL                                                          
                                                        1: allows kernels with multiple reduce axes and also multiplication of UOps.CAST'd buffers                                                            
                                                        2: allows kernels with M, N, K axes that are not multiples of the tensor core dimensions by applying padding those axes as needed                     
                                                      """                                                                                                                                                     
                                                      if tc_opt is None: tc_opt = TC_OPT.value                                                                                                           #     codegen/kernel.py             :   362: L: {'use_tensor_cores': 1, 'axis': 0}    
                                                      if not self.opts.tensor_cores and use_tensor_cores != 2: return False                                                                              #     codegen/kernel.py             :   363: L: {'tc_opt': 0}    

                                                  class Kernel:                                                                                                                                          #     codegen/kernel.py             :    54: 
                                                    def hand_coded_optimizations(self) -> Kernel:                                                                                                        #     codegen/kernel.py             :   490: 
                                                      self.required_optimizations()                                                                                                                      #     codegen/kernel.py             :   491: 
                                                  
                                                      # should use matvec - TODO: adjust/tune based on the wide vs tall/large vs small mat
                                                      MV_BLOCKSIZE, MV_THREADS_PER_ROW, MV_ROWS_PER_THREAD = getenv("MV_BLOCKSIZE", 4), getenv("MV_THREADS_PER_ROW", 8), getenv("MV_ROWS_PER_THREAD", 4) #     codegen/kernel.py             :   494: 
                                                      if self.opts.has_local and getenv("MV",1) != 0 and (MV_BLOCKSIZE > 1 or MV_THREADS_PER_ROW > 1 or MV_ROWS_PER_THREAD > 1) and  \                   #     codegen/kernel.py             :   495: L: {'MV_BLOCKSIZE': 4, 'MV_THREADS_PER_ROW': 8, 'MV_ROWS_PER_THREAD': 4}    
                                                          self.reduceop is not None and self.reduceop.arg[0] is ReduceOps.SUM and len(self.full_shape) >= 2 and self.opts.has_shared and \                    
                                                          (mulop:=self.reduceop.src[0]).arg is BinaryOps.MUL and mulop.src[0].op is UOps.LOAD and mulop.src[1].op is UOps.LOAD:                               
                                                  
                                                      if self.opts.has_local and self.opts.has_shared and all_int(self.sts[0].shape[:self.first_reduce]):                                                #     codegen/kernel.py             :   511: 
                                                  
                                                      # upcast float4 images
                                                      for buf_index,buf in enumerate(self.bufs):                                                                                                         #     codegen/kernel.py             :   530: 
                                                        unit_stride_axes_mul_4 = [i for i in self.sts[buf_index].unit_stride_axes(ignore_valid=True) if self.sts[buf_index].shape[i]%4 == 0]             #     codegen/kernel.py             :   531: L: {'buf': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), 'buf_index': 0}    

                                                          @dataclass(frozen=True)                                                                                                                        #     shape/shapetracker.py         :    36: 
                                                          class ShapeTracker:                                                                                                                                 
                                                            def unit_stride_axes(self, ignore_valid=False) -> List[int]: return [i for i,st in enumerate(self.real_strides(ignore_valid)) if st == 1]    #     shape/shapetracker.py         :   117: 

                                                        if buf.src[0].dtype.__class__ is ImageDType:                                                                                                     #     codegen/kernel.py             :   532: L: {'unit_stride_axes_mul_4': []}    
                                                  
                                                      # no more opt if we are grouping
                                                      if self.group_for_reduces: return self                                                                                                             #     codegen/kernel.py             :   541: L: {'buf': UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)), 'buf_index': 2, 'unit_stride_axes_mul_4': []}    
                                                  
                                                      # **** below this line need to be optional and benchmarked ****
                                                  
                                                      # TODO: doing extra upcasts with images doesn't work for some reason (maybe has to do with to_image_idx)
                                                      # to trigger the above bug, remove prod(self.full_shape[self.first_upcast:]) from the below
                                                      # expression and run test/test_ops.py with IMAGE=2
                                                      # if there are small dims with lots of valid masks, upcast them (they might be from Tensor.stack)
                                                      # this can be made much smarter
                                                      to_upcast: List[int] = []                                                                                                                          #     codegen/kernel.py             :   550: 
                                                      # upcast leading axes first (hack-ish for winograd; we actually want to upcast masked axes with low stride first)
                                                      for axis in range(self.first_reduce):                                                                                                              #     codegen/kernel.py             :   552: L: {'to_upcast': []}    
                                                        # we might want to be able to split axes that are masked, or refuse to merge them in simplify_merge_adjacent
                                                        # for now skip upcasting here if there is a symbolic axis
                                                        if isinstance(self.full_shape[axis], int) and self.full_shape[axis] <= 7 and any(st.axis_is_masked(axis) for st in self.sts) and \               #     codegen/kernel.py             :   555: L: {'axis': 0}    
                                                          prod(self.full_shape[self.first_upcast:]) * prod(self.full_shape[j] for j in to_upcast) * self.full_shape[axis] <= 7 * 7:                           

                                                          @dataclass(frozen=True)                                                                                                                        #     shape/shapetracker.py         :    36: 
                                                          class ShapeTracker:                                                                                                                                 
                                                            def axis_is_masked(self, axis:int) -> bool:                                                                                                  #     shape/shapetracker.py         :   134: 
                                                              _, valid = self.to_indexed_uops()                                                                                                          #     shape/shapetracker.py         :   135: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'axis': 0}    

                                                                @dataclass(frozen=True)                                                                                                                  #     shape/shapetracker.py         :    36: 
                                                                class ShapeTracker:                                                                                                                           
                                                                  def to_indexed_uops(self, _idxs:Optional[List[UOp]]=None) -> Tuple[UOp, UOp]:                                                          #     shape/shapetracker.py         :    70: 
                                                                    idxs = [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(s)), i) for i,s in enumerate(self.shape)] \       #     shape/shapetracker.py         :    71: 
                                                                      if _idxs is None else _idxs                                                                                                             

                                                                      # TODO: this needs to be replaced, there shouldn't be variables in the shapetracker, only ints and UOps                            #     shape/shapetracker.py         :    14: 
                                                                      def variable_to_uop(x, ctx=None) -> UOp: return UOp.const(dtypes.pyint, x) if isinstance(x, int) else x.render(render_ops, ctx)         

                                                                    idx, valid = _uop_view(self.views[-1], idxs, UOp.const(dtypes.bool, True))                                                           #     shape/shapetracker.py         :    73: L: {'idxs': [UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),))]}    

                                                                      def _uop_view(view:View, idxs:List[UOp], vexpr:UOp) -> Tuple[UOp, UOp]:                                                            #     shape/shapetracker.py         :    25: 
                                                                        # TODO: dtypes.realint
                                                                        iexpr = variable_to_uop(view.offset)                                                                                             #     shape/shapetracker.py         :    27: L: {'view': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True), 'vexpr': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                                        for idx,sh,st,m in zip(idxs, view.shape, view.strides, view.mask if view.mask is not None else [None]*len(view.shape)):          #     shape/shapetracker.py         :    28: L: {'iexpr': UOp(UOps.CONST, dtypes.pyint, arg=0, src=())}    
                                                                          if sh != 1 and st != 0: iexpr = iexpr + idx*variable_to_uop(st)                                                                #     shape/shapetracker.py         :    29: L: {'idx': UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)), 'sh': 3, 'st': 1}    
                                                                          if m is not None:                                                                                                              #     shape/shapetracker.py         :    30: L: {'iexpr': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),))}    
                                                                        return iexpr, vexpr                                                                                                              #     shape/shapetracker.py         :    33: 

                                                                    for view in reversed(self.views[0:-1]):                                                                                              #     shape/shapetracker.py         :    74: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'valid': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                                    return idx, valid                                                                                                                    #     shape/shapetracker.py         :    82: 

                                                              return axis in [x.arg for x in graph_rewrite(valid, constant_folder).sparents if x.op is UOps.RANGE]                                       #     shape/shapetracker.py         :   136: L: {'axis': 0, '_': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),))}    

                                                                def graph_rewrite(sink:UOp, pm:PatternMatcher) -> UOp:                                                                                   #     ops.py                        :   326: 
                                                                  nodes: Dict[Tuple, UOp] = {}                                                                                                           #     ops.py                        :   327: L: {'sink': UOp(UOps.CONST, dtypes.bool, arg=True, src=()), 'pm': <tinygrad.ops.PatternMatcher object at 0x78973055a470>}    
                                                                  replace: Dict[UOp, UOp] = {}                                                                                                           #     ops.py                        :   328: L: {'nodes': {}}    
                                                                  return __inner_rewrite(sink)                                                                                                           #     ops.py                        :   337: L: {'replace': {}, '__inner_rewrite': <function graph_rewrite.<locals>.__inner_rewrite at 0x78971f0da4d0>}    

                                                                    def graph_rewrite(sink:UOp, pm:PatternMatcher) -> UOp:                                                                               #     ops.py                        :   326: 
                                                                      def __inner_rewrite(n:UOp) -> UOp:                                                                                                 #     ops.py                        :   329: 
                                                                        if rn := replace.get(n): return rn                                                                                               #     ops.py                        :   330: L: {'n': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                                        replace_source = (n.op, n.dtype, new_src:=tuple(__inner_rewrite(y) for y in n.src), n.arg)                                       #     ops.py                        :   331: 
                                                                        if found := nodes.get(replace_source): replace[n] = found                                                                        #     ops.py                        :   332: L: {'new_src': (), 'replace_source': (<UOps.CONST: 10>, dtypes.bool, (), True)}    
                                                                        else:                                                                                                                            #     ops.py                        :   333: 
                                                                          x = UOp(*replace_source) if new_src != n.src else n                                                                            #     ops.py                        :   334: 
                                                                          nodes[replace_source] = replace[n] = found = __inner_rewrite(new_x) if (new_x := pm.rewrite(x)) else x                         #     ops.py                        :   335: L: {'x': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    

                                                                            class PatternMatcher:                                                                                                        #     ops.py                        :   267: 
                                                                              def rewrite(self, uop:UOp) -> Optional[UOp]:                                                                               #     ops.py                        :   279: 
                                                                                ler = set([(u.op, u.arg) for u in uop.src] + [(u.op, None) for u in uop.src])                                            #     ops.py                        :   280: L: {'self': <tinygrad.ops.PatternMatcher object at 0x78973055a470>, 'uop': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                                                for p,fxn,early_reject in itertools.chain(self.pdict[(uop.op, uop.arg)], self.pdict[(uop.op, None)]):                    #     ops.py                        :   281: L: {'ler': set()}    
                                                                                return None                                                                                                              #     ops.py                        :   284: 

                                                                        return found                                                                                                                     #     ops.py                        :   336: L: {'n': UOp(UOps.CONST, dtypes.bool, arg=True, src=()), '__inner_rewrite': <function graph_rewrite.<locals>.__inner_rewrite at 0x78971f0da4d0>, 'nodes': {(<UOps.CONST: 10>, dtypes.bool, (), True): UOp(UOps.CONST, dtypes.bool, arg=True, src=())}, 'pm': <tinygrad.ops.PatternMatcher object at 0x78973055a470>, 'replace': {UOp(UOps.CONST, dtypes.bool, arg=True, src=()): UOp(UOps.CONST, dtypes.bool, arg=True, src=())}, 'new_src': (), 'replace_source': (<UOps.CONST: 10>, dtypes.bool, (), True), 'found': UOp(UOps.CONST, dtypes.bool, arg=True, src=()), 'x': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    

                                                      for axis in to_upcast[::-1]: self.apply_opt(Opt(OptOps.UPCAST, axis, 0))                                                                           #     codegen/kernel.py             :   559: 
                                                  
                                                      # potentially do more upcasts of non reduce axes based on a heuristic
                                                      upcasted_axis = set()                                                                                                                              #     codegen/kernel.py             :   562: 
                                                      while prod(self.sts[0].shape[:self.first_reduce]) >= 1024:                                                                                         #     codegen/kernel.py             :   563: L: {'upcasted_axis': set()}    
                                                  
                                                      # if last dim is small(ish) and it's a reduce dim, upcast the reduce (loop unrolling). no simplify needed since it's just an upcast.
                                                      if self.first_reduce < self.first_upcast and (prod(self.full_shape[self.first_upcast:]) <= 4 or not any(r for _,_,r in self.upcasted_axis(self.full_buf_index))) and (self.upcasted == 0 or prod(self.full_shape[-self.upcasted:]) < 64):  # noqa: E501 #     codegen/kernel.py             :   577: 
                                                  
                                                      # if nothing at all is upcasted and it's easy to, do an upcast
                                                      # TODO: this is breaking the tests
                                                      for splits in [4]:                                                                                                                                 #     codegen/kernel.py             :   591: 
                                                        if self.upcasted == 0 and self.full_unupcasted_shape and self.full_unupcasted_shape[-1] % splits == 0:                                           #     codegen/kernel.py             :   592: L: {'splits': 4}    

                                                          class Kernel:                                                                                                                                  #     codegen/kernel.py             :    54: 
                                                            @property                                                                                                                                    #     codegen/kernel.py             :   159: 
                                                            def full_unupcasted_shape(self) -> Tuple[sint, ...]: return self.full_shape[:self.first_upcast]                                                   

                                                  
                                                      # **** local groups ****
                                                  
                                                      if self.opts.has_local:                                                                                                                            #     codegen/kernel.py             :   597: 
                                                  
                                                      return self                                                                                                                                        #     codegen/kernel.py             :   615: 

                                                if BEAM >= 1:                                                                                                                                            #     engine/realize.py             :    23: L: {'used_tensor_cores': False}    
                                              if logkerns is not None: logkerns.writelines([f"{(k.ast, k.applied_opts)}\n"])                                                                             #     engine/realize.py             :    62: 
                                              if DEBUG >= 5: print((k.ast, k.applied_opts)) # print here to show final applied_opts for all kernels instead of just in beam_search                       #     engine/realize.py             :    63: 
                                              return k                                                                                                                                                   #     engine/realize.py             :    64: 

                                            class Kernel:                                                                                                                                                #     codegen/kernel.py             :    54: 
                                              def to_program(self, name_override:Optional[str]=None) -> Program:                                                                                         #     codegen/kernel.py             :   755: 
                                                self.linearize()                                                                                                                                         #     codegen/kernel.py             :   756: 

                                                  class Kernel:                                                                                                                                          #     codegen/kernel.py             :    54: 
                                                    def linearize(self) -> Kernel:                                                                                                                       #     codegen/kernel.py             :   738: 
                                                      modified_ast = self.get_optimized_ast()                                                                                                            #     codegen/kernel.py             :   739: 

                                                        class Kernel:                                                                                                                                    #     codegen/kernel.py             :    54: 
                                                          def get_optimized_ast(self) -> UOp:                                                                                                            #     codegen/kernel.py             :   632: 
                                                            return fixup_ast(self.ast)                                                                                                                   #     codegen/kernel.py             :   734: L: {'fixup_ast': <functools._lru_cache_wrapper object at 0x78971f110880>}    

                                                              class Kernel:                                                                                                                              #     codegen/kernel.py             :    54: 
                                                                def get_optimized_ast(self) -> UOp:                                                                                                      #     codegen/kernel.py             :   632: 
                                                                  # set the shapetrackers to the optimized ones, fixup reduceop                                                                          #     codegen/kernel.py             :   636: 
                                                                  # transformed to the final UOp                                                                                                              
                                                                  @functools.lru_cache(None)                                                                                                                  
                                                                  def fixup_ast(op:UOp, apply_to_st=None) -> UOp:                                                                                             
                                                                    arg = op.arg                                                                                                                         #     codegen/kernel.py             :   637: L: {'op': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    
                                                                    if op.op in BUFFER_UOPS:                                                                                                             #     codegen/kernel.py             :   638: 
                                                                    if op.op is UOps.REDUCE_AXIS:                                                                                                        #     codegen/kernel.py             :   645: 
                                                                    elif op.op is UOps.SINK:                                                                                                             #     codegen/kernel.py             :   731: 
                                                                      arg = KernelInfo(self.local_dims, self.upcasted, self.dont_use_locals)                                                             #     codegen/kernel.py             :   732: 
                                                                    return replace(op, src=tuple(fixup_ast(x, apply_to_st) for x in op.src), arg=arg)                                                    #     codegen/kernel.py             :   733: L: {'arg': KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False)}    

                                                                      class Kernel:                                                                                                                      #     codegen/kernel.py             :    54: 
                                                                        def get_optimized_ast(self) -> UOp:                                                                                              #     codegen/kernel.py             :   632: 
                                                                          # set the shapetrackers to the optimized ones, fixup reduceop                                                                  #     codegen/kernel.py             :   636: 
                                                                          # transformed to the final UOp                                                                                                      
                                                                          @functools.lru_cache(None)                                                                                                          
                                                                          def fixup_ast(op:UOp, apply_to_st=None) -> UOp:                                                                                     
                                                                            arg = op.arg                                                                                                                 # OLD codegen/kernel.py             :   637: 
                                                                            if op.op in BUFFER_UOPS:                                                                                                     # OLD codegen/kernel.py             :   638: 
                                                                              # for locals, we use the ShapeTracker that's in the srcs
                                                                              st = op.st_arg if op.src[0].op is UOps.DEFINE_LOCAL else self.sts[self.bufs.index(op)]                                     #     codegen/kernel.py             :   640: L: {'op': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}    
                                                                              st_uop = (st if apply_to_st is None else apply_to_st(st)).to_uop()                                                         #     codegen/kernel.py             :   641: L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                                              if op.op is UOps.CONST: return replace(op, src=(st_uop,))                                                                  #     codegen/kernel.py             :   642: L: {'st_uop': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}    
                                                                              if op.op is UOps.STORE: return replace(op, src=(op.src[0], st_uop, fixup_ast(op.src[2], apply_to_st)))                     #     codegen/kernel.py             :   643: 

                                                                                class Kernel:                                                                                                            #     codegen/kernel.py             :    54: 
                                                                                  def get_optimized_ast(self) -> UOp:                                                                                    #     codegen/kernel.py             :   632: 
                                                                                    # set the shapetrackers to the optimized ones, fixup reduceop                                                        #     codegen/kernel.py             :   636: 
                                                                                    # transformed to the final UOp                                                                                            
                                                                                    @functools.lru_cache(None)                                                                                                
                                                                                    def fixup_ast(op:UOp, apply_to_st=None) -> UOp:                                                                           
                                                                                      arg = op.arg                                                                                                       # OLD codegen/kernel.py             :   637: 
                                                                                      if op.op in BUFFER_UOPS:                                                                                           # OLD codegen/kernel.py             :   638: 
                                                                                        # for locals, we use the ShapeTracker that's in the srcs
                                                                                        st = op.st_arg if op.src[0].op is UOps.DEFINE_LOCAL else self.sts[self.bufs.index(op)]                           # OLD codegen/kernel.py             :   640: 
                                                                                        st_uop = (st if apply_to_st is None else apply_to_st(st)).to_uop()                                               # OLD codegen/kernel.py             :   641: 
                                                                                        if op.op is UOps.CONST: return replace(op, src=(st_uop,))                                                        # OLD codegen/kernel.py             :   642: 
                                                                                        if op.op is UOps.STORE: return replace(op, src=(op.src[0], st_uop, fixup_ast(op.src[2], apply_to_st)))           # OLD codegen/kernel.py             :   643: 
                                                                                        return replace(op, src=(op.src[0], st_uop, *[fixup_ast(x, apply_to_st) for x in op.src[2:]]))                    #     codegen/kernel.py             :   644: L: {'op': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'st_uop': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}    

                                                  
                                                      if DEBUG >= 3:                                                                                                                                     #     codegen/kernel.py             :   741: L: {'modified_ast': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    
                                                      verify_ast(modified_ast)                                                                                                                           #     codegen/kernel.py             :   746: 
                                                  
                                                      self.uops:List[UOp] = linearize_uop(full_graph_rewrite(ast_to_uop(modified_ast, self.opts), self.opts))                                            #     codegen/kernel.py             :   748: 

                                                        def ast_to_uop(ast:UOp, opts:Renderer) -> UOp: return IndependentLowerer().lower(ast, opts)                                                      #     codegen/lowerer.py            :   132: 

                                                          class IndependentLowerer:                                                                                                                      #     codegen/lowerer.py            :    37: 
                                                            def lower(self, ast:UOp, opts:Renderer) -> UOp:                                                                                              #     codegen/lowerer.py            :    38: 
                                                              self.output_count = len(ast.src)                                                                                                           #     codegen/lowerer.py            :    39: G: {'all_int': <function all_int at 0x789730f584c0>, 'get_contraction': <function get_contraction at 0x789730f59000>, 'prod': <function prod at 0x7897327a7520>, 'partition': <function partition at 0x789730f58d30>, 'flatten': <function flatten at 0x789730f588b0>, '_limit_dims': <function _limit_dims at 0x78972dbabac0>, 'get_grouped_dims': <function get_grouped_dims at 0x78972dbabb50>, 'IndependentLowerer': <class 'tinygrad.codegen.lowerer.IndependentLowerer'>, 'ast_to_uop': <function ast_to_uop at 0x78972dbabbe0>}    L: {'self': <tinygrad.codegen.lowerer.IndependentLowerer object at 0x78971f0cdea0>, 'ast': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>}    
                                                          
                                                              ki = ast.arg if isinstance(ast.arg, KernelInfo) else KernelInfo()                                                                          #     codegen/lowerer.py            :    41: 
                                                              # NOTE: assumes the shape is <global dims> <local dims> <group_for_reduces> <reduces> <upcasts/unrolls>
                                                              full_shape = ast.full_shape                                                                                                                #     codegen/lowerer.py            :    43: L: {'ki': KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False)}    

                                                                @dataclass(frozen=True, eq=False)                                                                                                        #     ops.py                        :    58: 
                                                                class UOp:                                                                                                                                    
                                                                  @functools.cached_property                                                                                                             #     ops.py                        :   132: 
                                                                  def full_shape(self) -> Tuple[sint, ...]:                                                                                                   
                                                                    if self.op is UOps.SHAPETRACKER: return self.arg.shape                                                                               #     ops.py                        :   133: L: {'self': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    
                                                                    # NOTE: UOps.DEFINE_GLOBAL and UOps.DEFINE_LOCAL don't have shape
                                                                    return tuple(max(x) for x in zip(*[x.full_shape for x in self.src if x.op not in {UOps.DEFINE_GLOBAL, UOps.DEFINE_LOCAL}]))          #     ops.py                        :   135: 

                                                              first_upcasted = len(full_shape)-ki.upcasted                                                                                               #     codegen/lowerer.py            :    44: L: {'full_shape': (3,)}    
                                                              first_output_st: ShapeTracker = ast.src[0].st_arg                                                                                          #     codegen/lowerer.py            :    45: L: {'first_upcasted': 1}    
                                                              # if there's no reduce, this is first_upcasted
                                                              first_reduce = [x!=y for x,y in zip(first_output_st.shape[:first_upcasted]+(0,), full_shape[:first_upcasted]+(1,))].index(True)            #     codegen/lowerer.py            :    47: L: {'first_output_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                              local_loads = [x for x in ast.parents if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL]                                           #     codegen/lowerer.py            :    48: L: {'first_reduce': 1}    
                                                              # NOTE: sum up the reduced axes looking across all local loads, yields the number of grouped reduces
                                                              group_for_reduces = sum([any(j!=y for j in x) for x,y in zip(                                                                              #     codegen/lowerer.py            :    50: L: {'local_loads': []}    
                                                                [[l.st_arg.shape[i] for l in local_loads] for i in range(first_reduce,first_upcasted)],                                                       
                                                                first_output_st.shape[first_reduce:first_upcasted])]) if local_loads else 0                                                                   
                                                              global_dims = first_reduce-ki.local_dims                                                                                                   #     codegen/lowerer.py            :    53: L: {'group_for_reduces': 0}    
                                                          
                                                              if opts.has_local:                                                                                                                         #     codegen/lowerer.py            :    55: L: {'global_dims': 1}    
                                                              else:                                                                                                                                      #     codegen/lowerer.py            :    63: 
                                                                # all loops are RANGES
                                                                self.idxs = [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(g)), (i, False))                                 #     codegen/lowerer.py            :    65: 
                                                                             for i,g in enumerate(full_shape[:first_reduce])]                                                                                 
                                                          
                                                              # reduce loops
                                                              self.idxs += [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(g)), (i, True))                                   #     codegen/lowerer.py            :    69: 
                                                                for i,g in enumerate(full_shape[first_reduce+group_for_reduces:first_upcasted], start=first_reduce+group_for_reduces)]                        
                                                          
                                                              # upcast loops
                                                              for i,g in enumerate(full_shape[first_upcasted:], start=first_upcasted):                                                                   #     codegen/lowerer.py            :    73: 
                                                          
                                                              # late indexes (group for reduce)
                                                              self.ridxs = self.idxs[:]                                                                                                                  #     codegen/lowerer.py            :    78: 
                                                              for a in range(first_reduce, first_reduce+group_for_reduces):                                                                              #     codegen/lowerer.py            :    79: 
                                                          
                                                              self.uop_cache: Dict[UOp, UOp] = {}                                                                                                        #     codegen/lowerer.py            :    82: 
                                                              return self.to_uop(ast)                                                                                                                    #     codegen/lowerer.py            :    83: 

                                                                class IndependentLowerer:                                                                                                                #     codegen/lowerer.py            :    37: 
                                                                  def to_uop(self, x:UOp) -> UOp:                                                                                                        #     codegen/lowerer.py            :    85: 
                                                                    if uop:=self.uop_cache.get(x, None): return uop                                                                                      #     codegen/lowerer.py            :    86: L: {'x': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}    
                                                                    ret = self._to_uop(x)                                                                                                                #     codegen/lowerer.py            :    87: 

                                                                      class IndependentLowerer:                                                                                                          #     codegen/lowerer.py            :    37: 
                                                                        def _to_uop(self, x:UOp) -> UOp:                                                                                                 #     codegen/lowerer.py            :    91: 
                                                                          if x.op in BUFFER_UOPS:                                                                                                        #     codegen/lowerer.py            :    92: 
                                                                      
                                                                          in_uops = tuple(self.to_uop(y) for y in x.src)                                                                                 #     codegen/lowerer.py            :   112: 

                                                                            class IndependentLowerer:                                                                                                    #     codegen/lowerer.py            :    37: 
                                                                              def _to_uop(self, x:UOp) -> UOp:                                                                                           #     codegen/lowerer.py            :    91: 
                                                                                if x.op in BUFFER_UOPS:                                                                                                  # OLD codegen/lowerer.py            :    92: 
                                                                                  idx, valid = x.st_arg.to_indexed_uops(self.ridxs if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL else self.idxs) #     codegen/lowerer.py            :    93: L: {'x': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}    
                                                                                  # TODO: check has_valid in UPat, not here
                                                                                  has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                        #     codegen/lowerer.py            :    95: L: {'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'valid': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                                                  if x.op is UOps.CONST: return valid.where(UOp.const(x.dtype, x.arg), UOp.const(x.dtype, 0))                            #     codegen/lowerer.py            :    96: L: {'has_valid': False}    
                                                                                  buf = x.src[0]                                                                                                         #     codegen/lowerer.py            :    97: 
                                                                                  if x.op is UOps.LOAD:                                                                                                  #     codegen/lowerer.py            :    98: L: {'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                                                  # NOTE: only store the local reduceop in the threads that are actually doing the reduce
                                                                                  store_back = x.src[0].op is UOps.DEFINE_LOCAL and x.src[2].op is UOps.REDUCE_AXIS and \                                #     codegen/lowerer.py            :   102: 
                                                                                    x.src[2].src[0].op is UOps.LOAD and x.src[2].src[0].src[0].op is UOps.DEFINE_LOCAL                                        
                                                                                  # NOTE: If we're storing the reduced value back into each thread, need to zero-out the reduced axes
                                                                                  if store_back: idx, _ = x.st_arg.to_indexed_uops([UOp.const(u.dtype, 0) if i in x.src[2].arg[1] else u for i,u in enumerate(self.idxs)]) #     codegen/lowerer.py            :   105: L: {'store_back': False}    
                                                                                  if x.src[0].op is UOps.DEFINE_GLOBAL or store_back:                                                                    #     codegen/lowerer.py            :   106: 
                                                                                    for oidx, ridx in zip(self.idxs, self.ridxs):                                                                        #     codegen/lowerer.py            :   107: 
                                                                                      if oidx != ridx: valid = valid * oidx.eq(0)                                                                        #     codegen/lowerer.py            :   108: L: {'oidx': UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)), 'ridx': UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),))}    
                                                                                    has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                      #     codegen/lowerer.py            :   109: 
                                                                                  return UOp(UOps.STORE, None, (buf, idx, self.to_uop(x.src[2])) + ((valid,) if has_valid else ()))                      #     codegen/lowerer.py            :   110: 

                                                                                    class IndependentLowerer:                                                                                            #     codegen/lowerer.py            :    37: 
                                                                                      def _to_uop(self, x:UOp) -> UOp:                                                                                   #     codegen/lowerer.py            :    91: 
                                                                                        if x.op in BUFFER_UOPS:                                                                                          # OLD codegen/lowerer.py            :    92: 
                                                                                          idx, valid = x.st_arg.to_indexed_uops(self.ridxs if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL else self.idxs) # OLD codegen/lowerer.py            :    93: 
                                                                                          # TODO: check has_valid in UPat, not here
                                                                                          has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                # OLD codegen/lowerer.py            :    95: 
                                                                                          if x.op is UOps.CONST: return valid.where(UOp.const(x.dtype, x.arg), UOp.const(x.dtype, 0))                    # OLD codegen/lowerer.py            :    96: 
                                                                                          buf = x.src[0]                                                                                                 # OLD codegen/lowerer.py            :    97: 
                                                                                          if x.op is UOps.LOAD:                                                                                          # OLD codegen/lowerer.py            :    98: 
                                                                                            barrier = (UOp(UOps.BARRIER, None, (self.to_uop(x.src[2]),)),) if x.src[0].op is UOps.DEFINE_LOCAL else ()   #     codegen/lowerer.py            :    99: L: {'x': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=())}    
                                                                                            return UOp(UOps.LOAD, x.dtype, (buf, idx) + ((UOp.const(x.dtype, 0), valid) if has_valid else ()) + barrier) #     codegen/lowerer.py            :   100: L: {'barrier': ()}    

                                                                                    class IndependentLowerer:                                                                                            #     codegen/lowerer.py            :    37: 
                                                                                      def to_uop(self, x:UOp) -> UOp:                                                                                    #     codegen/lowerer.py            :    85: 
                                                                                        if uop:=self.uop_cache.get(x, None): return uop                                                                  # OLD codegen/lowerer.py            :    86: 
                                                                                        ret = self._to_uop(x)                                                                                            # OLD codegen/lowerer.py            :    87: 
                                                                                        self.uop_cache[x] = ret                                                                                          #     codegen/lowerer.py            :    88: L: {'ret': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n    x2:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n      UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n         x2,\n        UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n      UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),))}    
                                                                                        return ret                                                                                                       #     codegen/lowerer.py            :    89: 

                                                                                    class IndependentLowerer:                                                                                            #     codegen/lowerer.py            :    37: 
                                                                                      def _to_uop(self, x:UOp) -> UOp:                                                                                   #     codegen/lowerer.py            :    91: 
                                                                                        if x.op in BUFFER_UOPS:                                                                                          # OLD codegen/lowerer.py            :    92: 
                                                                                          idx, valid = x.st_arg.to_indexed_uops(self.ridxs if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL else self.idxs) # OLD codegen/lowerer.py            :    93: 
                                                                                          # TODO: check has_valid in UPat, not here
                                                                                          has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                # OLD codegen/lowerer.py            :    95: 
                                                                                          if x.op is UOps.CONST: return valid.where(UOp.const(x.dtype, x.arg), UOp.const(x.dtype, 0))                    # OLD codegen/lowerer.py            :    96: 
                                                                                          buf = x.src[0]                                                                                                 # OLD codegen/lowerer.py            :    97: 
                                                                                          if x.op is UOps.LOAD:                                                                                          # OLD codegen/lowerer.py            :    98: 
                                                                                            barrier = (UOp(UOps.BARRIER, None, (self.to_uop(x.src[2]),)),) if x.src[0].op is UOps.DEFINE_LOCAL else ()   # OLD codegen/lowerer.py            :    99: 
                                                                                            return UOp(UOps.LOAD, x.dtype, (buf, idx) + ((UOp.const(x.dtype, 0), valid) if has_valid else ()) + barrier) # OLD codegen/lowerer.py            :   100: 
                                                                                          # NOTE: only store the local reduceop in the threads that are actually doing the reduce
                                                                                          store_back = x.src[0].op is UOps.DEFINE_LOCAL and x.src[2].op is UOps.REDUCE_AXIS and \                        # OLD codegen/lowerer.py            :   102: 
                                                                                            x.src[2].src[0].op is UOps.LOAD and x.src[2].src[0].src[0].op is UOps.DEFINE_LOCAL                           # OLD 
                                                                                          # NOTE: If we're storing the reduced value back into each thread, need to zero-out the reduced axes
                                                                                          if store_back: idx, _ = x.st_arg.to_indexed_uops([UOp.const(u.dtype, 0) if i in x.src[2].arg[1] else u for i,u in enumerate(self.idxs)]) # OLD codegen/lowerer.py            :   105: 
                                                                                          if x.src[0].op is UOps.DEFINE_GLOBAL or store_back:                                                            # OLD codegen/lowerer.py            :   106: 
                                                                                            for oidx, ridx in zip(self.idxs, self.ridxs):                                                                # OLD codegen/lowerer.py            :   107: 
                                                                                              if oidx != ridx: valid = valid * oidx.eq(0)                                                                # OLD codegen/lowerer.py            :   108: 
                                                                                            has_valid = valid.op is not UOps.CONST or valid.arg is not True                                              # OLD codegen/lowerer.py            :   109: 
                                                                                          return UOp(UOps.STORE, None, (buf, idx, self.to_uop(x.src[2])) + ((valid,) if has_valid else ()))              # OLD codegen/lowerer.py            :   110: 
                                                                                    
                                                                                        in_uops = tuple(self.to_uop(y) for y in x.src)                                                                   # OLD codegen/lowerer.py            :   112: 
                                                                                        if x.op is UOps.REDUCE_AXIS:                                                                                     #     codegen/lowerer.py            :   113: L: {'x': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)), 'in_uops': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n    x2:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n      UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n         x2,\n        UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n      UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),)), UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n  UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),)))}    
                                                                                        return UOp(x.op, x.dtype, in_uops, x.arg)                                                                        #     codegen/lowerer.py            :   130: 

                                                        def full_graph_rewrite(sink:UOp, opts:Optional[Renderer]=None) -> UOp:                                                                           #     codegen/uopgraph.py           :   494: 
                                                          assert sink.op is UOps.SINK, f"sink isn't sink, it's {sink.op}"                                                                                #     codegen/uopgraph.py           :   496: G: {'linearize_cnt': 0, 'full_graph_rewrite': <function full_graph_rewrite at 0x78973059eb90>, 'linearize_uop': <function linearize_uop at 0x78973059ec20>}    L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>}    
                                                          folder = constant_folder + transcendental_folding(tuple() if TRANSCENDENTAL >= 2 or opts is None else tuple(opts.code_for_op.keys()))          #     codegen/uopgraph.py           :   497: 

                                                            @functools.lru_cache(None)                                                                                                                   #     codegen/uopgraph.py           :   155: 
                                                            def transcendental_folding(ops):                                                                                                                  
                                                              return PatternMatcher([(UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=k), cast(Callable, v))            #     codegen/uopgraph.py           :   156: L: {'ops': (<UnaryOps.SQRT: 6>, <UnaryOps.RECIP: 7>, <BinaryOps.ADD: 1>, <BinaryOps.MAX: 4>, <BinaryOps.IDIV: 3>, <BinaryOps.MUL: 2>, <BinaryOps.MOD: 5>, <BinaryOps.CMPLT: 6>, <BinaryOps.CMPNE: 7>, <BinaryOps.XOR: 8>, <BinaryOps.AND: 12>, <BinaryOps.OR: 11>, <TernaryOps.WHERE: 1>)}    
                                                                                     for k,v in ((UnaryOps.EXP2, xexp2), (UnaryOps.LOG2, xlog2), (UnaryOps.SIN, xsin)) if k not in ops])                      

                                                            class PatternMatcher:                                                                                                                        #     ops.py                        :   267: 
                                                              @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                   #     ops.py                        :   277: 
                                                              def __add__(self, more:PatternMatcher): return PatternMatcher(self.patterns+more.patterns)                                                      

                                                        
                                                          # do graph rewrite
                                                          acc_number = 0                                                                                                                                 #     codegen/uopgraph.py           :   500: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'folder': <tinygrad.ops.PatternMatcher object at 0x78971f0cee90>}    
                                                          sink = graph_rewrite(sink, folder)                                                                                                             #     codegen/uopgraph.py           :   501: 

                                                            # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                  #     dtype.py                      :    31: 
                                                            class PtrDType(DType):                                                                                                                            
                                                              def __hash__(self): return super().__hash__()                                                                                              #     dtype.py                      :    33: 

                                                            class PatternMatcher:                                                                                                                        #     ops.py                        :   267: 
                                                              def rewrite(self, uop:UOp) -> Optional[UOp]:                                                                                               #     ops.py                        :   279: 
                                                                ler = set([(u.op, u.arg) for u in uop.src] + [(u.op, None) for u in uop.src])                                                            # OLD ops.py                        :   280: 
                                                                for p,fxn,early_reject in itertools.chain(self.pdict[(uop.op, uop.arg)], self.pdict[(uop.op, None)]):                                    # OLD ops.py                        :   281: 
                                                                  if not early_reject.issubset(ler): continue                                                                                            #     ops.py                        :   282: L: {'self': <tinygrad.ops.PatternMatcher object at 0x78971f0cee90>, 'uop': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n  UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n  UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)), 'ler': {(<UOps.CONST: 10>, None), (<UOps.RANGE: 26>, None), (<UOps.CONST: 10>, 1), (<UOps.RANGE: 26>, (0, False))}, 'p': UPat((UOps.ALU), BinaryOps.MUL, name='x', dtype={dtypes.bool}, allow_any_len=True, src=(None)), 'fxn': <function <lambda> at 0x78973059e560>, 'early_reject': set()}    
                                                                  if (matches := _match(uop, p, {})) and (ret:=fxn(**matches[0])) is not None: return ret # NOTE: if it returns None, we keep trying to match #     ops.py                        :   283: 

                                                                    def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:                                                         #     ops.py                        :   253: 
                                                                      if (pat.name is not None and store.setdefault(pat.name, uop) is not uop) or \                                                      #     ops.py                        :   254: L: {'pat': UPat((UOps.ALU), BinaryOps.MUL, name='x', dtype={dtypes.bool}, allow_any_len=True, src=(None)), 'store': {}}    
                                                                         (pat.dtype is not None and uop.dtype not in pat.dtype) or \                                                                          
                                                                         (pat.arg is not None and pat.arg != uop.arg) or \                                                                                    
                                                                         (pat.op is not None and uop.op not in pat.op): return []                                                                             

                                                                    def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:                                                         #     ops.py                        :   253: 
                                                                      if (pat.name is not None and store.setdefault(pat.name, uop) is not uop) or \                                                      # OLD ops.py                        :   254: 
                                                                         (pat.dtype is not None and uop.dtype not in pat.dtype) or \                                                                     # OLD 
                                                                         (pat.arg is not None and pat.arg != uop.arg) or \                                                                               # OLD 
                                                                         (pat.op is not None and uop.op not in pat.op): return []                                                                        # OLD 
                                                                      if pat.src is None: return [store]                                                                                                 #     ops.py                        :   258: L: {'pat': UPat((UOps.ALU), BinaryOps.MUL, name=None, dtype=None, allow_any_len=False, src=[\n  UPat(None, None, name='x', dtype=None, allow_any_len=True, src=(None)),\n  UPat((UOps.CONST), 1, name=None, dtype=None, allow_any_len=True, src=(None)),]), 'store': {}}    
                                                                      res: List[Dict[str, UOp]] = []                                                                                                     #     ops.py                        :   259: 
                                                                      for vp in pat.src:                                                                                                                 #     ops.py                        :   260: L: {'res': []}    
                                                                        if pat.allowed_len != 0 and len(uop.src) != pat.allowed_len: return []                                                           #     ops.py                        :   261: L: {'vp': (UPat(None, None, name='x', dtype=None, allow_any_len=True, src=(None)), UPat((UOps.CONST), 1, name=None, dtype=None, allow_any_len=True, src=(None)))}    
                                                                        new_stores = [store.copy()]                                                                                                      #     ops.py                        :   262: 
                                                                        for uu, vv in zip(uop.src, vp): new_stores = [rstore for nstore in new_stores for rstore in _match(uu, vv, nstore)]              #     ops.py                        :   263: L: {'new_stores': [{}]}    
                                                                        res.extend(new_stores)                                                                                                           #     ops.py                        :   264: L: {'new_stores': [{'x': UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),))}], 'uu': UOp(UOps.CONST, dtypes.pyint, arg=1, src=()), 'vv': UPat((UOps.CONST), 1, name=None, dtype=None, allow_any_len=True, src=(None))}    
                                                                      return res                                                                                                                         #     ops.py                        :   265: L: {'vp': (UPat((UOps.CONST), 1, name=None, dtype=None, allow_any_len=True, src=(None)), UPat(None, None, name='x', dtype=None, allow_any_len=True, src=(None))), 'new_stores': [], 'vv': UPat(None, None, name='x', dtype=None, allow_any_len=True, src=(None))}    

                                                            @dataclass(frozen=True, eq=False)                                                                                                            #     ops.py                        :    58: 
                                                            class UOp:                                                                                                                                        
                                                              @property                                                                                                                                  #     ops.py                        :   157: 
                                                              def vmin(self) -> UOp: return x if (x:=self._min_max[0]) is not None and not math.isnan(x.arg) else self.sconst(dtypes.min(cast(DType, self.dtype)))      

                                                                @dataclass(frozen=True, eq=False)                                                                                                        #     ops.py                        :    58: 
                                                                class UOp:                                                                                                                                    
                                                                  @functools.cached_property                                                                                                             #     ops.py                        :   161: 
                                                                  def _min_max(self) -> Tuple[Optional[UOp], Optional[UOp]]:                                                                                  
                                                                    # NOTE: returned UOp is assumed to be CONST
                                                                    if self.op is UOps.DEFINE_VAR and self.src: return self.src[0], self.src[1] if isinstance(self.src[1].arg, int) else None            #     ops.py                        :   163: L: {'self': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))}    
                                                                    if self.op is UOps.RANGE: return self.src[0].vmin, (self.src[1]-1).vmax                                                              #     ops.py                        :   164: 
                                                                    # TODO: UOps.SPECIAL is UOps.DEFINE_VAR
                                                                    if self.op is UOps.SPECIAL: return self.const(0), self.const(self.arg[1]-1) if isinstance(self.arg[1], int) else None                #     ops.py                        :   166: 
                                                                    if self.op is UOps.CONST: return self, self                                                                                          #     ops.py                        :   167: 
                                                                    if self.op is UOps.ALU and cast(DType, self.dtype).count == 1:                                                                       #     ops.py                        :   168: 
                                                                      s0,s1 = [cast(UOp, self.src[i] if i < len(self.src) else None) for i in range(2)]                                                  #     ops.py                        :   169: 
                                                                      if self.arg is BinaryOps.ADD: return self.sconst(s0.vmin.arg+s1.vmin.arg), self.sconst(s0.vmax.arg+s1.vmax.arg)                    #     ops.py                        :   170: L: {'s0': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),)), 's1': UOp(UOps.CONST, dtypes.int, arg=2, src=())}    

                                                                        @dataclass(frozen=True, eq=False)                                                                                                #     ops.py                        :    58: 
                                                                        class UOp:                                                                                                                            
                                                                          @functools.cached_property                                                                                                     #     ops.py                        :   161: 
                                                                          def _min_max(self) -> Tuple[Optional[UOp], Optional[UOp]]:                                                                          
                                                                            # NOTE: returned UOp is assumed to be CONST
                                                                            if self.op is UOps.DEFINE_VAR and self.src: return self.src[0], self.src[1] if isinstance(self.src[1].arg, int) else None    # OLD ops.py                        :   163: 
                                                                            if self.op is UOps.RANGE: return self.src[0].vmin, (self.src[1]-1).vmax                                                      # OLD ops.py                        :   164: 
                                                                            # TODO: UOps.SPECIAL is UOps.DEFINE_VAR
                                                                            if self.op is UOps.SPECIAL: return self.const(0), self.const(self.arg[1]-1) if isinstance(self.arg[1], int) else None        # OLD ops.py                        :   166: 
                                                                            if self.op is UOps.CONST: return self, self                                                                                  # OLD ops.py                        :   167: 
                                                                            if self.op is UOps.ALU and cast(DType, self.dtype).count == 1:                                                               # OLD ops.py                        :   168: 
                                                                              s0,s1 = [cast(UOp, self.src[i] if i < len(self.src) else None) for i in range(2)]                                          # OLD ops.py                        :   169: 
                                                                              if self.arg is BinaryOps.ADD: return self.sconst(s0.vmin.arg+s1.vmin.arg), self.sconst(s0.vmax.arg+s1.vmax.arg)            # OLD ops.py                        :   170: 
                                                                            return None, None                                                                                                            #     ops.py                        :   184: L: {'self': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),))}    

                                                                        class dtypes:                                                                                                                    #     dtype.py                      :    38: 
                                                                          @staticmethod                                                                                                                  #     dtype.py                      :    59: 
                                                                          def min(dtype:DType):                                                                                                               
                                                                            if dtypes.is_int(dtype): return 0 if dtypes.is_unsigned(dtype) else -2**(dtype.itemsize*8-1)                                 #     dtype.py                      :    60: L: {'dtype': dtypes.int}    

                                                                        @dataclass(frozen=True, eq=False)                                                                                                #     ops.py                        :    58: 
                                                                        class UOp:                                                                                                                            
                                                                          def sconst(self:Union[UOp, DType, None], b:ConstType|Variable):                                                                #     ops.py                        :   111: 
                                                                            return UOp._const(cast(DType, self.dtype if isinstance(self, UOp) else self).scalar() if self is not None else self, b)      #     ops.py                        :   112: L: {'b': -2147483648}    

                                                                        @dataclass(frozen=True, eq=False)                                                                                                #     ops.py                        :    58: 
                                                                        class UOp:                                                                                                                            
                                                                          @property                                                                                                                      #     ops.py                        :   159: 
                                                                          def vmax(self) -> UOp: return x if (x:=self._min_max[1]) is not None and not math.isnan(x.arg) else self.sconst(dtypes.max(cast(DType, self.dtype)))      

                                                                            class dtypes:                                                                                                                #     dtype.py                      :    38: 
                                                                              @staticmethod                                                                                                              #     dtype.py                      :    63: 
                                                                              def max(dtype:DType):                                                                                                           
                                                                                if dtypes.is_int(dtype): return (2**(dtype.itemsize*8-(0 if dtypes.is_unsigned(dtype) else 1)))-1                        #     dtype.py                      :    64: 

                                                        
                                                          # rewrite pyint to int32
                                                          sink = graph_rewrite(sink, no_pyint)                                                                                                           #     codegen/uopgraph.py           :   504: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))}    
                                                        
                                                          # expand
                                                          linearize_cnt += 1                                                                                                                             #     codegen/uopgraph.py           :   507: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))}    
                                                          if linearize_cnt != getenv("DEBUG_EXPAND", 0):                                                                                                 #     codegen/uopgraph.py           :   508: G: {'linearize_cnt': 1}    
                                                            sink = graph_rewrite(sink, folder+(expander+float4_folding if opts is not None and opts.supports_float4 else expander))                      #     codegen/uopgraph.py           :   509: 

                                                              def do_expand(root:UOp):                                                                                                                   #     codegen/uopgraph.py           :   374: 
                                                                expands = [x for x in root.src if x.op is UOps.EXPAND]                                                                                   #     codegen/uopgraph.py           :   375: L: {'root': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                                if len(expands) == 0: return None                                                                                                        #     codegen/uopgraph.py           :   376: L: {'expands': []}    

                                                              def create_gate(root:UOp) -> Optional[UOp]:                                                                                                #     codegen/uopgraph.py           :   431: 
                                                                return None if len(root.src) == 3 or (ret:=_gate_srcs(root, root.src[3])) is root else ret                                               #     codegen/uopgraph.py           :   437: L: {'root': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), '_gate_srcs': <functools._lru_cache_wrapper object at 0x78971f1119b0>}    

                                                              def fold_expanded(ex, buf):                                                                                                                #     codegen/uopgraph.py           :    13: 
                                                                if buf.dtype != PtrDType(dtypes.float) and buf.dtype != PtrDType(dtypes.half) and not isinstance(buf.dtype, ImageDType): return None     #     codegen/uopgraph.py           :    14: L: {'ex': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    

                                                                  # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                            #     dtype.py                      :    31: 
                                                                  class PtrDType(DType):                                                                                                                      
                                                                    def __ne__(self, dt): return not (self == dt)                                                                                        #     dtype.py                      :    35: 

                                                                      # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                        #     dtype.py                      :    31: 
                                                                      class PtrDType(DType):                                                                                                                  
                                                                        def __eq__(self, dt): return self.priority==dt.priority and self.itemsize==dt.itemsize and self.name==dt.name and self.count==dt.count #     dtype.py                      :    34: 

                                                            sink = graph_rewrite(sink, folder+reducer)                                                                                                   #     codegen/uopgraph.py           :   510: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'folder': <tinygrad.ops.PatternMatcher object at 0x78971f0cee90>}    

                                                              def fix_unfoldable_image_load(load:UOp, buf:UOp):                                                                                          #     codegen/uopgraph.py           :    71: 
                                                                if not isinstance(buf.dtype, ImageDType) or cast(DType, load.src[1].dtype).count == 2: return None                                       #     codegen/uopgraph.py           :    72: L: {'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), 'load': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    

                                                              def no_vectorized_alu(alu):                                                                                                                #     codegen/uopgraph.py           :   425: 
                                                                if alu.dtype.count == 1: return None                                                                                                     #     codegen/uopgraph.py           :   426: L: {'alu': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))}    

                                                              def delete_redundant_gates(root:UOp) -> Optional[UOp]:                                                                                     #     codegen/uopgraph.py           :   459: 
                                                                if len(root.src) == 3 or (gate:=find_gate(root)) is None or gate.src[0] is not root.src[3]: return None                                  #     codegen/uopgraph.py           :   464: L: {'root': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'find_gate': <functools._lru_cache_wrapper object at 0x78971f1124b0>}    

                                                        
                                                          # for PTX only
                                                          if opts is not None and opts.extra_matcher is not None: sink = graph_rewrite(sink, folder+opts.extra_matcher)                                  #     codegen/uopgraph.py           :   513: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'folder': <tinygrad.ops.PatternMatcher object at 0x78971f0cee90>}    
                                                          return sink                                                                                                                                    #     codegen/uopgraph.py           :   514: 

                                                        def linearize_uop(sink:UOp, skip_check:bool=False) -> List[UOp]:                                                                                 #     codegen/uopgraph.py           :   516: 
                                                          assert sink.op is UOps.SINK, f"sink isn't sink, it's {sink.op}"                                                                                #     codegen/uopgraph.py           :   517: L: {'skip_check': False}    
                                                          # filter nodes that don't link to a sink
                                                          # BFS toposort
                                                          children: Dict[UOp, List[UOp]] = {}                                                                                                            #     codegen/uopgraph.py           :   520: 
                                                          range_srcs: Dict[UOp, Dict[UOp, None]] = {}                                                                                                    #     codegen/uopgraph.py           :   521: L: {'children': {}}    
                                                          in_degree: Dict[UOp, int] = {}                                                                                                                 #     codegen/uopgraph.py           :   522: L: {'range_srcs': {}}    
                                                          get_children_dfs(sink, children, range_srcs, in_degree)                                                                                        #     codegen/uopgraph.py           :   523: L: {'in_degree': {}}    

                                                            def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                       #     codegen/uopgraph.py           :   482: 
                                                              if u in children: return srcs[u]                                                                                                           #     codegen/uopgraph.py           :   483: L: {'u': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'srcs': {}}    
                                                              srcs[u] = {}                                                                                                                               #     codegen/uopgraph.py           :   484: 
                                                              children[u] = []                                                                                                                           #     codegen/uopgraph.py           :   485: 
                                                              for x in u.src:                                                                                                                            #     codegen/uopgraph.py           :   486: 
                                                                srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                           #     codegen/uopgraph.py           :   487: L: {'x': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}    

                                                                  def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                 #     codegen/uopgraph.py           :   482: 
                                                                    if u in children: return srcs[u]                                                                                                     # OLD codegen/uopgraph.py           :   483: 
                                                                    srcs[u] = {}                                                                                                                         # OLD codegen/uopgraph.py           :   484: 
                                                                    children[u] = []                                                                                                                     # OLD codegen/uopgraph.py           :   485: 
                                                                    for x in u.src:                                                                                                                      # OLD codegen/uopgraph.py           :   486: 
                                                                      srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                     # OLD codegen/uopgraph.py           :   487: 
                                                                    in_degree[u] = len(u.src)                                                                                                            #     codegen/uopgraph.py           :   490: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                                    return srcs[u]                                                                                                                       #     codegen/uopgraph.py           :   491: 

                                                                  def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                 #     codegen/uopgraph.py           :   482: 
                                                                    if u in children: return srcs[u]                                                                                                     # OLD codegen/uopgraph.py           :   483: 
                                                                    srcs[u] = {}                                                                                                                         # OLD codegen/uopgraph.py           :   484: 
                                                                    children[u] = []                                                                                                                     # OLD codegen/uopgraph.py           :   485: 
                                                                    for x in u.src:                                                                                                                      # OLD codegen/uopgraph.py           :   486: 
                                                                      srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                     # OLD codegen/uopgraph.py           :   487: 
                                                                      if x.op is UOps.RANGE and x.arg[1]: srcs[u][x] = None                                                                              #     codegen/uopgraph.py           :   488: L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'x': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                                      children[x].append(u)                                                                                                              #     codegen/uopgraph.py           :   489: 

                                                        
                                                          # scope children impact the toposort and END* insertion
                                                          scope_children = {p:get_recursive_children(p, END_FOR_UOP[p.op][0]) for p in reversed(in_degree) if p.op in END_FOR_UOP}                       #     codegen/uopgraph.py           :   531: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'skip_check': False, 'range_srcs': {UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): {}, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): {}, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): {}, UOp(UOps.CONST, dtypes.int, arg=0, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=3, src=()): {}, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): {}, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=2, src=()): {}}, 'get_recursive_children': <functools._lru_cache_wrapper object at 0x78971f112610>}    

                                                            def linearize_uop(sink:UOp, skip_check:bool=False) -> List[UOp]:                                                                             #     codegen/uopgraph.py           :   516: 
                                                              @functools.lru_cache(None)                                                                                                                 #     codegen/uopgraph.py           :   526: 
                                                              def get_recursive_children(x:UOp, end:UOps, include_self=False) -> Set[UOp]:                                                                    
                                                                if x.op is UOps.SINK: return set()                                                                                                       #     codegen/uopgraph.py           :   527: L: {'include_self': False, 'end': <UOps.PHI: 23>, 'x': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))}    
                                                                return set.union({x} if include_self else set(), *([get_recursive_children(u, end, True) for u in children[x] if x.op is not end]))      #     codegen/uopgraph.py           :   528: 

                                                          range_phi = {r:[p for p in scope_children[r] if p.op is UOps.PHI] for r in scope_children if r.op is UOps.RANGE}                               #     codegen/uopgraph.py           :   532: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'skip_check': False, 'range_srcs': {UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): {}, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): {}, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): {}, UOp(UOps.CONST, dtypes.int, arg=0, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=3, src=()): {}, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): {}, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=2, src=()): {}}, 'in_degree': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 0, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 2, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 0, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 2, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 0, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 2, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): 3, UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): 1}, 'scope_children': {UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): {UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}}}    
                                                        
                                                          queue:List[Tuple[int, UOp]] = []                                                                                                               #     codegen/uopgraph.py           :   534: L: {'range_phi': {UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): []}}    
                                                        
                                                          for u in children:                                                                                                                             #     codegen/uopgraph.py           :   547: L: {'queue': [], 'push': <function linearize_uop.<locals>.push at 0x78971f11bbe0>}    
                                                            if in_degree[u] == 0: push(u)                                                                                                                #     codegen/uopgraph.py           :   548: L: {'u': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))}    

                                                              def linearize_uop(sink:UOp, skip_check:bool=False) -> List[UOp]:                                                                           #     codegen/uopgraph.py           :   516: 
                                                                def push(u:UOp):                                                                                                                         #     codegen/uopgraph.py           :   535: 
                                                                  priority = 0                                                                                                                           #     codegen/uopgraph.py           :   536: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                                  # prefer ranges that depend on the least number of independent ranges
                                                                  if u.op is UOps.RANGE and u.arg[1]:                                                                                                    #     codegen/uopgraph.py           :   538: L: {'priority': 0}    
                                                                  # prefer uops that are loop children
                                                                  else:                                                                                                                                  #     codegen/uopgraph.py           :   543: 
                                                                    priority -= sum([(l.arg[0]+1) + 1000*l.arg[1] for l,ss in scope_children.items() if l.op is UOps.RANGE and u in ss])                 #     codegen/uopgraph.py           :   544: 
                                                                  heapq.heappush(queue, (priority, u))                                                                                                   #     codegen/uopgraph.py           :   545: 

                                                              @dataclass(frozen=True, eq=False)                                                                                                          #     ops.py                        :    58: 
                                                              class UOp:                                                                                                                                      
                                                                def __lt__(self, x:UOp): return self.cmp_tuple < x.cmp_tuple                                                                             #     ops.py                        :    71: 

                                                                  @dataclass(frozen=True, eq=False)                                                                                                      #     ops.py                        :    58: 
                                                                  class UOp:                                                                                                                                  
                                                                    @functools.cached_property                                                                                                           #     ops.py                        :    67: 
                                                                    def cmp_tuple(self) -> Tuple[int, Any, Optional[DType], Tuple[UOp, ...]]:                                                                 
                                                                      # NOTE: this sort of DEFINE_VAR shouldn't have to be here. only for PTX
                                                                      return (self.op.value, (self.arg if self.op is not UOps.DEFINE_VAR else self.arg.expr) if self.op is not UOps.ALU else \           #     ops.py                        :    69: L: {'self': UOp(UOps.CONST, dtypes.int, arg=0, src=())}    
                                                                              self.arg.value, self.dtype, self.src)                                                                                           

                                                        
                                                          scope_end: Dict[UOp, UOp] = {}                                                                                                                 #     codegen/uopgraph.py           :   550: L: {'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'skip_check': False, 'children': {UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): [], UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): [UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))], UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))], UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], UOp(UOps.CONST, dtypes.int, arg=0, src=()): [UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))], UOp(UOps.CONST, dtypes.int, arg=3, src=()): [UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))], UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))], UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): [UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))], UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): [UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], UOp(UOps.CONST, dtypes.int, arg=2, src=()): [UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))]}, 'in_degree': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 0, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 2, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 0, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 2, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 0, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 2, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): 3, UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): 1}, 'get_recursive_children': <functools._lru_cache_wrapper object at 0x78971f112610>, 'push': <function linearize_uop.<locals>.push at 0x78971f11bbe0>, 'u': UOp(UOps.CONST, dtypes.int, arg=2, src=())}    
                                                          _uops: List[UOp] = []                                                                                                                          #     codegen/uopgraph.py           :   551: L: {'scope_end': {}}    
                                                          while queue:                                                                                                                                   #     codegen/uopgraph.py           :   552: L: {'_uops': []}    
                                                            p,x = heapq.heappop(queue)                                                                                                                   #     codegen/uopgraph.py           :   553: 
                                                            if DEBUG >= 7: print(f"{p:5d}",x)                                                                                                            #     codegen/uopgraph.py           :   554: L: {'p': 0, 'x': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                            if x in scope_children: scope_end[x] = x                                                                                                     #     codegen/uopgraph.py           :   555: 
                                                            if x.op is UOps.DEFINE_ACC:                                                                                                                  #     codegen/uopgraph.py           :   556: 
                                                            else: _uops.append(x)                                                                                                                        #     codegen/uopgraph.py           :   559: 
                                                            for u, ss in scope_children.items():                                                                                                         #     codegen/uopgraph.py           :   560: 
                                                              if x in ss:                                                                                                                                #     codegen/uopgraph.py           :   561: L: {'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), 'ss': {UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}}    
                                                            for u in children[x]:                                                                                                                        #     codegen/uopgraph.py           :   564: 
                                                              in_degree[u] -= 1                                                                                                                          #     codegen/uopgraph.py           :   565: L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}    
                                                              if in_degree[u] == 0: push(u)                                                                                                              #     codegen/uopgraph.py           :   566: 
                                                          #while queue: # branches differently:                                                                                                          # OLD codegen/uopgraph.py           :   552: 
                                                            p,x = heapq.heappop(queue)                                                                                                                   # OLD codegen/uopgraph.py           :   553: 
                                                            if DEBUG >= 7: print(f"{p:5d}",x)                                                                                                            # OLD codegen/uopgraph.py           :   554: 
                                                            if x in scope_children: scope_end[x] = x                                                                                                     # OLD codegen/uopgraph.py           :   555: 
                                                            if x.op is UOps.DEFINE_ACC:                                                                                                                  # OLD codegen/uopgraph.py           :   556: 
                                                            else: _uops.append(x)                                                                                                                        # OLD codegen/uopgraph.py           :   559: 
                                                            for u, ss in scope_children.items():                                                                                                         # OLD codegen/uopgraph.py           :   560: 
                                                              if x in ss:                                                                                                                                # OLD codegen/uopgraph.py           :   561: 
                                                                ss.remove(x)                                                                                                                             #     codegen/uopgraph.py           :   562: L: {'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), 'p': -1, 'x': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                                if len(ss) == 0: scope_end[u] = x                                                                                                        #     codegen/uopgraph.py           :   563: 
                                                        
                                                          # end scopes in toposort order
                                                          for u, x in scope_end.items(): _uops.insert(_uops.index(x)+1, UOp(END_FOR_UOP[u.op][1], None, (u,)))                                           #     codegen/uopgraph.py           :   569: L: {'p': 0, 'x': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))}    
                                                        
                                                          # sanity checks (NOTE: these can cause things to be skipped in BEAM)
                                                          if not skip_check:                                                                                                                             #     codegen/uopgraph.py           :   572: L: {'x': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}    
                                                            bad_ops = dedup([x.op for x in _uops if x.op in {UOps.EXPAND, UOps.CONTRACT, UOps.REDUCE, UOps.REDUCE_AXIS, UOps.SHAPETRACKER}])             #     codegen/uopgraph.py           :   573: 
                                                            try:                                                                                                                                         #     codegen/uopgraph.py           :   574: L: {'bad_ops': []}    
                                                              type_verify(_uops)                                                                                                                         #     codegen/uopgraph.py           :   575: 
                                                              assert _uops[-1].op is UOps.SINK, f"didn't end with SINK, ended with {_uops[-1]}"                                                          #     codegen/uopgraph.py           :   576: 
                                                              assert len(bad_ops) == 0, f"bad UOps left in list: {bad_ops}"                                                                              #     codegen/uopgraph.py           :   577: 
                                                              all_stores = [x.src[0:2]+x.src[3:] for x in _uops if x.op is UOps.STORE and not _islocalbuf(x.src[0])]                                     #     codegen/uopgraph.py           :   582: L: {'_islocalbuf': <function linearize_uop.<locals>._islocalbuf at 0x78971f0d3880>}    

                                                                def linearize_uop(sink:UOp, skip_check:bool=False) -> List[UOp]:                                                                         #     codegen/uopgraph.py           :   516: 
                                                                      # TODO: this should be enabled, and the valid clause should be removed                                                             #     codegen/uopgraph.py           :   581: 
                                                                      # NOTE: multiple identical stores to DEFINE_LOCAL is okay                                                                               
                                                                      # NOTE: for PTX you have to propogate through some the calculations to determine if it is a store to DEFINE_LOCAL                       
                                                                      def _islocalbuf(u: UOp): return u.op is UOps.DEFINE_LOCAL or any(_islocalbuf(x) for x in u.src if u.op in [UOps.ALU, UOps.CAST])        

                                                              assert len(all_stores) == len(dedup(all_stores)), "repeated stores in uops"                                                                #     codegen/uopgraph.py           :   583: L: {'all_stores': [(UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)))]}    
                                                        
                                                          # strip the SINK
                                                          return _uops[:-1]                                                                                                                              #     codegen/uopgraph.py           :   592: 

                                                      if DEBUG >= 5: print_uops(self.uops)                                                                                                               #     codegen/kernel.py             :   749: 
                                                      if getenv("GRAPHUOPS"):                                                                                                                            #     codegen/kernel.py             :   750: 
                                                      return self                                                                                                                                        #     codegen/kernel.py             :   753: 

                                                src = self.opts.render(name:=to_function_name(ansiname:=(name_override if name_override is not None else self.name)), self.uops)                         #     codegen/kernel.py             :   757: 

                                                  class Kernel:                                                                                                                                          #     codegen/kernel.py             :    54: 
                                                    @functools.cached_property                                                                                                                           #     codegen/kernel.py             :   621: 
                                                    def name(self) -> str:                                                                                                                                    
                                                      # kernel name (before late upcast)
                                                      name = ("r" if self.reduceop else ("C" if all(x.op in BUFFER_UOPS for x in self.ast.parents) else "E")) + \                                        #     codegen/kernel.py             :   623: 
                                                                   (f"{len(self.ast.src)}_" if len(self.ast.src) > 1 else "_") + \                                                                            
                                                                   colored('_', 'BLACK').join([colored(str(x), c) for x,c in zip(self.full_shape, self.colors())])                                            

                                                        class Kernel:                                                                                                                                    #     codegen/kernel.py             :    54: 
                                                          @property                                                                                                                                      #     codegen/kernel.py             :   150: 
                                                          def reduceop(self) -> Optional[UOp]: return self.reduceops[0] if len(self.reduceops) > 0 else None                                                  

                                                        class Kernel:                                                                                                                                    #     codegen/kernel.py             :    54: 
                                                          # there's eight chunks of the shape                                                                                                            #     codegen/kernel.py             :   181: 
                                                          # blue   -- global dims                                                                                                                             
                                                          # cyan   -- local dims (warp ones first)                                                                                                            
                                                          #  *** self.first_reduce                                                                                                                            
                                                          # green  -- reduce-local dims                                                                                                                       
                                                          # white  -- reduce-late upcasted dim (self.upcast_in_mid_reduce_axes)                                                                               
                                                          # red    -- reduce loops                                                                                                                            
                                                          #  *** self.upcasted                                                                                                                                
                                                          # purple -- reduce upcasted                                                                                                                         
                                                          # yellow -- normal upcasted dimensions                                                                                                              
                                                          def colors(self) -> List[str]:                                                                                                                      
                                                            # first non local non reduce dims are global (blue)
                                                            colors = ["blue"] * self.global_dims if not self.dont_use_locals else ["BLUE"] * self.global_dims                                            #     codegen/kernel.py             :   183: 

                                                              class Kernel:                                                                                                                              #     codegen/kernel.py             :    54: 
                                                                @property                                                                                                                                #     codegen/kernel.py             :   169: 
                                                                def global_dims(self) -> int: return self.first_reduce-self.local_dims                                                                        

                                                            # after global are local_dims; warp ones used in tensor cores must be closest to first_reduce (cyan)
                                                            colors += ["cyan"] * self.local_dims                                                                                                         #     codegen/kernel.py             :   185: L: {'colors': ['blue']}    
                                                            # between first_reduce and first_reduce + group_for_reduces, they are either upcast mid reduce (white), or late upcasted (green)
                                                            colors += ["white" if i in self.upcast_in_mid_reduce_axes else "green" for i in range(self.first_reduce, self.first_reduce + self.group_for_reduces)]  # noqa: E501 #     codegen/kernel.py             :   187: 
                                                            # between first_reduce + group_for_reduces and upcasted, they are reduce (red)
                                                            colors += ["red"] * (self.first_upcast - (self.first_reduce + self.group_for_reduces))                                                       #     codegen/kernel.py             :   189: 
                                                            # upcasted dimensions are reduce (magenta) or normal (yellow)
                                                            colors += ["magenta" if self.full_shape[i] != self.sts[0].shape[i] else "yellow" for i in range(self.first_upcast, self.shape_len)]          #     codegen/kernel.py             :   191: 
                                                            assert len(colors) == self.shape_len, "colors size mismatch"                                                                                 #     codegen/kernel.py             :   192: 
                                                            return colors                                                                                                                                #     codegen/kernel.py             :   193: 

                                                  
                                                      # name the function something unique
                                                      Kernel.kernel_cnt[(function_name := to_function_name(name))] += 1                                                                                  #     codegen/kernel.py             :   628: L: {'name': 'E_\x1b[34m3\x1b[0m'}    

                                                        @functools.lru_cache(maxsize=None)                                                                                                               #     helpers.py                    :    78: 
                                                        def to_function_name(s:str): return ''.join([c if c in (string.ascii_letters+string.digits+'_') else f'{ord(c):02X}' for c in ansistrip(s)])          

                                                          def ansistrip(s:str): return re.sub('\x1b\\[(K|.*?m)', '', s)                                                                                  #     helpers.py                    :    31: 

                                                      suffix = f"{'n'+str(Kernel.kernel_cnt[function_name]-1)}" if Kernel.kernel_cnt[function_name] > 1 else ""                                          #     codegen/kernel.py             :   629: L: {'function_name': 'E_3'}    
                                                      return name+colored(suffix, 'BLACK')                                                                                                               #     codegen/kernel.py             :   630: L: {'suffix': ''}    

                                                  class CStyleLanguage(Renderer):                                                                                                                        #     renderer/cstyle.py            :     9: 
                                                    def render(self, name:str, uops:List[UOp]) -> str:                                                                                                   #     renderer/cstyle.py            :    96: 
                                                      kernel = []                                                                                                                                        #     renderer/cstyle.py            :    97: G: {'AMDRenderer': <class 'tinygrad.renderer.cstyle.AMDRenderer'>, 'NVRenderer': <class 'tinygrad.renderer.cstyle.NVRenderer'>, 'HIPRenderer': <class 'tinygrad.renderer.cstyle.HIPRenderer'>}    L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'self': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>}    
                                                      bufs: Dict[UOp, Tuple[str, Tuple[DType, bool]]] = {}                                                                                               #     renderer/cstyle.py            :    98: L: {'kernel': []}    
                                                      depth = 1                                                                                                                                          #     renderer/cstyle.py            :    99: L: {'bufs': {}}    
                                                  
                                                      c: DefaultDict[str, int] = defaultdict(int)                                                                                                        #     renderer/cstyle.py            :   102: L: {'depth': 1, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>}    
                                                      r: Dict[UOp, str] = {}                                                                                                                             #     renderer/cstyle.py            :   103: L: {'c': defaultdict(<class 'int'>, {})}    
                                                  
                                                      child_count = Counter(v for ru in uops for v in ru.src)                                                                                            #     renderer/cstyle.py            :   112: L: {'r': {}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>}    
                                                  
                                                      seen_vars = set()                                                                                                                                  #     renderer/cstyle.py            :   114: L: {'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1})}    
                                                      for u in uops:                                                                                                                                     #     renderer/cstyle.py            :   115: L: {'seen_vars': set()}    
                                                        uop,dtype,src,args = u.op,u.dtype,u.src,u.arg                                                                                                    #     renderer/cstyle.py            :   116: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                        # these four uops don't have output dtypes
                                                        if uop is UOps.IF:                                                                                                                               #     renderer/cstyle.py            :   118: L: {'uop': <UOps.DEFINE_GLOBAL: 6>, 'dtype': PtrDType(dtypes.int), 'src': (), 'args': 0}    
                                                        elif uop is UOps.BARRIER: kk(self.barrier)                                                                                                       #     renderer/cstyle.py            :   121: 
                                                        elif uop in {UOps.ENDRANGE, UOps.ENDIF}:                                                                                                         #     renderer/cstyle.py            :   122: 
                                                        elif uop is UOps.STORE:                                                                                                                          #     renderer/cstyle.py            :   125: 
                                                        else:                                                                                                                                            #     renderer/cstyle.py            :   131: 
                                                          assert dtype is not None, f"None dtype for uop {uop}"                                                                                          #     renderer/cstyle.py            :   132: 
                                                          if uop is UOps.RANGE:                                                                                                                          #     renderer/cstyle.py            :   133: 
                                                          elif uop is UOps.ALU:                                                                                                                          #     renderer/cstyle.py            :   136: 
                                                          elif uop is UOps.SPECIAL:                                                                                                                      #     renderer/cstyle.py            :   146: 
                                                          elif uop is UOps.DEFINE_VAR:                                                                                                                   #     renderer/cstyle.py            :   149: 
                                                          elif uop is UOps.LOAD:                                                                                                                         #     renderer/cstyle.py            :   154: 
                                                          elif uop is UOps.PHI:                                                                                                                          #     renderer/cstyle.py            :   159: 
                                                          elif uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}:                                                                                         #     renderer/cstyle.py            :   162: 
                                                          elif uop is UOps.DEFINE_LOCAL:                                                                                                                 #     renderer/cstyle.py            :   172: 
                                                          elif uop is UOps.DEFINE_GLOBAL:                                                                                                                #     renderer/cstyle.py            :   175: 
                                                            bufs[u] = (nm:=f"data{args}", (dtype, False))                                                                                                #     renderer/cstyle.py            :   176: 
                                                            r[u] = nm                                                                                                                                    #     renderer/cstyle.py            :   177: L: {'nm': 'data0'}    
                                                          elif uop is UOps.WMMA: kk(f"{self.render_dtype(dtype)} {ssa('wmma',u)} = __{args[0]}({r[src[0]]}, {r[src[1]]}, {r[src[2]]});")                 #     renderer/cstyle.py            :   178: L: {'u': UOp(UOps.CONST, dtypes.int, arg=0, src=()), 'uop': <UOps.CONST: 10>, 'dtype': dtypes.int, 'nm': 'data1'}    
                                                          elif uop is UOps.DEFINE_ACC: kk(f"{self.render_dtype(dtype)} {ssa('acc',u)} = {r[src[0]]};")                                                   #     renderer/cstyle.py            :   179: 
                                                          elif uop is UOps.CONST: r[u] = self.render_const(args, dtype) if args >= 0 else f"({self.render_const(args, dtype)})"                          #     renderer/cstyle.py            :   180: 

                                                            class CStyleLanguage(Renderer):                                                                                                              #     renderer/cstyle.py            :     9: 
                                                              # returns a str expression of the const with the given type                                                                                #     renderer/cstyle.py            :    48: 
                                                              def render_const(self, x:ConstType, dtype:DType) -> str:                                                                                        
                                                                assert dtype.count == 1, f"consts should be scalar, got {dtype}"                                                                         #     renderer/cstyle.py            :    49: L: {'x': 0}    
                                                                if math.isnan(x): val = self.nan                                                                                                         #     renderer/cstyle.py            :    50: 
                                                                elif math.isinf(x): val = ("-" if x < 0 else "") + self.infinity                                                                         #     renderer/cstyle.py            :    51: 
                                                                elif dtype == dtypes.bool: val = "1" if x else "0"                                                                                       #     renderer/cstyle.py            :    52: 
                                                                elif dtype == dtypes.float: val = f"{x}f"                                                                                                #     renderer/cstyle.py            :    53: 
                                                                elif dtype == dtypes.uint64: val = f"{x}ULL"                                                                                             #     renderer/cstyle.py            :    54: 
                                                                else: val = str(x)                                                                                                                       #     renderer/cstyle.py            :    55: 
                                                                return (self.render_cast(val, dtype) if dtype not in [dtypes.float, dtypes.int, dtypes.bool] else val)                                   #     renderer/cstyle.py            :    56: L: {'val': '0'}    

                                                        #else: # branches differently:                                                                                                                   # OLD renderer/cstyle.py            :   131: 
                                                          assert dtype is not None, f"None dtype for uop {uop}"                                                                                          # OLD renderer/cstyle.py            :   132: 
                                                          if uop is UOps.RANGE:                                                                                                                          # OLD renderer/cstyle.py            :   133: 
                                                            kk(f"for (int {(expr := ssa('ridx',u))} = {r[src[0]]}; {expr} < {r[src[1]]}; {expr}++) {{")                                                  #     renderer/cstyle.py            :   134: L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': [], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 1, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>, 'c': defaultdict(<class 'int'>, {}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), 'uop': <UOps.RANGE: 26>, 'src': (UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=())), 'args': (0, False), 'nm': 'data1'}    

                                                              class CStyleLanguage(Renderer):                                                                                                            #     renderer/cstyle.py            :     9: 
                                                                def render(self, name:str, uops:List[UOp]) -> str:                                                                                       #     renderer/cstyle.py            :    96: 
                                                                  def ssa(prefix:str, u:Optional[UOp]=None):                                                                                             #     renderer/cstyle.py            :   105: 
                                                                    ret = f"{prefix}{c[prefix]}"                                                                                                         #     renderer/cstyle.py            :   107: L: {'prefix': 'ridx'}    
                                                                    if u is not None: r[u] = ret                                                                                                         #     renderer/cstyle.py            :   108: L: {'ret': 'ridx0'}    
                                                                    c[prefix] += 1                                                                                                                       #     renderer/cstyle.py            :   109: 
                                                                    return ret                                                                                                                           #     renderer/cstyle.py            :   110: 

                                                              class CStyleLanguage(Renderer):                                                                                                            #     renderer/cstyle.py            :     9: 
                                                                def render(self, name:str, uops:List[UOp]) -> str:                                                                                       #     renderer/cstyle.py            :    96: 
                                                                  def kk(s): kernel.append("  "*depth+s)                                                                                                 #     renderer/cstyle.py            :   100: 

                                                            depth += 1                                                                                                                                   #     renderer/cstyle.py            :   135: L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'self': <tinygrad.renderer.cstyle.ClangRenderer object at 0x78971f1d5180>, 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 1, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'uop': <UOps.RANGE: 26>, 'dtype': dtypes.int, 'src': (UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=())), 'args': (0, False), 'nm': 'data1', 'expr': 'ridx0'}    
                                                          elif uop is UOps.LOAD:                                                                                                                         #     renderer/cstyle.py            :   154: 
                                                            val = self.render_load(dtype, r[src[0]], src[0].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)                              #     renderer/cstyle.py            :   155: L: {'depth': 2, 'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), 'uop': <UOps.LOAD: 21>, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)))}    

                                                              def strip_parens(fst:str): return fst[1:-1] if fst[0] == '(' and fst[-1] == ')' and fst[1:-1].find('(') <= fst[1:-1].find(')') else fst    #     helpers.py                    :    37: 

                                                              class CStyleLanguage(Renderer):                                                                                                            #     renderer/cstyle.py            :     9: 
                                                                # returns a str expression of the loaded value with the output type                                                                      #     renderer/cstyle.py            :    59: 
                                                                def render_load(self, output_dtype, buf_name, buf_dtype, idx, local=False) -> str:                                                            
                                                                  if isinstance(buf_dtype, ImageDType):                                                                                                  #     renderer/cstyle.py            :    60: L: {'output_dtype': dtypes.int, 'buf_name': 'data1', 'buf_dtype': PtrDType(dtypes.int), 'idx': 'ridx0', 'local': False}    
                                                                  if self.uses_vload and buf_dtype.scalar() == dtypes.float16 and output_dtype.scalar() != dtypes.float16:                               #     renderer/cstyle.py            :    63: 
                                                                  if output_dtype.count > 1:                                                                                                             #     renderer/cstyle.py            :    65: 
                                                                  return f"*({buf_name}+{idx})" if self.uses_ptr_arithmetic else f"{buf_name}[{idx}]"                                                    #     renderer/cstyle.py            :    67: 

                                                            # NOTE: this relies on the load not happening if it's in the unselected branch
                                                            if len(src) > 3 and src[3].op is UOps.ALU: val = self.code_for_op[TernaryOps.WHERE](r[src[3]], val, r[src[2]], dtype)                        #     renderer/cstyle.py            :   157: L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>, 'c': defaultdict(<class 'int'>, {'ridx': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), 'uop': <UOps.LOAD: 21>, 'dtype': dtypes.int, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))), 'nm': 'data1', 'expr': 'ridx0', 'val': 'data1[ridx0]'}    
                                                            kk(f"{self.render_dtype(dtype)} {ssa('val',u)} = {val};")                                                                                    #     renderer/cstyle.py            :   158: 

                                                              class CStyleLanguage(Renderer):                                                                                                            #     renderer/cstyle.py            :     9: 
                                                                def render_dtype(self, var_dtype:DType) -> str:                                                                                          #     renderer/cstyle.py            :    93: 
                                                                  return self.type_map.get(scalar:=var_dtype.scalar(), scalar.name) + (str(var_dtype.count) if (var_dtype.count) > 1 else "")            #     renderer/cstyle.py            :    94: L: {'var_dtype': dtypes.int}    

                                                        #else: # branches differently:                                                                                                                   # OLD renderer/cstyle.py            :   131: 
                                                          assert dtype is not None, f"None dtype for uop {uop}"                                                                                          # OLD renderer/cstyle.py            :   132: 
                                                          if uop is UOps.RANGE:                                                                                                                          # OLD renderer/cstyle.py            :   133: 
                                                          elif uop is UOps.ALU:                                                                                                                          # OLD renderer/cstyle.py            :   136: 
                                                            # remove parens if ALU types are the same. TODO: can do more here
                                                            if args in {BinaryOps.ADD,BinaryOps.MUL,BinaryOps.XOR}: operands = [strip_parens(r[v]) if v.arg == args else r[v]for v in src]               #     renderer/cstyle.py            :   138: L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {', '    int val0 = data1[ridx0];'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>, 'c': defaultdict(<class 'int'>, {'ridx': 1, 'val': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0', UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 'val0'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), 'uop': <UOps.ALU: 17>, 'dtype': dtypes.int, 'src': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.CONST, dtypes.int, arg=2, src=())), 'args': <BinaryOps.ADD: 1>, 'nm': 'data1', 'expr': 'ridx0', 'val': 'data1[ridx0]'}    
                                                            val = self.code_for_op[args](*operands, dtype)                                                                                               #     renderer/cstyle.py            :   141: L: {'operands': ['val0', '2']}    
                                                            assert child_count[u] != 0, f"childless ALU op found {u}"                                                                                    #     renderer/cstyle.py            :   142: L: {'val': '(val0+2)'}    
                                                            # TODO: fix index rendering issue. fix clang nested max macro issue
                                                            if child_count[u] <= 1 and args is not BinaryOps.MAX and not getenv("EXPAND_SSA"): r[u] = val                                                #     renderer/cstyle.py            :   144: 
                                                      #for u in uops: # branches differently:                                                                                                            # OLD renderer/cstyle.py            :   115: 
                                                        uop,dtype,src,args = u.op,u.dtype,u.src,u.arg                                                                                                    # OLD renderer/cstyle.py            :   116: 
                                                        # these four uops don't have output dtypes                                                                                                       #     renderer/cstyle.py            :   117: 
                                                        # these four uops don't have output dtypes
                                                        if uop is UOps.IF:                                                                                                                               # OLD renderer/cstyle.py            :   118: 
                                                        elif uop is UOps.BARRIER: kk(self.barrier)                                                                                                       # OLD renderer/cstyle.py            :   121: 
                                                        elif uop in {UOps.ENDRANGE, UOps.ENDIF}:                                                                                                         # OLD renderer/cstyle.py            :   122: 
                                                        elif uop is UOps.STORE:                                                                                                                          # OLD renderer/cstyle.py            :   125: 
                                                          assert src[0].dtype is not None and src[2].dtype is not None                                                                                   #     renderer/cstyle.py            :   126: L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'uop': <UOps.STORE: 22>, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)))}    
                                                          # mark DEFINE_GLOBAL buf as writable
                                                          if src[0].op is UOps.DEFINE_GLOBAL: bufs[src[0]] = (bufs[src[0]][0], (bufs[src[0]][1][0], True))                                               #     renderer/cstyle.py            :   128: 
                                                          rendered_store = self.render_store(r[src[0]], src[0].dtype, r[src[2]], src[2].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)  #     renderer/cstyle.py            :   129: 

                                                            class CStyleLanguage(Renderer):                                                                                                              #     renderer/cstyle.py            :     9: 
                                                              # returns a str statement that does the store                                                                                              #     renderer/cstyle.py            :    81: 
                                                              def render_store(self, buf_name:str, buf_dtype:DType, var_name:str, var_dtype:DType, idx:str, local=False) -> str:                              
                                                                if isinstance(buf_dtype, ImageDType):                                                                                                    #     renderer/cstyle.py            :    82: L: {'buf_name': 'data0', 'buf_dtype': PtrDType(dtypes.int), 'var_name': '(val0+2)', 'var_dtype': dtypes.int, 'idx': 'ridx0', 'local': False}    
                                                                if self.uses_vload and buf_dtype.scalar() == dtypes.float16 and var_dtype.scalar() != dtypes.float16:                                    #     renderer/cstyle.py            :    85: 
                                                                if var_dtype.count > 1:                                                                                                                  #     renderer/cstyle.py            :    87: 
                                                                return f"*({buf_name}+{idx}) = {var_name};" if self.uses_ptr_arithmetic else f"{buf_name}[{idx}] = {var_name};"                          #     renderer/cstyle.py            :    90: 

                                                          kk(f"if ({r[src[3]]}) {{ {rendered_store} }}" if len(src) > 3 else rendered_store)                                                             #     renderer/cstyle.py            :   130: L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {', '    int val0 = data1[ridx0];'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), True)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x78971f0d37f0>, 'c': defaultdict(<class 'int'>, {'ridx': 1, 'val': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0', UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 'val0', UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): '(val0+2)'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x78971f0d84c0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'uop': <UOps.STORE: 22>, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))), 'nm': 'data1', 'expr': 'ridx0', 'val': '(val0+2)', 'operands': ['val0', '2'], 'rendered_store': 'data0[ridx0] = (val0+2);'}    
                                                      #for u in uops: # branches differently:                                                                                                            # OLD renderer/cstyle.py            :   115: 
                                                        uop,dtype,src,args = u.op,u.dtype,u.src,u.arg                                                                                                    # OLD renderer/cstyle.py            :   116: 
                                                        # these four uops don't have output dtypes                                                                                                       # OLD renderer/cstyle.py            :   117: 
                                                        # these four uops don't have output dtypes
                                                        if uop is UOps.IF:                                                                                                                               # OLD renderer/cstyle.py            :   118: 
                                                        elif uop is UOps.BARRIER: kk(self.barrier)                                                                                                       # OLD renderer/cstyle.py            :   121: 
                                                        elif uop in {UOps.ENDRANGE, UOps.ENDIF}:                                                                                                         # OLD renderer/cstyle.py            :   122: 
                                                          depth -= 1                                                                                                                                     #     renderer/cstyle.py            :   123: L: {'u': UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), 'uop': <UOps.ENDRANGE: 27>, 'src': (UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)}    
                                                          kk("}")                                                                                                                                        #     renderer/cstyle.py            :   124: L: {'depth': 1}    
                                                  
                                                      # NOTE: this relies on bufs dict preserving order
                                                      return self.render_kernel(name, kernel, list(bufs.values()), uops)                                                                                 #     renderer/cstyle.py            :   189: 

                                                        class ClangRenderer(CStyleLanguage):                                                                                                             #     renderer/cstyle.py            :   194: 
                                                          def render_kernel(self, function_name, kernel, bufs, uops, prefix=None) -> str:                                                                #     renderer/cstyle.py            :   209: 
                                                            prefix = [_make_clang_dtype(self, dtype) for dtype in dedup(uop.dtype for uop in uops if uop.dtype is not None and uop.dtype.count>1)]       #     renderer/cstyle.py            :   210: L: {'function_name': 'E_3', 'bufs': [('data0', (PtrDType(dtypes.int), True)), ('data1', (PtrDType(dtypes.int), False))], '__class__': <class 'tinygrad.renderer.cstyle.ClangRenderer'>}    
                                                            return super().render_kernel(function_name, kernel, bufs, uops, prefix)                                                                      #     renderer/cstyle.py            :   211: L: {'prefix': []}    

                                                              class CStyleLanguage(Renderer):                                                                                                            #     renderer/cstyle.py            :     9: 
                                                                def render_kernel(self, function_name:str, kernel:List[str], bufs:List[Tuple[str,Tuple[DType,bool]]], uops:List[UOp], prefix=None) -> str: #     renderer/cstyle.py            :    70: 
                                                                  tmp = "const sampler_t smp = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST;\n" if any(isinstance(dtype, ImageDType) for _,(dtype,_) in bufs) else ""  # noqa: E501 #     renderer/cstyle.py            :    71: 
                                                                  buftypes = [(name,f"{'write_only' if mutable else 'read_only'} image2d_t" if dtype.name.startswith('image') else                       #     renderer/cstyle.py            :    72: L: {'tmp': ''}    
                                                                              ("" if mutable else "const ")+self.buffer_prefix+self.render_dtype(dtype)+"*"+self.buffer_suffix if isinstance(dtype, PtrDType) else      
                                                                              self.arg_int_prefix if dtype == dtypes.int else None) for name,(dtype,mutable) in bufs]                                         
                                                                  prg = ''.join([f"{self.kernel_prefix}void {self.get_kernel_modifier(uops)}{function_name}(",] +                                        #     renderer/cstyle.py            :    75: L: {'buftypes': [('data0', 'int* restrict'), ('data1', 'const int* restrict')]}    
                                                                  [', '.join([f'{t} {name}' for name,t in buftypes] + self.extra_args)] +                                                                     
                                                                  [") {\n" + tmp] + ['\n'.join(kernel), "\n}"])                                                                                               

                                                                    class CStyleLanguage(Renderer):                                                                                                      #     renderer/cstyle.py            :     9: 
                                                                      def get_kernel_modifier(self, uops:List[UOp]) -> str: return ""                                                                    #     renderer/cstyle.py            :    69: 

                                                                  return prg if prefix is None else "\n".join(prefix)+f"\n{prg}"                                                                         #     renderer/cstyle.py            :    78: L: {'prg': 'void E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}    

                                            
                                                if getenv("RUN_PROCESS_REPLAY"):                                                                                                                         #     codegen/kernel.py             :   759: L: {'ansiname': 'E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', 'name': 'E_3', 'src': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}    
                                            
                                                # group non-local bufs by the op type (LOAD or STORE) and the buffer arg. take the max access of that buffer in bytes
                                                # TODO: these max and min don't work on symbolic, and results are very wrong.
                                                mem_bytes = sum(max(cast(DType, x.src[0].dtype).itemsize * x.st_arg.real_size() for x in group)                                                          #     codegen/kernel.py             :   765: 
                                                  for _, group in itertools.groupby([x for x in self.ast.parents if x.op in BUFFER_UOPS and x.src[0].op is UOps.DEFINE_GLOBAL],                               
                                                                    key=lambda x: (x.op, x.src[0].arg)))                                                                                                      

                                                  @dataclass(frozen=True)                                                                                                                                #     shape/shapetracker.py         :    36: 
                                                  class ShapeTracker:                                                                                                                                         
                                                    def real_size(self) -> int:                                                                                                                          #     shape/shapetracker.py         :    84: 
                                                      if 0 in self.shape: return 0                                                                                                                       #     shape/shapetracker.py         :    85: L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}    
                                                      idx, valid = self.to_indexed_uops()                                                                                                                #     shape/shapetracker.py         :    86: 
                                                      if not valid.vmax.arg: return 0                                                                                                                    #     shape/shapetracker.py         :    87: L: {'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'valid': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}    
                                                      assert idx.vmax.arg < 1e12, f"real_size broken for {self}"                                                                                         #     shape/shapetracker.py         :    88: 

                                                        @dataclass(frozen=True, eq=False)                                                                                                                #     ops.py                        :    58: 
                                                        class UOp:                                                                                                                                            
                                                          @functools.cached_property                                                                                                                     #     ops.py                        :   161: 
                                                          def _min_max(self) -> Tuple[Optional[UOp], Optional[UOp]]:                                                                                          
                                                            # NOTE: returned UOp is assumed to be CONST
                                                            if self.op is UOps.DEFINE_VAR and self.src: return self.src[0], self.src[1] if isinstance(self.src[1].arg, int) else None                    # OLD ops.py                        :   163: 
                                                            if self.op is UOps.RANGE: return self.src[0].vmin, (self.src[1]-1).vmax                                                                      # OLD ops.py                        :   164: 
                                                            # TODO: UOps.SPECIAL is UOps.DEFINE_VAR
                                                            if self.op is UOps.SPECIAL: return self.const(0), self.const(self.arg[1]-1) if isinstance(self.arg[1], int) else None                        # OLD ops.py                        :   166: 
                                                            if self.op is UOps.CONST: return self, self                                                                                                  # OLD ops.py                        :   167: 
                                                            if self.op is UOps.ALU and cast(DType, self.dtype).count == 1:                                                                               # OLD ops.py                        :   168: 
                                                              s0,s1 = [cast(UOp, self.src[i] if i < len(self.src) else None) for i in range(2)]                                                          # OLD ops.py                        :   169: 
                                                              if self.arg is BinaryOps.ADD: return self.sconst(s0.vmin.arg+s1.vmin.arg), self.sconst(s0.vmax.arg+s1.vmax.arg)                            # OLD ops.py                        :   170: 
                                                              if self.arg is BinaryOps.MUL and (s0.vmin.arg >= 0 or s1.vmin.arg >= 0):                                                                   #     ops.py                        :   171: L: {'self': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n  UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n  UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)), 's0': UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)), 's1': UOp(UOps.CONST, dtypes.pyint, arg=1, src=())}    
                                                                # handle at lease one is non-negative
                                                                Lmin, Lmax = (s0.vmin.arg, s0.vmax.arg) if s1.vmin.arg >= 0 else (s0.vmax.arg, s0.vmin.arg)                                              #     ops.py                        :   173: 
                                                                Rmin, Rmax = (s1.vmin.arg, s1.vmax.arg) if s0.vmin.arg >= 0 else (s1.vmax.arg, s1.vmin.arg)                                              #     ops.py                        :   174: L: {'Lmin': 0, 'Lmax': 2}    
                                                                assert math.isnan(Lmax*Rmax) or math.isnan(Lmin*Rmin) or Lmax*Rmax >= Lmin*Rmin, f"{Lmax=}, {Lmin=}, {Rmax=}, {Rmin=}"                   #     ops.py                        :   175: L: {'Rmin': 1, 'Rmax': 1}    
                                                                return self.sconst(Lmin*Rmin), self.sconst(Lmax*Rmax)                                                                                    #     ops.py                        :   176: 

                                                      return idx.vmax.arg+1                                                                                                                              #     shape/shapetracker.py         :    89: 

                                                return Program(ansiname, src, self.opts.device, self.uops, mem_estimate=mem_bytes,                                                                       #     codegen/kernel.py             :   768: L: {'mem_bytes': 24}    
                                                               global_size=[1,1,1] if self.opts.has_local else None, local_size=[1,1,1] if self.opts.has_local else None)                                     

                                                  @dataclass                                                                                                                                             #     renderer/__init__.py          :    18: 
                                                  class Program:                                                                                                                                              
                                                    def __post_init__(self):                                                                                                                             #     renderer/__init__.py          :    33: 
                                                      if not self._ran_post_init and self.uops is not None:                                                                                              #     renderer/__init__.py          :    34: G: {'Renderer': <class 'tinygrad.renderer.Renderer'>, 'cstyle': <module 'tinygrad.renderer.cstyle' from '/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py'>}    L: {'self': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[], outs=[], _ran_post_init=False)}    
                                                        # single pass through the uops
                                                        for u in self.uops:                                                                                                                              #     renderer/__init__.py          :    36: 
                                                          if u.op is UOps.DEFINE_VAR: self.vars.append(u.arg)                                                                                            #     renderer/__init__.py          :    37: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                          if u.op is UOps.DEFINE_GLOBAL: self.globals.append(u.arg)                                                                                      #     renderer/__init__.py          :    38: 
                                                          if u.op is UOps.STORE: self.outs.extend([x.arg for x in u.src[0].sparents if x.op is UOps.DEFINE_GLOBAL])                                      #     renderer/__init__.py          :    39: 
                                                          if u.op is UOps.SPECIAL:                                                                                                                       #     renderer/__init__.py          :    40: 
                                                        self.vars = sorted(self.vars, key=lambda v: v.expr)                                                                                              #     renderer/__init__.py          :    49: L: {'u': UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                        self.outs = sorted(dedup(self.outs))                                                                                                             #     renderer/__init__.py          :    50: 
                                                        self._ran_post_init = True                                                                                                                       #     renderer/__init__.py          :    51: 

                                          if getenv("FUZZ_UOPS"):                                                                                                                                        #     engine/realize.py             :   158: L: {'dname': 'CLANG', 'ckey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, False), 'bkey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, True), 'prg': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True)}    
                                          method_cache[ckey] = method_cache[bkey] = ret = CompiledRunner(replace(prg, dname=dname))                                                                      #     engine/realize.py             :   161: 

                                            class CompiledRunner(Runner):                                                                                                                                #     engine/realize.py             :    79: 
                                              def __init__(self, p:Program, precompiled:Optional[bytes]=None):                                                                                           #     engine/realize.py             :    80: 
                                                if DEBUG >= 4: print(p.src)                                                                                                                              #     engine/realize.py             :    81: L: {'self': <tinygrad.engine.realize.CompiledRunner object at 0x7897305a9660>, 'p': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True), '__class__': <class 'tinygrad.engine.realize.CompiledRunner'>}    
                                                self.p:Program = p                                                                                                                                       #     engine/realize.py             :    82: 
                                                self.lib:bytes = precompiled if precompiled is not None else Device[p.dname].compiler.compile_cached(p.src)                                              #     engine/realize.py             :    83: 

                                                  class Compiler:                                                                                                                                        #     device.py                     :   176: 
                                                    def compile_cached(self, src:str) -> bytes:                                                                                                          #     device.py                     :   179: 
                                                      if self.cachekey is None or (lib := diskcache_get(self.cachekey, src)) is None:                                                                    #     device.py                     :   180: L: {'self': <tinygrad.runtime.ops_clang.ClangCompiler object at 0x789735f92230>, 'src': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}    

                                                        def diskcache_get(table:str, key:Union[Dict, str, int]) -> Any:                                                                                  #     helpers.py                    :   228: 
                                                          if CACHELEVEL == 0: return None                                                                                                                #     helpers.py                    :   229: L: {'table': 'compile_clang', 'key': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}    
                                                          if isinstance(key, (str,int)): key = {"key": key}                                                                                              #     helpers.py                    :   230: 
                                                          conn = db_connection()                                                                                                                         #     helpers.py                    :   231: L: {'key': {'key': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}}    

                                                            def db_connection():                                                                                                                         #     helpers.py                    :   212: 
                                                              if _db_connection is None:                                                                                                                 #     helpers.py                    :   214: 
                                                                os.makedirs(CACHEDB.rsplit(os.sep, 1)[0], exist_ok=True)                                                                                 #     helpers.py                    :   215: 
                                                                _db_connection = sqlite3.connect(CACHEDB, timeout=60, isolation_level="IMMEDIATE")                                                       #     helpers.py                    :   216: 
                                                                # another connection has set it already or is in the process of setting it
                                                                # that connection will lock the database
                                                                with contextlib.suppress(sqlite3.OperationalError): _db_connection.execute("PRAGMA journal_mode=WAL").fetchone()                         #     helpers.py                    :   219: G: {'_db_connection': <sqlite3.Connection object at 0x7897305f4740>}    
                                                                if DEBUG >= 7: _db_connection.set_trace_callback(print)                                                                                  #     helpers.py                    :   220: 
                                                              return _db_connection                                                                                                                      #     helpers.py                    :   221: 

                                                          cur = conn.cursor()                                                                                                                            #     helpers.py                    :   232: L: {'table': 'compile_clang', 'key': {'key': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}, 'conn': <sqlite3.Connection object at 0x7897305f4740>}    
                                                          try:                                                                                                                                           #     helpers.py                    :   233: L: {'cur': <sqlite3.Cursor object at 0x78971f1cc340>}    
                                                            res = cur.execute(f"SELECT val FROM '{table}_{VERSION}' WHERE {' AND '.join([f'{x}=?' for x in key.keys()])}", tuple(key.values()))          #     helpers.py                    :   234: 
                                                          if (val:=res.fetchone()) is not None: return pickle.loads(val[0])                                                                              #     helpers.py                    :   237: L: {'res': <sqlite3.Cursor object at 0x78971f1cc340>}    

                                                      return lib                                                                                                                                         #     device.py                     :   184: L: {'lib': b"\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00>\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00 1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x008\x00\x08\x00@\x00\x0c\x00\x0b\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x05\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x04\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00Q\xe5td\x06\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00R\xe5td\x04\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00GNU\x00\xa7K\x95\xab\x90\x99\x8dZUPD\x8f\xce\x1ak\xe0\xf0\xf1\xe7\x99\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x08\x10\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\xdd\xe4\x87\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00E_3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x8b\x06\x83\xc0\x02\x89\x07\x8bF\x04\x83\xc0\x02\x89G\x04\x8bF\x08\x83\xc0\x02\x89G\x08\xc3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf5\xfe\xffo\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Ubuntu clang version 14.0.0-1ubuntu1.1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x01\x00\x07\x00P/\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0c\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00-\x00_DYNAMIC\x00E_3\x00\x00.symtab\x00.strtab\x00.shstrtab\x00.note.gnu.build-id\x00.gnu.hash\x00.dynsym\x00.dynstr\x00.text\x00.eh_frame\x00.dynamic\x00.comment\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1b\x00\x00\x00\x07\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00.\x00\x00\x00\xf6\xff\xffo\x02\x00\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x008\x00\x00\x00\x0b\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x03\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00H\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00N\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00X\x00\x00\x00\x06\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00a\x00\x00\x00\x01\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00(0\x00\x00\x00\x00\x00\x00x\x00\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x04\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\t\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xb00\x00\x00\x00\x00\x00\x00j\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"}    

                                                self.clprg = Device[p.dname].runtime(p.function_name, self.lib)                                                                                          #     engine/realize.py             :    84: 

                                                  @dataclass                                                                                                                                             #     renderer/__init__.py          :    18: 
                                                  class Program:                                                                                                                                              
                                                    @functools.cached_property                                                                                                                           #     renderer/__init__.py          :    64: 
                                                    def function_name(self) -> str: return to_function_name(self.name)                                                                                        

                                                  class ClangProgram:                                                                                                                                    #     runtime/ops_clang.py          :    14: 
                                                    def __init__(self, name:str, lib:bytes):                                                                                                             #     runtime/ops_clang.py          :    15: 
                                                      if DEBUG >= 6: cpu_objdump(lib)                                                                                                                    #     runtime/ops_clang.py          :    16: L: {'self': <tinygrad.runtime.ops_clang.ClangProgram object at 0x78971f2b3640>, 'name': 'E_3', 'lib': b"\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00>\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00 1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x008\x00\x08\x00@\x00\x0c\x00\x0b\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x05\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x04\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00Q\xe5td\x06\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00R\xe5td\x04\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00GNU\x00\xa7K\x95\xab\x90\x99\x8dZUPD\x8f\xce\x1ak\xe0\xf0\xf1\xe7\x99\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x08\x10\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\xdd\xe4\x87\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00E_3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x8b\x06\x83\xc0\x02\x89\x07\x8bF\x04\x83\xc0\x02\x89G\x04\x8bF\x08\x83\xc0\x02\x89G\x08\xc3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf5\xfe\xffo\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Ubuntu clang version 14.0.0-1ubuntu1.1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x01\x00\x07\x00P/\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0c\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00-\x00_DYNAMIC\x00E_3\x00\x00.symtab\x00.strtab\x00.shstrtab\x00.note.gnu.build-id\x00.gnu.hash\x00.dynsym\x00.dynstr\x00.text\x00.eh_frame\x00.dynamic\x00.comment\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1b\x00\x00\x00\x07\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00.\x00\x00\x00\xf6\xff\xffo\x02\x00\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x008\x00\x00\x00\x0b\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x03\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00H\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00N\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00X\x00\x00\x00\x06\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00a\x00\x00\x00\x01\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00(0\x00\x00\x00\x00\x00\x00x\x00\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x04\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\t\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xb00\x00\x00\x00\x00\x00\x00j\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"}    
                                                      self.name, self.lib = name, lib                                                                                                                    #     runtime/ops_clang.py          :    17: 
                                                      # write to disk so we can load it
                                                      with tempfile.NamedTemporaryFile(delete=True) as cached_file_path:                                                                                 #     runtime/ops_clang.py          :    19: 
                                                        pathlib.Path(cached_file_path.name).write_bytes(lib)                                                                                             #     runtime/ops_clang.py          :    20: L: {'cached_file_path': <tempfile._TemporaryFileWrapper object at 0x78971f114e50>}    
                                                        self.fxn = ctypes.CDLL(str(cached_file_path.name))[name]                                                                                         #     runtime/ops_clang.py          :    21: 

                                                super().__init__(p.name, p.dname, p.op_estimate, p.mem_estimate, p.lds_estimate)                                                                         #     engine/realize.py             :    85: 

                                                  @dataclass                                                                                                                                             #     renderer/__init__.py          :    18: 
                                                  class Program:                                                                                                                                              
                                                    @property                                                                                                                                            #     renderer/__init__.py          :    54: 
                                                    def op_estimate(self) -> sint: return self._ops_lds[0]                                                                                                    

                                                      @dataclass                                                                                                                                         #     renderer/__init__.py          :    18: 
                                                      class Program:                                                                                                                                          
                                                        @functools.cached_property                                                                                                                       #     renderer/__init__.py          :    58: 
                                                        def _ops_lds(self) -> Tuple[sint, sint]: return (0,0) if self.uops is None else flops_mem(self.uops, ignore_indexing=True)                            

                                                          def flops_mem(uops:List[UOp], ignore_indexing=False) -> Tuple[sint, sint]:                                                                     #     ops.py                        :   430: 
                                                            flops: sint = 0                                                                                                                              #     ops.py                        :   431: L: {'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'ignore_indexing': True}    
                                                            mem: sint = 0                                                                                                                                #     ops.py                        :   432: L: {'flops': 0}    
                                                            mults: sint = 1                                                                                                                              #     ops.py                        :   433: L: {'mem': 0}    
                                                            mult_stack: List[sint] = []                                                                                                                  #     ops.py                        :   434: L: {'mults': 1}    
                                                            dont_count: Set[UOp] = set()                                                                                                                 #     ops.py                        :   435: L: {'mult_stack': []}    
                                                            if ignore_indexing:                                                                                                                          #     ops.py                        :   436: L: {'dont_count': set()}    
                                                              for u in uops:                                                                                                                             #     ops.py                        :   437: 
                                                                if u.op is UOps.LOAD:                                                                                                                    #     ops.py                        :   438: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                                elif u.op is UOps.STORE:                                                                                                                 #     ops.py                        :   441: 
                                                                elif u.op is UOps.IF:                                                                                                                    #     ops.py                        :   444: 
                                                              #for u in uops: # branches differently:                                                                                                    # OLD ops.py                        :   437: 
                                                                if u.op is UOps.LOAD:                                                                                                                    # OLD ops.py                        :   438: 
                                                                  dont_count = dont_count.union(u.src[1].sparents)                                                                                       #     ops.py                        :   439: L: {'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                                  if len(u.src) > 3: dont_count = dont_count.union(u.src[2].sparents)                                                                    #     ops.py                        :   440: L: {'dont_count': {UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=())}}    
                                                                elif u.op is UOps.STORE:                                                                                                                 #     ops.py                        :   441: 
                                                                  dont_count = dont_count.union(u.src[1].sparents)                                                                                       #     ops.py                        :   442: L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}    
                                                                  if len(u.src) > 3: dont_count = dont_count.union(u.src[3].sparents)                                                                    #     ops.py                        :   443: L: {'dont_count': {UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.CONST, dtypes.int, arg=3, src=())}}    
                                                            for u in uops:                                                                                                                               #     ops.py                        :   446: L: {'u': UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                              if u.op is UOps.RANGE:                                                                                                                     #     ops.py                        :   447: L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}    
                                                              elif u.op is UOps.ENDRANGE:                                                                                                                #     ops.py                        :   450: 
                                                              elif u.op is UOps.SPECIAL:                                                                                                                 #     ops.py                        :   452: 
                                                              elif u.op is UOps.LOAD:                                                                                                                    #     ops.py                        :   454: 
                                                              elif u.op is UOps.STORE:                                                                                                                   #     ops.py                        :   457: 
                                                              elif u.op is UOps.ALU and u not in dont_count:                                                                                             #     ops.py                        :   460: 
                                                              elif u.op is UOps.WMMA and u not in dont_count:                                                                                            #     ops.py                        :   463: 
                                                            #for u in uops: # branches differently:                                                                                                      # OLD ops.py                        :   446: 
                                                              if u.op is UOps.RANGE:                                                                                                                     # OLD ops.py                        :   447: 
                                                                mult_stack.append(mults)                                                                                                                 #     ops.py                        :   448: L: {'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))}    
                                                                mults *= uop_alu_resolve(u.src[1] - u.src[0])                                                                                            #     ops.py                        :   449: 

                                                                  def uop_alu_resolve(u:UOp) -> sint:                                                                                                    #     ops.py                        :   374: 
                                                                    if u.op in {UOps.CONST, UOps.DEFINE_VAR}: return u.arg                                                                               #     ops.py                        :   375: L: {'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.MUL, src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=-1, src=()),)),))}    
                                                                    if u.op is UOps.ALU: return exec_alu(u.arg, cast(DType,u.dtype), tuple(map(uop_alu_resolve, u.src)))                                 #     ops.py                        :   376: 

                                                                      def exec_alu(op:Op, dtype:DType, operands): return truncate.get(dtype, lambda x: x)(python_alu[op](*operands))                     #     ops.py                        :   372: 

                                                              elif u.op is UOps.LOAD:                                                                                                                    #     ops.py                        :   454: 
                                                                assert u.dtype is not None                                                                                                               #     ops.py                        :   455: L: {'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'ignore_indexing': True, 'flops': 0, 'mem': 0, 'mults': 3, 'mult_stack': [1], 'dont_count': {UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.CONST, dtypes.int, arg=3, src=())}, 'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                                mem += u.dtype.itemsize * mults                                                                                                          #     ops.py                        :   456: 
                                                              elif u.op is UOps.ALU and u not in dont_count:                                                                                             #     ops.py                        :   460: 
                                                                assert u.dtype is not None                                                                                                               #     ops.py                        :   461: L: {'mem': 12, 'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))}    
                                                                flops += (mults * (2 if u.arg == TernaryOps.MULACC else 1)) * u.dtype.count                                                              #     ops.py                        :   462: 
                                                            #for u in uops: # branches differently:                                                                                                      # OLD ops.py                        :   446: 
                                                              if u.op is UOps.RANGE:                                                                                                                     # OLD ops.py                        :   447: 
                                                              elif u.op is UOps.ENDRANGE:                                                                                                                # OLD ops.py                        :   450: 
                                                              elif u.op is UOps.SPECIAL:                                                                                                                 # OLD ops.py                        :   452: 
                                                              elif u.op is UOps.LOAD:                                                                                                                    # OLD ops.py                        :   454: 
                                                              elif u.op is UOps.STORE:                                                                                                                   # OLD ops.py                        :   457: 
                                                                assert u.src[2].dtype is not None                                                                                                        #     ops.py                        :   458: L: {'flops': 3, 'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}    
                                                                mem += u.src[2].dtype.itemsize * mults                                                                                                   #     ops.py                        :   459: 
                                                            #for u in uops: # branches differently:                                                                                                      # OLD ops.py                        :   446: 
                                                              if u.op is UOps.RANGE:                                                                                                                     # OLD ops.py                        :   447: 
                                                              elif u.op is UOps.ENDRANGE:                                                                                                                # OLD ops.py                        :   450: 
                                                                mults = mult_stack.pop(-1)                                                                                                               #     ops.py                        :   451: L: {'mem': 24, 'u': UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}    
                                                            return flops, mem                                                                                                                            #     ops.py                        :   466: L: {'mults': 1}    

                                                  @dataclass                                                                                                                                             #     renderer/__init__.py          :    18: 
                                                  class Program:                                                                                                                                              
                                                    @property                                                                                                                                            #     renderer/__init__.py          :    56: 
                                                    def lds_estimate(self) -> sint: return self._ops_lds[1]                                                                                                   

                                        return ret                                                                                                                                                       #     engine/realize.py             :   162: L: {'dname': 'CLANG', 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'ckey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, False), 'bkey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, True), 'prg': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True), 'ret': <tinygrad.engine.realize.CompiledRunner object at 0x7897305a9660>}    

                                    return ExecItem(runner, [si.bufs[x] for x in runner.p.globals], si.metadata)                                                                                         #     engine/realize.py             :   193: L: {'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__]), 'runner': <tinygrad.engine.realize.CompiledRunner object at 0x7897305a9660>}    

                                class CompiledRunner(Runner):                                                                                                                                            #     engine/realize.py             :    79: 
                                  def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:                                                                 #     engine/realize.py             :    89: 
                                    global_size, local_size = self.p.launch_dims(var_vals)                                                                                                               #     engine/realize.py             :    90: L: {'self': <tinygrad.engine.realize.CompiledRunner object at 0x7897305a9660>, 'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>], 'wait': False, 'var_vals': {}}    

                                      @dataclass                                                                                                                                                         #     renderer/__init__.py          :    18: 
                                      class Program:                                                                                                                                                          
                                        def launch_dims(self, var_vals:Dict[Variable, int]):                                                                                                             #     renderer/__init__.py          :    66: 
                                          global_size = [sym_infer(sz, var_vals) for sz in self.global_size] if self.global_size is not None else None                                                   #     renderer/__init__.py          :    67: L: {'self': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True), 'var_vals': {}}    
                                          local_size = [sym_infer(sz, var_vals) for sz in self.local_size] if self.local_size is not None else None                                                      #     renderer/__init__.py          :    68: 
                                          return global_size, local_size                                                                                                                                 #     renderer/__init__.py          :    69: 

                                    if global_size is not None and local_size is None and all_int(self.p.global_size): # type: ignore[arg-type]                                                          #     engine/realize.py             :    91: 
                                    lra = {}                                                                                                                                                             #     engine/realize.py             :    97: 
                                    if global_size:                                                                                                                                                      #     engine/realize.py             :    98: L: {'lra': {}}    
                                    if local_size:                                                                                                                                                       #     engine/realize.py             :   101: 
                                    return self.clprg(*[x._buf for x in rawbufs], **lra, vals=tuple(var_vals[k] for k in self.p.vars), wait=wait)                                                        #     engine/realize.py             :   104: 

                                      class ClangProgram:                                                                                                                                                #     runtime/ops_clang.py          :    14: 
                                        def __call__(self, *bufs, vals=(), wait=False): return cpu_time_execution(lambda: self.fxn(*bufs, *vals), enable=wait)                                           #     runtime/ops_clang.py          :    23: 

                                          def cpu_time_execution(cb, enable):                                                                                                                            #     helpers.py                    :   289: 
                                            if enable: st = time.perf_counter()                                                                                                                          #     helpers.py                    :   290: L: {'cb': <function ClangProgram.__call__.<locals>.<lambda> at 0x78971f29de10>, 'enable': False}    
                                            cb()                                                                                                                                                         #     helpers.py                    :   291: 
                                            if enable: return time.perf_counter()-st                                                                                                                     #     helpers.py                    :   292: 

                        return self                                                                                                                                                                      #     tensor.py                     :   209: L: {'do_update_stats': True}    

                  buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)                                                                                                                       #     tensor.py                     :   250: L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>)> on CLANG with grad None>, 'cpu': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>)> on CLANG with grad None>}    
                  if self.device != "CLANG": buf.options = BufferOptions(nolru=True)                                                                                                                     #     tensor.py                     :   251: L: {'buf': <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>}    
                  return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)                                                                                                        #     tensor.py                     :   252: 

                    class _MallocAllocator(LRUAllocator):                                                                                                                                                #     device.py                     :   163: 
                      def copyout(self, dest:memoryview, src): ctypes.memmove(from_mv(dest), src, len(dest))                                                                                             #     device.py                     :   167: 

