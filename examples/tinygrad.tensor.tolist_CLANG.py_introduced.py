from tinygrad.tensor import Tensor                                                                                                                                                                       # ...grad.tensor.tolist_CLANG.py:     1    G: {'__name__': '__main__', '__doc__': None, '__package__': '', '__loader__': None, '__spec__': None, '__file__': 'examples/tinygrad.tensor.tolist_CLANG.py', '__cached__': None}     L: {}
from tinygrad.tensor import Tensor                            # noqa: F401                                                                                                                               # __init__.py                   :     1    G: {'__name__': 'tinygrad', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f3d0>, '__spec__': ModuleSpec(name='tinygrad', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f3d0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/__init__.cpython-310.pyc'}     L: {}
from __future__ import annotations                                                                                                                                                                       # tensor.py                     :     2    G: {'__name__': 'tinygrad.tensor', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f520>, '__spec__': ModuleSpec(name='tinygrad.tensor', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f520>, origin='/home/lorinbaum/code/tinygrad/tinygrad/tensor.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/tensor.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/tensor.cpython-310.pyc'}     L: {}
import dataclasses                                                                                                                                                                                       # tensor.py                     :     3    G: {}     L: {}
import time, math, itertools, functools, struct, sys, inspect                                                                                                                                            # tensor.py                     :     4    G: {}     L: {}
from contextlib import ContextDecorator                                                                                                                                                                  # tensor.py                     :     5    G: {}     L: {}
from typing import List, Tuple, Callable, Optional, ClassVar, Type, Union, Sequence, Dict, DefaultDict, cast, get_args, Set                                                                              # tensor.py                     :     6    G: {}     L: {}
from collections import defaultdict                                                                                                                                                                      # tensor.py                     :     7    G: {}     L: {}
import numpy as np                                                                                                                                                                                       # tensor.py                     :     8    G: {}     L: {}
from tinygrad.dtype import DType, DTypeLike, dtypes, ImageDType, ConstType, least_upper_float, least_upper_dtype, sum_acc_dtype, to_dtype                                                                # tensor.py                     :    10    G: {}     L: {}
from typing import Final, Optional, ClassVar, Set, Tuple, Dict, Union                                                                                                                                    # dtype.py                      :     1    G: {'__name__': 'tinygrad.dtype', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929b0421d0>, '__spec__': ModuleSpec(name='tinygrad.dtype', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929b0421d0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/dtype.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/dtype.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/dtype.cpython-310.pyc'}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # dtype.py                      :     2    G: {}     L: {}
import functools                                                                                                                                                                                         # dtype.py                      :     3    G: {}     L: {}
from tinygrad.helpers import getenv                                                                                                                                                                      # dtype.py                      :     4    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # helpers.py                    :     1    G: {'__name__': 'tinygrad.helpers', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929b0431f0>, '__spec__': ModuleSpec(name='tinygrad.helpers', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929b0431f0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/helpers.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/helpers.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/helpers.cpython-310.pyc'}     L: {}
import os, functools, platform, time, re, contextlib, operator, hashlib, pickle, sqlite3, tempfile, pathlib, string, ctypes, sys                                                                         # helpers.py                    :     2    G: {}     L: {}
import itertools, urllib.request, subprocess, shutil, math, json, contextvars                                                                                                                            # helpers.py                    :     3    G: {}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # helpers.py                    :     4    G: {}     L: {}
from typing import Dict, Tuple, Union, List, ClassVar, Optional, Iterable, Any, TypeVar, TYPE_CHECKING, Callable, Sequence                                                                               # helpers.py                    :     5    G: {}     L: {}
if TYPE_CHECKING:  # TODO: remove this and import TypeGuard from typing once minimum python supported version is 3.10                                                                                    # helpers.py                    :     6    G: {}     L: {}
T = TypeVar("T")                                                                                                                                                                                         # helpers.py                    :    10    G: {}     L: {}
U = TypeVar("U")                                                                                                                                                                                         # helpers.py                    :    11    G: {}     L: {}
OSX = platform.system() == "Darwin"                                                                                                                                                                      # helpers.py                    :    16    G: {}     L: {}
CI = os.getenv("CI", "") != ""                                                                                                                                                                           # helpers.py                    :    17    G: {}     L: {}

class Context(contextlib.ContextDecorator):                                                                                                                                                              # helpers.py                    :    82    G: {}     L: {}
  stack: ClassVar[List[dict[str, int]]] = [{}]                                                                                                                                                           # helpers.py                    :    83    G: {}     L: {}

class ContextVar:                                                                                                                                                                                        # helpers.py                    :    92    G: {}     L: {}
  _cache: ClassVar[Dict[str, ContextVar]] = {}                                                                                                                                                           # helpers.py                    :    93    G: {}     L: {}
  value: int                                                                                                                                                                                             # helpers.py                    :    94    G: {}     L: {}
  key: str                                                                                                                                                                                               # helpers.py                    :    95    G: {}     L: {}

DEBUG, IMAGE, BEAM, NOOPT, JIT = ContextVar("DEBUG", 0), ContextVar("IMAGE", 0), ContextVar("BEAM", 0), ContextVar("NOOPT", 0), ContextVar("JIT", 1)                                                     # helpers.py                    :   106    G: {}     L: {}

  class ContextVar:                                                                                                                                                                                      # helpers.py                    :    92    G: {}     L: {}
    def __new__(cls, key, default_value):                                                                                                                                                                # helpers.py                    :    96    G: {}     L: {}
      if key in ContextVar._cache: return ContextVar._cache[key]                                                                                                                                         # helpers.py                    :    97    G: {}     L: {'cls': <class 'tinygrad.helpers.ContextVar'>, 'key': 'DEBUG', 'default_value': 0, '__class__': <class 'tinygrad.helpers.ContextVar'>}
      instance = ContextVar._cache[key] = super().__new__(cls)                                                                                                                                           # helpers.py                    :    98    G: {}     L: {}
      instance.value, instance.key = getenv(key, default_value), key                                                                                                                                     # helpers.py                    :    99    G: {}     L: {}

        @functools.lru_cache(maxsize=None)                                                                                                                                                               # helpers.py                    :    79    G: {}     L: {}
        def getenv(key:str, default=0): return type(default)(os.getenv(key, default))

      return instance                                                                                                                                                                                    # helpers.py                    :   100    G: {}     L: {'cls': <class 'tinygrad.helpers.ContextVar'>, 'default_value': 0, '__class__': <class 'tinygrad.helpers.ContextVar'>, 'instance': <tinygrad.helpers.ContextVar object at 0x7c929a10fcd0>}

WINO, THREEFRY, CAPTURING, TRACEMETA = ContextVar("WINO", 0), ContextVar("THREEFRY", 0), ContextVar("CAPTURING", 1), ContextVar("TRACEMETA", 1)                                                          # helpers.py                    :   107    G: {}     L: {}

GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING = ContextVar("GRAPH", 0), getenv("GRAPHPATH", "/tmp/net"), ContextVar("SAVE_SCHEDULE", 0), ContextVar("RING", 1)                                                   # helpers.py                    :   108    G: {}     L: {}

MULTIOUTPUT, PROFILE, PROFILEPATH = ContextVar("MULTIOUTPUT", 1), ContextVar("PROFILE", 0), ContextVar("PROFILEPATH", temp("tinygrad_profile.json"))                                                     # helpers.py                    :   109    G: {}     L: {}

  def temp(x:str) -> str: return (pathlib.Path(tempfile.gettempdir()) / x).as_posix()                                                                                                                    # helpers.py                    :    80    G: {}     L: {}

USE_TC, TC_OPT, TRANSCENDENTAL = ContextVar("TC", 1), ContextVar("TC_OPT", 0), ContextVar("TRANSCENDENTAL", 1)                                                                                           # helpers.py                    :   110    G: {}     L: {}

FUSE_ARANGE, FUSE_CONV_BW = ContextVar("FUSE_ARANGE", 0), ContextVar("FUSE_CONV_BW", 0)                                                                                                                  # helpers.py                    :   111    G: {}     L: {}

SPLIT_REDUCEOP, ARANGE_DIFF = ContextVar("SPLIT_REDUCEOP", 1), ContextVar("ARANGE_DIFF", 0)                                                                                                              # helpers.py                    :   112    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # helpers.py                    :   115    G: {}     L: {}
class Metadata:
  name: str                                                                                                                                                                                              # helpers.py                    :   116    G: {}     L: {'__module__': 'tinygrad.helpers', '__qualname__': 'Metadata'}
  caller: str                                                                                                                                                                                            # helpers.py                    :   117    G: {}     L: {}
  backward: bool = False                                                                                                                                                                                 # helpers.py                    :   118    G: {}     L: {}

_METADATA: contextvars.ContextVar[Optional[Metadata]] = contextvars.ContextVar("_METADATA", default=None)                                                                                                # helpers.py                    :   122    G: {}     L: {}

class GlobalCounters:                                                                                                                                                                                    # helpers.py                    :   126    G: {}     L: {}
  global_ops: ClassVar[int] = 0                                                                                                                                                                          # helpers.py                    :   127    G: {}     L: {}
  global_mem: ClassVar[int] = 0                                                                                                                                                                          # helpers.py                    :   128    G: {}     L: {}
  time_sum_s: ClassVar[float] = 0.0                                                                                                                                                                      # helpers.py                    :   129    G: {}     L: {}
  kernel_count: ClassVar[int] = 0                                                                                                                                                                        # helpers.py                    :   130    G: {}     L: {}
  mem_used: ClassVar[int] = 0   # NOTE: this is not reset                                                                                                                                                # helpers.py                    :   131    G: {}     L: {}

class ProfileLogger:                                                                                                                                                                                     # helpers.py                    :   165    G: {}     L: {}
  writers: int = 0                                                                                                                                                                                       # helpers.py                    :   166    G: {}     L: {}
  mjson: List[Dict] = []                                                                                                                                                                                 # helpers.py                    :   167    G: {}     L: {}
  actors: Dict[Union[str, Tuple[str, str]], int] = {}                                                                                                                                                    # helpers.py                    :   168    G: {}     L: {}

_cache_dir: str = getenv("XDG_CACHE_HOME", os.path.expanduser("~/Library/Caches" if OSX else "~/.cache"))                                                                                                # helpers.py                    :   205    G: {}     L: {}

CACHEDB: str = getenv("CACHEDB", os.path.abspath(os.path.join(_cache_dir, "tinygrad", "cache.db")))                                                                                                      # helpers.py                    :   206    G: {}     L: {}

CACHELEVEL = getenv("CACHELEVEL", 2)                                                                                                                                                                     # helpers.py                    :   207    G: {}     L: {}

VERSION = 16                                                                                                                                                                                             # helpers.py                    :   209    G: {}     L: {}
_db_connection = None                                                                                                                                                                                    # helpers.py                    :   210    G: {}     L: {}
_db_tables = set()                                                                                                                                                                                       # helpers.py                    :   239    G: {}     L: {}

ConstType = Union[float, int, bool]                                                                                                                                                                      # dtype.py                      :     6    G: {}     L: {}

@dataclass(frozen=True, order=True)                                                                                                                                                                      # dtype.py                      :     9    G: {}     L: {}
class DType:
  priority: int  # this determines when things get upcasted                                                                                                                                              # dtype.py                      :    10    G: {}     L: {'__module__': 'tinygrad.dtype', '__qualname__': 'DType', '__annotations__': {}}
  itemsize: int                                                                                                                                                                                          # dtype.py                      :    11    G: {}     L: {}
  name: str                                                                                                                                                                                              # dtype.py                      :    12    G: {}     L: {}
  fmt: Optional[str]                                                                                                                                                                                     # dtype.py                      :    13    G: {}     L: {}
  count: int                                                                                                                                                                                             # dtype.py                      :    14    G: {}     L: {}

# dependent typing?                                                                                                                                                                                      # dtype.py                      :    23    G: {}     L: {}
@dataclass(frozen=True, repr=False)
class ImageDType(DType):
  shape: Tuple[int, ...]   # arbitrary arg for the dtype, used in image for the shape                                                                                                                    # dtype.py                      :    24    G: {}     L: {'__module__': 'tinygrad.dtype', '__qualname__': 'ImageDType', '__annotations__': {}}
  base: DType                                                                                                                                                                                            # dtype.py                      :    25    G: {}     L: {}

class dtypes:                                                                                                                                                                                            # dtype.py                      :    38    G: {}     L: {}
  pyint: Final[DType] = DType(-1, 8, "pyint", None, 1)   # arbitrary precision integer, same itemsize to int64 so min/max works                                                                          # dtype.py                      :    69    G: {}     L: {}
  bool: Final[DType] = DType(0, 1, "bool", '?', 1)                                                                                                                                                       # dtype.py                      :    70    G: {}     L: {}
  int8: Final[DType] = DType(1, 1, "char", 'b', 1)                                                                                                                                                       # dtype.py                      :    71    G: {}     L: {}
  uint8: Final[DType] = DType(2, 1, "unsigned char", 'B', 1)                                                                                                                                             # dtype.py                      :    72    G: {}     L: {}
  int16: Final[DType] = DType(3, 2, "short", 'h', 1)                                                                                                                                                     # dtype.py                      :    73    G: {}     L: {}
  uint16: Final[DType] = DType(4, 2, "unsigned short", 'H', 1)                                                                                                                                           # dtype.py                      :    74    G: {}     L: {}
  int32: Final[DType] = DType(5, 4, "int", 'i', 1)                                                                                                                                                       # dtype.py                      :    75    G: {}     L: {}
  uint32: Final[DType] = DType(6, 4, "unsigned int", 'I', 1)                                                                                                                                             # dtype.py                      :    76    G: {}     L: {}
  int64: Final[DType] = DType(7, 8, "long", 'l', 1)                                                                                                                                                      # dtype.py                      :    77    G: {}     L: {}
  uint64: Final[DType] = DType(8, 8, "unsigned long", 'L', 1)                                                                                                                                            # dtype.py                      :    78    G: {}     L: {}
  float16: Final[DType] = DType(9, 2, "half", 'e', 1)                                                                                                                                                    # dtype.py                      :    79    G: {}     L: {}
  bfloat16: Final[DType] = DType(10, 2, "__bf16", None, 1)                                                                                                                                               # dtype.py                      :    81    G: {}     L: {}
  float32: Final[DType] = DType(11, 4, "float", 'f', 1)                                                                                                                                                  # dtype.py                      :    82    G: {}     L: {}
  float64: Final[DType] = DType(12, 8, "double", 'd', 1)                                                                                                                                                 # dtype.py                      :    83    G: {}     L: {}
  half = float16; float = float32; double = float64 # noqa: E702                                                                                                                                         # dtype.py                      :    86    G: {}     L: {}
  uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 # noqa: E702                                                                                                                             # dtype.py                      :    87    G: {}     L: {}
  char = int8; short = int16; int = int32; long = int64 # noqa: E702                                                                                                                                     # dtype.py                      :    88    G: {}     L: {}
  default_float: ClassVar[DType] = float32                                                                                                                                                               # dtype.py                      :    96    G: {}     L: {}
  default_int: ClassVar[DType] = int32                                                                                                                                                                   # dtype.py                      :    97    G: {}     L: {}

if (env_default_float := getenv("DEFAULT_FLOAT", "")):                                                                                                                                                   # dtype.py                      :    99    G: {}     L: {}

DTypeLike = Union[str, DType]                                                                                                                                                                            # dtype.py                      :   103    G: {}     L: {}
promo_lattice = { dtypes.bool: [dtypes.int8, dtypes.uint8], dtypes.int8: [dtypes.int16], dtypes.int16: [dtypes.int32], dtypes.int32: [dtypes.int64],                                                     # dtype.py                      :   108    G: {}     L: {}
  dtypes.int64: [dtypes.float16, dtypes.bfloat16], dtypes.uint8: [dtypes.int16, dtypes.uint16], dtypes.uint16: [dtypes.int32, dtypes.uint32],
  dtypes.uint32: [dtypes.int64, dtypes.uint64], dtypes.uint64: [dtypes.float16, dtypes.bfloat16],
  dtypes.float16: [dtypes.float32], dtypes.bfloat16: [dtypes.float32], dtypes.float32: [dtypes.float64], }
DTYPES_DICT = {k: v for k, v in dtypes.__dict__.items() if not (k.startswith(('__', 'default', 'pyint')) or v.__class__ is staticmethod)}                                                                # dtype.py                      :   122    G: {}     L: {}
INVERSE_DTYPES_DICT = {v.name:k for k,v in DTYPES_DICT.items()}                                                                                                                                          # dtype.py                      :   123    G: {}     L: {}
INVERSE_DTYPES_DICT['pyint'] = 'pyint'                                                                                                                                                                   # dtype.py                      :   124    G: {}     L: {}
from tinygrad.helpers import argfix, make_pair, flatten, prod, all_int, round_up, merge_dicts, argsort, getenv, get_shape, fully_flatten, dedup                                                          # tensor.py                     :    11    G: {}     L: {}
from tinygrad.helpers import IMAGE, DEBUG, WINO, THREEFRY, _METADATA, Metadata, TRACEMETA                                                                                                                # tensor.py                     :    12    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # tensor.py                     :    13    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # lazy.py                       :     1    G: {'__name__': 'tinygrad.lazy', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a13ef80>, '__spec__': ModuleSpec(name='tinygrad.lazy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a13ef80>, origin='/home/lorinbaum/code/tinygrad/tinygrad/lazy.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/lazy.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/lazy.cpython-310.pyc'}     L: {}
from typing import Union, Optional, Any, Tuple, List, get_args                                                                                                                                           # lazy.py                       :     2    G: {}     L: {}
from tinygrad.dtype import dtypes, DType, DTypeLike, ConstType, to_dtype                                                                                                                                 # lazy.py                       :     3    G: {}     L: {}
from tinygrad.helpers import prod, getenv, all_int, all_same, DEBUG, _METADATA, Metadata, SPLIT_REDUCEOP                                                                                                 # lazy.py                       :     4    G: {}     L: {}
from tinygrad.ops import MetaOps, UnaryOps, BinaryOps, TernaryOps, ReduceOps, Op, exec_alu, python_alu                                                                                                   # lazy.py                       :     5    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # ops.py                        :     1    G: {'__name__': 'tinygrad.ops', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a13fd90>, '__spec__': ModuleSpec(name='tinygrad.ops', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a13fd90>, origin='/home/lorinbaum/code/tinygrad/tinygrad/ops.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/ops.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/ops.cpython-310.pyc'}     L: {}
from collections import defaultdict                                                                                                                                                                      # ops.py                        :     2    G: {}     L: {}
from typing import Any, DefaultDict, List, Optional, Set, Union, Tuple, Dict, Callable, cast, TYPE_CHECKING                                                                                              # ops.py                        :     3    G: {}     L: {}
import math, operator, ctypes, struct, functools, hashlib, itertools                                                                                                                                     # ops.py                        :     4    G: {}     L: {}
from enum import Enum, auto                                                                                                                                                                              # ops.py                        :     5    G: {}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # ops.py                        :     6    G: {}     L: {}
from tinygrad.dtype import ConstType, ImageDType, PtrDType, dtypes, DType                                                                                                                                # ops.py                        :     7    G: {}     L: {}
from tinygrad.helpers import merge_dicts, pretty_print, prod                                                                                                                                             # ops.py                        :     8    G: {}     L: {}
from tinygrad.shape.symbolic import Variable, sint                                                                                                                                                       # ops.py                        :     9    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # shape/symbolic.py             :     1    G: {'__name__': 'tinygrad.shape.symbolic', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a13d120>, '__spec__': ModuleSpec(name='tinygrad.shape.symbolic', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a13d120>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/symbolic.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/symbolic.cpython-310.pyc'}     L: {}
import functools                                                                                                                                                                                         # shape/symbolic.py             :     2    G: {}     L: {}
from math import gcd                                                                                                                                                                                     # shape/symbolic.py             :     3    G: {}     L: {}
from tinygrad.helpers import partition                                                                                                                                                                   # shape/symbolic.py             :     4    G: {}     L: {}
from typing import List, Dict, Callable, Tuple, Type, Union, Optional, Any, Set, Mapping                                                                                                                 # shape/symbolic.py             :     5    G: {}     L: {}

class Node:                                                                                                                                                                                              # shape/symbolic.py             :    10    G: {}     L: {}
  b: Union[Node, int]                                                                                                                                                                                    # shape/symbolic.py             :    11    G: {}     L: {}
  min: int                                                                                                                                                                                               # shape/symbolic.py             :    12    G: {}     L: {}
  max: sint                                                                                                                                                                                              # shape/symbolic.py             :    13    G: {}     L: {}

sint = Union[int, Variable, MulNode, SumNode]                                                                                                                                                            # shape/symbolic.py             :   304    G: {}     L: {}
render_python: Dict[Type, Callable[..., str]] = {                                                                                                                                                        # shape/symbolic.py             :   312    G: {}     L: {}
  Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
    else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
    else f"{self.expr}"),
  NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
  MulNode: render_mulnode,
  DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
  ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
  LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
  SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
  AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
}
if TYPE_CHECKING:                                                                                                                                                                                        # ops.py                        :    10    G: {}     L: {}

# these are the llops your accelerator must implement, along with toCpu                                                                                                                                  # ops.py                        :    17    G: {}     L: {}
# the Enum class doesn't work with mypy, this is static. sorry it's ugly
# NOTE: MOD, CMPLT don't have to be implemented on vectors, just scalars
# NOTE: many GPUs don't have DIV, but UnaryOps.RECIP doesn't work for integer division
class UnaryOps(Enum):
  """A -> A (elementwise)"""                                                                                                                                                                             # ops.py                        :    18    G: {}     L: {}
  EXP2 = auto(); LOG2 = auto(); CAST = auto(); BITCAST = auto(); SIN = auto(); SQRT = auto(); NEG = auto(); RECIP = auto() # noqa: E702                                                                  # ops.py                        :    19    G: {}     L: {}

class BinaryOps(Enum):                                                                                                                                                                                   # ops.py                        :    20    G: {}     L: {}
  """A + A -> A (elementwise)"""                                                                                                                                                                         # ops.py                        :    21    G: {}     L: {}
  ADD = auto(); MUL = auto(); IDIV = auto(); MAX = auto(); MOD = auto(); CMPLT = auto(); CMPNE = auto(); XOR = auto() # noqa: E702                                                                       # ops.py                        :    22    G: {}     L: {}
  SHL = auto(); SHR = auto(); OR = auto(); AND = auto(); THREEFRY = auto() # noqa: E702                                                                                                                  # ops.py                        :    23    G: {}     L: {}

class TernaryOps(Enum):                                                                                                                                                                                  # ops.py                        :    24    G: {}     L: {}
  """A + A + A -> A (elementwise)"""                                                                                                                                                                     # ops.py                        :    25    G: {}     L: {}
  WHERE = auto(); MULACC = auto() # noqa: E702                                                                                                                                                           # ops.py                        :    26    G: {}     L: {}

class ReduceOps(Enum):                                                                                                                                                                                   # ops.py                        :    27    G: {}     L: {}
  """A -> B (reduce)"""                                                                                                                                                                                  # ops.py                        :    28    G: {}     L: {}
  SUM = auto(); MAX = auto(); WMMA = auto() # noqa: E702                                                                                                                                                 # ops.py                        :    29    G: {}     L: {}

class MetaOps(Enum):                                                                                                                                                                                     # ops.py                        :    30    G: {}     L: {}
  EMPTY = auto(); CONST = auto(); COPY = auto(); CONTIGUOUS = auto(); CUSTOM = auto(); ASSIGN = auto(); VIEW = auto() # noqa: E702                                                                       # ops.py                        :    31    G: {}     L: {}

Op = Union[UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps]                                                                                                                                          # ops.py                        :    32    G: {}     L: {}
UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}                                                                                                                          # ops.py                        :    35    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # ops.py                        :    38    G: {}     L: {}
class KernelInfo:
  local_dims: int = 0           # number of local dimensions  (this is remapping RANGE to SPECIAL)                                                                                                       # ops.py                        :    39    G: {}     L: {'__module__': 'tinygrad.ops', '__qualname__': 'KernelInfo'}
  upcasted: int = 0             # count that are upcasted     (this is remapping RANGE to EXPAND)                                                                                                        # ops.py                        :    40    G: {}     L: {}
  dont_use_locals: bool = False # don't use local indexing                                                                                                                                               # ops.py                        :    41    G: {}     L: {}

python_alu: Dict[Op, Callable]  = {                                                                                                                                                                      # ops.py                        :    51    G: {}     L: {}
  UnaryOps.LOG2: lambda x: math.log2(x) if x > 0 else -math.inf if x == 0 else math.nan, UnaryOps.EXP2: hook_overflow(math.inf, lambda x: 2**x),
  UnaryOps.SQRT: lambda x: math.sqrt(x) if x >= 0 else math.nan, UnaryOps.RECIP: lambda x: 1/x if x != 0 else math.copysign(math.inf, x),
  UnaryOps.SIN: lambda x: math.sin(x) if not math.isinf(x) else math.nan, UnaryOps.NEG: lambda x: (not x) if isinstance(x, bool) else -x,
  BinaryOps.SHR: operator.rshift, BinaryOps.SHL: operator.lshift, BinaryOps.MUL: operator.mul, BinaryOps.ADD: operator.add,
  BinaryOps.XOR: operator.xor, BinaryOps.MAX: max, BinaryOps.CMPNE: operator.ne, BinaryOps.CMPLT: operator.lt,
  BinaryOps.OR: operator.or_, BinaryOps.AND: operator.and_,
  BinaryOps.MOD: lambda x,y: abs(int(x))%abs(int(y))*(1,-1)[x<0], BinaryOps.IDIV: lambda x,y: abs(x)//abs(y)*(1,-1)[x*y<0] if y != 0 else x*math.inf,
  TernaryOps.MULACC: lambda x,y,z: (x*y)+z, TernaryOps.WHERE: lambda x,y,z: y if x else z}

  def hook_overflow(dv, fxn):                                                                                                                                                                            # ops.py                        :    45    G: {}     L: {}
    return wfxn                                                                                                                                                                                          # ops.py                        :    49    G: {}     L: {}

truncate: Dict[DType, Callable] = {dtypes.bool: bool,                                                                                                                                                    # ops.py                        :    68    G: {}     L: {}
  # TODO: bfloat16
  dtypes.float16: truncate_fp16, dtypes.float32: lambda x: ctypes.c_float(x).value, dtypes.float64: lambda x: ctypes.c_double(x).value,
  dtypes.uint8: lambda x: ctypes.c_uint8(x).value, dtypes.uint16: lambda x: ctypes.c_uint16(x).value,
  dtypes.uint32: lambda x: ctypes.c_uint32(x).value, dtypes.uint64: lambda x: ctypes.c_uint64(x).value,
  dtypes.int8: lambda x: ctypes.c_int8(x).value, dtypes.int16: lambda x: ctypes.c_int16(x).value, dtypes.int32: lambda x: ctypes.c_int32(x).value \
      if isinstance(x,int) else x, dtypes.int64: lambda x: ctypes.c_int64(x).value}

# the order of these UOps controls the order of the toposort                                                                                                                                             # ops.py                        :    79    G: {}     L: {}
class UOps(Enum):
  SINK = auto(); EXT = auto(); EXPAND = auto(); CONTRACT = auto(); SHAPETRACKER = auto()  # noqa: E702                                                                                                   # ops.py                        :    81    G: {}     L: {}
  DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702                                                                                                   # ops.py                        :    82    G: {}     L: {}
  CONST = auto(); SPECIAL = auto() # noqa: E702                                                                                                                                                          # ops.py                        :    83    G: {}     L: {}
  NOOP = auto(); GEP = auto() # noqa: E702                                                                                                                                                               # ops.py                        :    84    G: {}     L: {}
  CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702                                                                                                                                       # ops.py                        :    86    G: {}     L: {}
  ALU = auto(); REDUCE = auto(); REDUCE_AXIS = auto(); WMMA = auto() # noqa: E702                                                                                                                        # ops.py                        :    87    G: {}     L: {}
  LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702                                                                                                                                               # ops.py                        :    89    G: {}     L: {}
  BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702                                                                                                                                             # ops.py                        :    91    G: {}     L: {}
  ENDRANGE = auto(); ENDIF = auto() # noqa: E702                                                                                                                                                         # ops.py                        :    93    G: {}     L: {}

BUFFER_UOPS = {UOps.LOAD, UOps.STORE, UOps.CONST}                                                                                                                                                        # ops.py                        :    95    G: {}     L: {}
END_FOR_UOP = {UOps.IF:(UOps.STORE, UOps.ENDIF), UOps.RANGE:(UOps.PHI, UOps.ENDRANGE)}                                                                                                                   # ops.py                        :    97    G: {}     L: {}

@dataclass(frozen=True, eq=False)                                                                                                                                                                        # ops.py                        :   100    G: {}     L: {}
class UOp:
  op: UOps                                                                                                                                                                                               # ops.py                        :   101    G: {}     L: {'__module__': 'tinygrad.ops', '__qualname__': 'UOp'}
  dtype: Optional[DType] = None                                                                                                                                                                          # ops.py                        :   102    G: {}     L: {}
  src: Tuple[UOp, ...] = tuple()                                                                                                                                                                         # ops.py                        :   103    G: {}     L: {}
  arg: Any = None                                                                                                                                                                                        # ops.py                        :   104    G: {}     L: {}

@dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                               # ops.py                        :   229    G: {}     L: {}
class NOp(UOp):
  name:Optional[str] = None                                                                                                                                                                              # ops.py                        :   230    G: {}     L: {'__module__': 'tinygrad.ops', '__qualname__': 'NOp'}
  src:Tuple[NOp, ...] = tuple()                                                                                                                                                                          # ops.py                        :   231    G: {}     L: {}
  allow_any_len:bool = False                                                                                                                                                                             # ops.py                        :   232    G: {}     L: {}

from tinygrad.shape.symbolic import sint, Variable                                                                                                                                                       # lazy.py                       :     6    G: {}     L: {}
from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                                     # lazy.py                       :     7    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # shape/shapetracker.py         :     2    G: {'__name__': 'tinygrad.shape.shapetracker', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a13fac0>, '__spec__': ModuleSpec(name='tinygrad.shape.shapetracker', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a13fac0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/shapetracker.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/shapetracker.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/shapetracker.cpython-310.pyc'}     L: {}
import functools                                                                                                                                                                                         # shape/shapetracker.py         :     3    G: {}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # shape/shapetracker.py         :     4    G: {}     L: {}
from typing import Tuple, List, Optional, Dict, Set, Iterable, cast, Any                                                                                                                                 # shape/shapetracker.py         :     5    G: {}     L: {}
from tinygrad.helpers import merge_dicts, getenv                                                                                                                                                         # shape/shapetracker.py         :     6    G: {}     L: {}
from tinygrad.shape.symbolic import Variable, MulNode, Node, SumNode, NumNode, DivNode, ModNode, LtNode, AndNode, sint                                                                                   # shape/shapetracker.py         :     7    G: {}     L: {}
from tinygrad.shape.view import View, strides_for_shape                                                                                                                                                  # shape/shapetracker.py         :     8    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # shape/view.py                 :     1    G: {'__name__': 'tinygrad.shape.view', '__doc__': None, '__package__': 'tinygrad.shape', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a1a38e0>, '__spec__': ModuleSpec(name='tinygrad.shape.view', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a1a38e0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/shape/view.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/view.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/shape/__pycache__/view.cpython-310.pyc'}     L: {}
import functools, operator, itertools, math                                                                                                                                                              # shape/view.py                 :     2    G: {}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # shape/view.py                 :     3    G: {}     L: {}
from typing import Tuple, List, Optional, Dict, Set, cast                                                                                                                                                # shape/view.py                 :     4    G: {}     L: {}
from tinygrad.helpers import prod, all_int, argsort                                                                                                                                                      # shape/view.py                 :     5    G: {}     L: {}
from tinygrad.shape.symbolic import Node, NumNode, Variable, sint, sym_infer, create_lt_node, create_ge_node                                                                                             # shape/view.py                 :     6    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # shape/view.py                 :    85    G: {}     L: {}
class View:
  shape:Tuple[sint, ...]                                                                                                                                                                                 # shape/view.py                 :    86    G: {}     L: {'__module__': 'tinygrad.shape.view', '__qualname__': 'View', '__annotations__': {}}
  strides:Tuple[sint, ...]                                                                                                                                                                               # shape/view.py                 :    87    G: {}     L: {}
  offset:sint                                                                                                                                                                                            # shape/view.py                 :    88    G: {}     L: {}
  mask:Optional[Tuple[Tuple[sint, sint], ...]]                                                                                                                                                           # shape/view.py                 :    89    G: {}     L: {}
  contiguous:bool                                                                                                                                                                                        # shape/view.py                 :    90    G: {}     L: {}

from tinygrad.dtype import dtypes                                                                                                                                                                        # shape/shapetracker.py         :     9    G: {}     L: {}
from tinygrad.ops import UOp, UOps                                                                                                                                                                       # shape/shapetracker.py         :    10    G: {}     L: {}
from tinygrad.codegen.uopgraph import graph_rewrite                                                                                                                                                      # shape/shapetracker.py         :    11    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # codegen/uopgraph.py           :     1    G: {'__name__': 'tinygrad.codegen.uopgraph', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a1d9180>, '__spec__': ModuleSpec(name='tinygrad.codegen.uopgraph', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a1d9180>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/uopgraph.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/uopgraph.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/uopgraph.cpython-310.pyc'}     L: {}
from typing import Optional, Tuple, Dict, List, Set, Union, cast, TYPE_CHECKING, Any, DefaultDict, Callable                                                                                              # codegen/uopgraph.py           :     2    G: {}     L: {}
import functools, itertools, heapq, math, operator                                                                                                                                                       # codegen/uopgraph.py           :     3    G: {}     L: {}
from collections import defaultdict                                                                                                                                                                      # codegen/uopgraph.py           :     4    G: {}     L: {}
from tinygrad.dtype import dtypes, PtrDType, ImageDType, DType                                                                                                                                           # codegen/uopgraph.py           :     5    G: {}     L: {}
from tinygrad.ops import UnaryOps, BinaryOps, exec_alu, UOp, NOp, UOps, UPat, PatternMatcher, END_FOR_UOP, type_verify, print_uops                                                                       # codegen/uopgraph.py           :     6    G: {}     L: {}
from tinygrad.helpers import DEBUG, getenv, flatten, dedup, TRANSCENDENTAL, prod, CI, all_same, partition                                                                                                # codegen/uopgraph.py           :     7    G: {}     L: {}
from tinygrad.codegen.transcendental import xexp2, xlog2, xsin, TRANSCENDENTAL_SUPPORTED_DTYPES                                                                                                          # codegen/uopgraph.py           :     8    G: {}     L: {}
import math, functools                                                                                                                                                                                   # codegen/transcendental.py     :     1    G: {'__name__': 'tinygrad.codegen.transcendental', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a1db0a0>, '__spec__': ModuleSpec(name='tinygrad.codegen.transcendental', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a1db0a0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/transcendental.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/transcendental.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/transcendental.cpython-310.pyc'}     L: {}
from typing import Tuple, List                                                                                                                                                                           # codegen/transcendental.py     :     2    G: {}     L: {}
from tinygrad.dtype import dtypes, DType                                                                                                                                                                 # codegen/transcendental.py     :     3    G: {}     L: {}
from tinygrad.ops import UOp                                                                                                                                                                             # codegen/transcendental.py     :     4    G: {}     L: {}
TRANSCENDENTAL_SUPPORTED_DTYPES = {dtypes.float16, dtypes.float32, dtypes.float64}                                                                                                                       # codegen/transcendental.py     :     6    G: {}     L: {}
if TYPE_CHECKING: from tinygrad.renderer import Renderer                                                                                                                                                 # codegen/uopgraph.py           :     9    G: {}     L: {}
float4_folding = PatternMatcher([                                                                                                                                                                        # codegen/uopgraph.py           :    83    G: {}     L: {}
  (UPat(UOps.EXPAND, src=UPat(UOps.LOAD, src=(UPat(name="buf"), UPat()), allow_any_len=True), name="ex"), fold_expanded),
  (UPat({UOps.BARRIER, UOps.SINK}, src=UPat(UOps.STORE, src=(UPat(name="buf"), UPat(), UPat()), allow_any_len=True), name="ex"), fold_expanded),
  (UPat(UOps.VECTORIZE, src=UPat(UOps.REDUCE), name="vec"), vectorize_reduce),
  (UPat(UOps.VECTORIZE, src=UPat({UOps.ALU, UOps.CAST, UOps.BITCAST}), name="vec"), vectorize_alu),
])

  class UPat:                                                                                                                                                                                            # ops.py                        :   243    G: {}     L: {}
    def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,                                                             # ops.py                        :   244    G: {}     L: {}
                 name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
      self.op: Optional[Tuple[UOps, ...]] = None if op is None else (tuple(op) if isinstance(op, set) else (op,))                                                                                        # ops.py                        :   246    G: {}     L: repr failed
      self.dtype: Optional[Tuple[DType, ...]] = None if dtype is None else (tuple(dtype) if isinstance(dtype, set) else (dtype,))                                                                        # ops.py                        :   247    G: {}     L: {}
      self.arg, self.name = arg, name                                                                                                                                                                    # ops.py                        :   248    G: {}     L: {}
      self.src: Any = None                                                                                                                                                                               # ops.py                        :   249    G: {}     L: {}
      if isinstance(src, list): self.src = list(itertools.permutations(src))                                                                                                                             # ops.py                        :   251    G: {}     L: {}
      elif isinstance(src, tuple): self.src = [src]                                                                                                                                                      # ops.py                        :   253    G: {}     L: {}
      elif isinstance(src, UPat): self.src = [itertools.repeat(src)]                                                                                                                                     # ops.py                        :   255    G: {}     L: {}
      self.allowed_len: int = 0 if allow_any_len or isinstance(src, UPat) or src is None else len(src)                                                                                                   # ops.py                        :   257    G: {}     L: {}

  class PatternMatcher:                                                                                                                                                                                  # ops.py                        :   280    G: {}     L: {}
    def __init__(self, patterns:List[Tuple[Union[UPat, NOp], Callable]]):                                                                                                                                # ops.py                        :   281    G: {}     L: {}
      self.patterns = patterns                                                                                                                                                                           # ops.py                        :   282    G: {}     L: {'self': <tinygrad.ops.PatternMatcher object at 0x7c929a1dbdf0>, 'patterns': [(UPat((UOps.EXPAND), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.LOAD), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),)), <function fold_expanded at 0x7c9299706200>), (UPat((UOps.BARRIER, UOps.SINK), None, name='ex', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.STORE), None, name=None, dtype=None, allow_any_len=True, src=(\n    UPat(None, None, name='buf', dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),\n    UPat(None, None, name=None, dtype=None, allow_any_len=True, src=(None)),)),)), <function fold_expanded at 0x7c9299706200>), (UPat((UOps.VECTORIZE), None, name='vec', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.REDUCE), None, name=None, dtype=None, allow_any_len=True, src=(None)),)), <function vectorize_reduce at 0x7c929979ca60>), (UPat((UOps.VECTORIZE), None, name='vec', dtype=None, allow_any_len=True, src=(\n  UPat((UOps.CAST, UOps.BITCAST, UOps.ALU), None, name=None, dtype=None, allow_any_len=True, src=(None)),)), <function vectorize_alu at 0x7c929979caf0>)]}
      self.pdict: DefaultDict[Tuple[UOps, Any], List[Tuple[UPat, Callable]]] = defaultdict(list)                                                                                                         # ops.py                        :   283    G: {}     L: {}
      for p,fxn in self.patterns:                                                                                                                                                                        # ops.py                        :   285    G: {}     L: {}
        if isinstance(p, NOp): p = p.compile()                                                                                                                                                           # ops.py                        :   286    G: {}     L: {}
        assert p.op is not None                                                                                                                                                                          # ops.py                        :   287    G: {}     L: {}
        for uop in p.op: self.pdict[(uop, p.arg)].append((p, fxn))                                                                                                                                       # ops.py                        :   288    G: {}     L: {}

constant_folder = PatternMatcher([                                                                                                                                                                       # codegen/uopgraph.py           :   204    G: {}     L: {}
  # bool ADD is OR, MUL is AND. prevents other rules to rewrite bool ADD/MUL incorrectly
  (UPat(UOps.ALU, BinaryOps.ADD, dtype=dtypes.bool, name="x"), lambda x: UOp(x.op, x.dtype, x.src, BinaryOps.OR)),
  (UPat(UOps.ALU, BinaryOps.MUL, dtype=dtypes.bool, name="x"), lambda x: UOp(x.op, x.dtype, x.src, BinaryOps.AND)),
  # VECTORIZE/GEP
  (NOp(UOps.GEP, src=(NOp(UOps.VECTORIZE, name="cast"),), name="gep"), lambda gep, cast: cast.src[gep.arg]),
  *[(NOp(UOps.VECTORIZE, dtypes.float.vec(i), tuple(NOp(UOps.GEP, dtypes.float,
                         src=(NOp.var('x', dtype=dtypes.float.vec(i)),), arg=j) for j in range(i))), lambda x: x) for i in [2, 4, 8, 16]],
  *[(NOp(UOps.VECTORIZE, dtypes.half.vec(i), tuple(NOp(UOps.GEP, dtypes.half,
                         src=(NOp.var('x', dtype=dtypes.half.vec(i)),), arg=j) for j in range(i))), lambda x: x) for i in [2, 4, 8, 16]],
  # tensor core with a 0 input is acc
  *[(NOp(UOps.WMMA, src=(NOp(UOps.VECTORIZE, src=tuple(NOp.const(None, 0.0) for _ in range(i))), NOp.var(), NOp.var('acc'))),
     lambda acc: acc) for i in [2, 4, 8]],
  *[(NOp(UOps.WMMA, src=(NOp.var(), NOp(UOps.VECTORIZE, src=tuple(NOp.const(None, 0.0) for _ in range(i))), NOp.var('acc'))),
     lambda acc: acc) for i in [2, 4, 8]],
  # tensor core cleanups
  *[(NOp(UOps.REDUCE, src=(NOp(UOps.EXPAND, src=tuple(NOp(UOps.GEP, dtypes.float, src=(NOp.var('x'),), arg=i) for i in range(j)), name="expand"),)
    ,name="reduce", allow_any_len=True), reduce_before_expand) for j in [2,4,8]],
  (NOp.var("add") + NOp(UOps.WMMA, name="wmma"),
    lambda add, wmma: UOp(wmma.op, wmma.dtype, (wmma.src[0], wmma.src[1], wmma.src[2]+add), wmma.arg)),
  # threefry
  (NOp(UOps.ALU, dtype=dtypes.uint64, src=(NOp.var("x"), NOp.var("seed")), arg=BinaryOps.THREEFRY), threefry2x32),
  # extra arange loop folding because we don't fold adds. TODO: fold adds
  (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng") +
                          NOp.var("idx2") + NOp.var("idx3"))
   .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),
  (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng") +
                          NOp.var("idx2"))
   .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),
  # arange loop folding (reduce)
  (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))
   .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),
  (NOp(UOps.REDUCE, src=((NOp.var("idx") - NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))
   .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)),), arg=BinaryOps.ADD, name="reduce", allow_any_len=True),
   lambda **kwargs: loop_collapse(mval=UOp.const(dtypes.int, -1), **kwargs)),
  # arange loop folding (unrolled)
  (NOp(UOps.REDUCE, src=((NOp.var("idx") + NOp.cvar("mval") * NOp(UOps.RANGE, src=(NOp.var("loop_start"), NOp.var("loop_end")), name="rng"))
   .lt(NOp.cvar("compval")).where(NOp.cvar("multconst"), NOp.const(None, 0)) + NOp.var("extra"),),
   arg=BinaryOps.ADD, name="reduce", allow_any_len=True), loop_collapse),
  # indexing (with a multiply offset)!
  (NOp(UOps.REDUCE, src=(NOp.var('idx').eq(NOp(UOps.RANGE, name="rng")).cast()*
    NOp(UOps.LOAD, src=(NOp.var("buf"), NOp.var('add')+NOp.var('mul')*NOp(UOps.RANGE, name="rng")), name="ld"),),
    arg=BinaryOps.ADD, name="reduce", allow_any_len=True), index_collapse),
  (NOp(UOps.REDUCE, src=(NOp.var('idx').ne(NOp(UOps.RANGE, name="rng")).__neg__().cast()*
    NOp(UOps.LOAD, src=(NOp.var("buf"), NOp(UOps.RANGE, name="rng")), name="ld"),),
    arg=BinaryOps.ADD, name="reduce", allow_any_len=True),
    lambda **kwargs: index_collapse(add=UOp.const(dtypes.int, 0), mul=UOp.const(dtypes.int, 1), **kwargs)),
  (NOp(UOps.REDUCE, src=(NOp.var('idx').eq(NOp(UOps.RANGE, name="rng")).where(
    NOp(UOps.LOAD, src=(NOp.var("buf"), NOp.var('add')+NOp.var('mul')*NOp(UOps.RANGE, name="rng")), name="ld"), NOp.const(None, 0.0)),),
    arg=BinaryOps.ADD, name="reduce", allow_any_len=True), index_collapse),
  # other arange folders
  (NOp.cvar("c1") - (NOp.var("x") + NOp.cvar("c2")), lambda c1, c2, x: (c1-c2)-x),  # c1 - (x + c2) -> (c1-c2) - x
  (-(NOp.var("x") * NOp.cvar("c1")), lambda x, c1: x*-c1),
  # max folding
  (NOp.max(NOp.var('x'), NOp.var('y')), lambda x,y: x if x.vmin.arg >= y.vmax.arg else y if x.vmax.arg <= y.vmin.arg else None),
  # const rules
  (NOp(UOps.GEP, src=(NOp.cvar("c"),), name="root"), lambda root, c: root.const(c.arg)),
  (UPat(UOps.CAST, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: root.const(c.arg)),
  # a REDUCE without ranges is a NOOP
  (NOp(UOps.REDUCE, src=(NOp.var('x'),)), lambda x: x),
  # GEP on a const is the const
  (NOp(UOps.GEP, src=(NOp.cvar("x"),), name="root"), lambda root,x: root.const(x.arg)),
  # a conditional with the same results either way is a noop, also fold const conditionals
  (NOp.var().where(NOp.var("val"), NOp.var("val")), lambda val: val),
  (NOp.cvar('gate').where(NOp.var('c0'), NOp.var('c1')), lambda gate, c0, c1: c0 if gate.arg else c1),
  # ** constant folding **
  (UPat(UOps.ALU, name="root", src=UPat(UOps.CONST)), lambda root: root.const(exec_alu(root.arg, root.dtype, [x.arg for x in root.src]))),
  # ** self folding **
  (-(-NOp.var('x')), lambda x: x),    # -(-x) -> x
  (NOp.var('x') + 0, lambda x: x),    # x+0 -> x
  (NOp.var('x') * 1, lambda x: x),    # x*1 -> x
  (NOp.var('x') * -1, lambda x: -x),  # x*-1 -> -x
  (NOp.var('x') // NOp.var('x'), lambda x: x.const(1)), # x//x -> 1
  (NOp.var('x') // 1, lambda x: x),   # x//1 -> x
  (NOp.var('x') // -1, lambda x: -x), # x//-1 -> -x
  (NOp.var('x') / NOp.var('x'), lambda x: x.const(1)), # x/x -> 1
  (NOp.var('x', dtype=dtypes.bool) & NOp.cvar('c'), lambda x,c: x if c.arg else c),
  (NOp.var('x', dtype=dtypes.bool) | NOp.cvar('c'), lambda x,c: c if c.arg else x),
  # ** zero folding **
  # x*0 -> 0 or 0*x -> 0
  # if x is nan or inf it should render the nan value.
  # NOTE: this can be wrong for loaded NaN
  (NOp.var('x') * 0, lambda x: x.const(float('nan') if isinstance(x.arg, float) and (math.isnan(x.arg) or math.isinf(x.arg)) else 0)),
  # x-x -> 0
  (NOp.var('x') - NOp.var('x'), lambda x: x.const(0)),
  # min==max -> CONST (slow!)
  (UPat({UOps.ALU, UOps.DEFINE_VAR}, name='x'), lambda x: x.const(x.vmin.arg) if x.vmin.arg == x.vmax.arg else None),
  # ** load/store folding **
  (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.load(NOp.var("buf"), NOp.var("idx"))), lambda buf,idx:UOp(UOps.NOOP)),
  # ** two stage add/mul folding **
  ((NOp.var('x') + NOp.cvar('c1')) + NOp.cvar('c2'), lambda x,c1,c2: x+x.const(exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, c2.arg]))),
  ((NOp.var("x") * NOp.cvar("c1")) * NOp.cvar("c2"), lambda x,c1,c2: x*x.const(exec_alu(BinaryOps.MUL, x.dtype, [c1.arg, c2.arg]))),
  # *** rules from symbolic ***
  # ** lt **
  # c0*x<c1 for positive int c0,c1
  ((NOp.cvar('c0')*NOp.var('x')).lt(NOp.cvar('c1')),
   lambda x,c0,c1: x.lt(math.ceil(c1.arg/c0.arg)) if dtypes.is_int(x.dtype) and c0.arg > 0 and c1.arg > 0 else None),
  # mul add lt
  (((NOp.cvar('c0')*NOp.var('x'))+NOp.var('x2')).lt(NOp.cvar('c1')),
   lambda x,x2,c0,c1: x.lt(c1.arg//c0.arg) if c1.arg % c0.arg == 0 and c0.arg > x2.vmax.arg and x2.vmin.arg >= 0 else None),
  # generic lt folding (use div)
  (NOp.var('x').lt(NOp.cvar('c')), lambda x,c: newx.src[0].lt(newx.src[1]) if 0 < c.arg and dtypes.is_int(x.dtype) and \
   not dtypes.is_unsigned(x.dtype) and (newx:=div_folding(x,c.arg)) is not None and newx.op is UOps.ALU and newx.arg is BinaryOps.IDIV else None),
  # ** div **
  # # div folding
  (NOp.var('x') // NOp.cvar('c'), lambda x,c:
   newx if 0 < c.arg and not dtypes.is_unsigned(x.dtype) and (newx:=div_folding(x,c.arg)) is not None else None),
  # ** mod **
  # mod folding
  (NOp.var('x') % NOp.cvar('c'), lambda x,c: newx if 0 < c.arg and (newx:=mod_folding(x,c.arg)) is not None else None),
  # mul mod
  ((NOp.cvar('c0')*NOp.var('x')) % NOp.cvar('c1'), lambda x,c0,c1: (x%(c1.arg//c0.arg))*c0 if c1.arg%c0.arg == 0 else None),
  # (x%c)+(x//c)*c = x
  (NOp.var('x')%NOp.cvar('c')+(NOp.var('x')//NOp.cvar('c'))*NOp.cvar('c'), lambda x,c: x),
  # ** combine terms **
  # -(x+y) -> -x + -y
  (-(NOp.var("x") + NOp.var("y")), lambda x,y: (-x)+(-y)),
  # (x+c0)*c1 -> x*c1+c0*c1. only for signed int, float have inf*0=nan issue
  ((NOp.var("x") + NOp.cvar("c0")) * NOp.cvar("c1"), lambda x,c0,c1:
   x*c1+c0.arg*c1.arg if dtypes.is_int(x.dtype) and not dtypes.is_unsigned(x.dtype) else None),
  # (x*c0)+(x*c1) -> x*(c0+c1)
  (NOp.var("x") * NOp.cvar("c0") + NOp.var("x") * NOp.cvar("c1"), lambda x,c0,c1: x*exec_alu(BinaryOps.ADD, x.dtype, [c0.arg, c1.arg])),
  # (x*c0)+(y*c0) -> (x+y)*c0
  #((NOp.var("x") * NOp.cvar("c0")) + (NOp.var("y") * NOp.cvar("c0")), lambda x,y,c0: c0*(x+y)),
  # (x*x2)/x2 -> x
  ((NOp.var("x") * NOp.var("x2")) / NOp.var("x2"), lambda x,x2: x),
  # (x//c0)//c1 -> x//(c0*c1)
  ((NOp.var("x") // NOp.cvar("c0")) // NOp.cvar("c1"), lambda x,c0,c1: x//x.const(exec_alu(BinaryOps.MUL, x.dtype, [c0.arg, c1.arg]))),
  # (x/x1)/x2 -> x/(x1*x2)
  ((NOp.var("x") / NOp.var("x2")) / NOp.var("x3"), lambda x,x2,x3: x/(x2*x3)),
  # c0 + x < c1 -> x < c1 - c0
  ((NOp.cvar("c0") + NOp.var("x")).lt(NOp.cvar("c1")), lambda x,c0,c1: UOp.lt(x, x.const(exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, -c0.arg])))),
  # (x+x*c0)-> x*(c0+1)
  (NOp.var("x") + NOp.var("x") * NOp.cvar("c0"), lambda x,c0: x*(c0.arg+1)),
  # x!=0 -> (bool)x
  (NOp.var("x").ne(0), lambda x: x.cast(dtypes.bool)),
  # bool != 1 -> not bool
  (NOp.var("x", dtype=dtypes.bool).ne(1), lambda x: -x),
  # TODO: can do the invert of this (flip alt/load) when we fix double ops
  (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.var("gate").where(NOp.var("alt"), NOp.load(NOp.var("buf"), NOp.var("idx")))),
   lambda buf, idx, gate, alt: UOp.store(buf, idx, alt, gate)),
  # VECTORIZE-PHI-GEP -> PHI-VECTORIZE
  (NOp(UOps.VECTORIZE, src=tuple(NOp(UOps.PHI, src=(NOp(UOps.GEP, src=(NOp.var("val"),), arg=i), NOp.var(f"v{i}"))) for i in range(4)), name="root"),
   lambda root, val, v0, v1, v2, v3: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1, v2, v3))))),
  (NOp(UOps.VECTORIZE, src=tuple(NOp(UOps.PHI, src=(NOp(UOps.GEP, src=(NOp.var("val"),), arg=i), NOp.var(f"v{i}"))) for i in range(2)), name="root"),
   lambda root, val, v0, v1: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1))))),
  # cast NOOP (NOTE: it's str to deal with PtrDType)
  (NOp(UOps.CAST, name="root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
  (NOp(UOps.VECTORIZE, name="root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
  # fold gated LOAD/STORE
  (NOp.load(NOp.var("buf"), NOp.var("idx"), NOp.var("var"), NOp.const(dtypes.bool, True)), lambda buf,idx,var: UOp.load(buf, idx, dtype=var.dtype)),
  (NOp.load(NOp.var("buf"), NOp.var("idx"), NOp.var("var"), NOp.const(dtypes.bool, True), NOp.var("barrier")),
   lambda buf,idx,var,barrier: UOp.load(buf, idx, barrier, dtype=var.dtype)),
  (NOp.load(NOp.var(), NOp.var(), NOp.var("var"), NOp.const(dtypes.bool, False)), lambda var: var),
  (NOp.load(NOp.var(), NOp.var(), NOp.var("var"), NOp.const(dtypes.bool, False), NOp.var()), lambda var: var),
  (NOp.store(NOp.var("buf"), NOp.var("idx"), NOp.var("val"), NOp.const(dtypes.bool, True)), UOp.store),
  (NOp.store(NOp.var(), NOp.var(), NOp.var(), NOp.const(dtypes.bool, False)), lambda: UOp(UOps.NOOP)),
  # remove NOOPs from SINK
  (NOp(UOps.SINK, name="root"),
    lambda root: UOp(UOps.SINK, root.dtype, a, root.arg) if len(a:=tuple(x for x in root.src if x.op is not UOps.NOOP)) != len(root.src) else None),
  # ** move add consts to end (NOTE: this is still happening before constant folding) **
  (UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(UOps.CONST, name='c1'), UPat(name='x'))), lambda c1,x: x+c1 if x.op is not UOps.CONST else None),
  (UPat(UOps.ALU, BinaryOps.ADD, src=[UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name='x'), UPat(UOps.CONST, name='c1'))), UPat(name='y')]),
    lambda x,c1,y: (x+y)+c1),
])

  @dataclass(frozen=True, order=True)                                                                                                                                                                    # dtype.py                      :     9    G: {}     L: {}
  class DType:
    def vec(self, sz:int):                                                                                                                                                                               # dtype.py                      :    16    G: {}     L: {}
      assert sz > 1 and self.count == 1, f"can't vectorize {self} with size {sz}"                                                                                                                        # dtype.py                      :    17    G: {}     L: {'self': dtypes.float, 'sz': 2}
      return DType(self.priority, self.itemsize*sz, f"{INVERSE_DTYPES_DICT[self.name]}{sz}", None, sz)                                                                                                   # dtype.py                      :    18    G: {}     L: {}

  @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                             # ops.py                        :   229    G: {}     L: {}
  class NOp(UOp):
    @staticmethod                                                                                                                                                                                        # ops.py                        :   234    G: {}     L: {}
    def var(name:Optional[str]=None, dtype:Optional[DType]=None): return NOp(UOps.NOOP, dtype=dtype, name=name)

  @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                             # ops.py                        :   229    G: {}     L: {}
  class NOp(UOp):
    def const(self:Union[UOp, DType, None], b:ConstType|Variable): return NOp((x:=UOp.const(self, b)).op, x.dtype, x.src, x.arg)                                                                         # ops.py                        :   237    G: {}     L: {}

      @dataclass(frozen=True, eq=False)                                                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
      class UOp:
        def const(self:Union[UOp, DType, None], b:ConstType|Variable): return UOp._const(self.dtype if isinstance(self, UOp) else self, b)                                                               # ops.py                        :   150    G: {}     L: {}

          @dataclass(frozen=True, eq=False)                                                                                                                                                              # ops.py                        :   100    G: {}     L: {}
          class UOp:
            @staticmethod                                                                                                                                                                                # ops.py                        :   155    G: {}     L: {}
            @functools.lru_cache(maxsize=None)
            def _const(dtype:Optional[DType], b:ConstType|Variable):
              if isinstance(b, Variable): return UOp(UOps.DEFINE_VAR, dtype, (UOp.const(dtypes.int, b.min), UOp.const(dtypes.int, cast(int,b.max))), b)                                                  # ops.py                        :   157    G: {}     L: {'dtype': None}
              if dtype is not None and dtype != (sdtype := dtype.scalar()):                                                                                                                              # ops.py                        :   158    G: {}     L: {}
              return UOp(UOps.CONST, dtype, arg=dtypes.as_const(b, dtype) if dtype is not None else b)                                                                                                   # ops.py                        :   160    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __add__(self, x): return self.alu(BinaryOps.ADD, self.ufix(x))                                                                                                                                   # ops.py                        :   131    G: {}     L: {}

      @dataclass(frozen=True, eq=False)                                                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
      class UOp:
        def ufix(self, x): return self.const(x) if not isinstance(x, UOp) else x                                                                                                                         # ops.py                        :   126    G: {}     L: {}

      @dataclass(frozen=True, eq=False)                                                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
      class UOp:
        def alu(self, arg, *src:UOp):                                                                                                                                                                    # ops.py                        :   161    G: {}     L: {}
          return type(self)(UOps.ALU, dtypes.bool if arg in {BinaryOps.CMPLT, BinaryOps.CMPNE} else (self, *src)[-1].dtype, (self,)+src, arg)                                                            # ops.py                        :   162    G: {}     L: {'arg': <BinaryOps.ADD: 1>, 'src': (NOp(UOps.WMMA, None, arg=None, src=()),)}

  @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                             # ops.py                        :   229    G: {}     L: {}
  class NOp(UOp):
    @staticmethod                                                                                                                                                                                        # ops.py                        :   236    G: {}     L: {}
    def cvar(name:Optional[str]=None, dtype:Optional[DType]=None): return NOp(UOps.CONST, dtype=dtype, name=name)

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __mul__(self, x): return self.alu(BinaryOps.MUL, self.ufix(x))                                                                                                                                   # ops.py                        :   134    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def lt(self, x): return self.alu(BinaryOps.CMPLT, self.ufix(x))                                                                                                                                      # ops.py                        :   144    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def where(self, x, y): return self.alu(TernaryOps.WHERE, x, y)                                                                                                                                       # ops.py                        :   148    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __sub__(self, x): return self.alu(BinaryOps.ADD, self.ufix(-x))                                                                                                                                  # ops.py                        :   133    G: {}     L: {}

      @dataclass(frozen=True, eq=False)                                                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
      class UOp:
        def __neg__(self): return self.alu(UnaryOps.NEG)                                                                                                                                                 # ops.py                        :   130    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def eq(self, x): return -self.ne(x)                                                                                                                                                                  # ops.py                        :   143    G: {}     L: {}

      @dataclass(frozen=True, eq=False)                                                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
      class UOp:
        def ne(self, x): return self.alu(BinaryOps.CMPNE, self.ufix(x))                                                                                                                                  # ops.py                        :   142    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def cast(self, dtype=None): return type(self)(UOps.CAST, dtype, (self,))                                                                                                                             # ops.py                        :   127    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def max(self, x): return self.alu(BinaryOps.MAX, x)                                                                                                                                                  # ops.py                        :   146    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __floordiv__(self, x): return self.alu(BinaryOps.IDIV, self.ufix(x))                                                                                                                             # ops.py                        :   136    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __truediv__(self, x): return self.alu(BinaryOps.MUL, self.ufix(x).alu(UnaryOps.RECIP))                                                                                                           # ops.py                        :   137    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __and__(self, x): return self.alu(BinaryOps.AND, self.ufix(x))                                                                                                                                   # ops.py                        :   140    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __or__(self, x): return self.alu(BinaryOps.OR, self.ufix(x))                                                                                                                                     # ops.py                        :   141    G: {}     L: {}

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    @staticmethod                                                                                                                                                                                        # ops.py                        :   164    G: {}     L: {}
    def load(*src:UOp, dtype:Optional[DType]=None, **kwargs): return type(src[0])(UOps.LOAD, dtype, tuple(src)+tuple(kwargs.values()))

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    @staticmethod                                                                                                                                                                                        # ops.py                        :   166    G: {}     L: {}
    def store(*src:UOp, **kwargs): return type((src:=(*src, *kwargs.values()))[0])(UOps.STORE, None, src)

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def __mod__(self, x): return self.alu(BinaryOps.MOD, self.ufix(x))                                                                                                                                   # ops.py                        :   138    G: {}     L: {}

  @dataclass(frozen=True, order=True)                                                                                                                                                                    # dtype.py                      :     9    G: {}     L: {}
  class DType:
    def scalar(self): return DTYPES_DICT[self.name[:-len(str(self.count))]] if self.count > 1 else self                                                                                                  # dtype.py                      :    19    G: {}     L: {}

  class dtypes:                                                                                                                                                                                          # dtype.py                      :    38    G: {}     L: {}
    @staticmethod                                                                                                                                                                                        # dtype.py                      :    57    G: {}     L: {}
    def as_const(val: ConstType, dtype:DType): return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)

      class dtypes:                                                                                                                                                                                      # dtype.py                      :    38    G: {}     L: {}
        @staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool                                                                                                         # dtype.py                      :    44    G: {}     L: {}
        @functools.lru_cache(None)
        def is_int(x: DType) -> bool: return x.scalar() in {dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64, dtypes.pyint} or dtypes.is_unsigned(x)

          class dtypes:                                                                                                                                                                                  # dtype.py                      :    38    G: {}     L: {}
            @staticmethod                                                                                                                                                                                # dtype.py                      :    47    G: {}     L: {}
            @functools.lru_cache(None)
            def is_unsigned(x: DType) -> bool: return x.scalar() in {dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64}

      class dtypes:                                                                                                                                                                                      # dtype.py                      :    38    G: {}     L: {}
        @staticmethod                                                                                                                                                                                    # dtype.py                      :    41    G: {}     L: {}
        @functools.lru_cache(None)
        def is_float(x: DType) -> bool: return x.scalar() in {dtypes.float16, dtypes.bfloat16, dtypes.float32, dtypes.float64}

  @dataclass(frozen=True, repr=False)  # reuse repr from UOp                                                                                                                                             # ops.py                        :   229    G: {}     L: {}
  class NOp(UOp):
    def compile(self: NOp, name:Optional[str]=None) -> UPat:                                                                                                                                             # ops.py                        :   239    G: {}     L: {}
      return UPat(name=self.name, dtype=self.dtype) if self.op is UOps.NOOP else UPat(self.op, self.arg, (list if self.commutative()                                                                     # ops.py                        :   240    G: {}     L: {'self': NOp(UOps.GEP, None, arg=None, src=(\n  NOp(UOps.VECTORIZE, None, arg=None, src=()),)), 'name': None}
        else tuple)(src.compile() for src in self.src) or None, self.name or name, self.dtype, self.allow_any_len)

        @dataclass(frozen=True, eq=False)                                                                                                                                                                # ops.py                        :   100    G: {}     L: {}
        class UOp:
          def commutative(self) -> bool:                                                                                                                                                                 # ops.py                        :   105    G: {}     L: {}
            return (self.op is UOps.ALU and \                                                                                                                                                            # ops.py                        :   106    G: {}     L: {}
              self.arg in {BinaryOps.ADD, BinaryOps.MUL, BinaryOps.MAX, BinaryOps.CMPNE, BinaryOps.XOR, BinaryOps.AND, BinaryOps.OR})

acc_number = 0                                                                                                                                                                                           # codegen/uopgraph.py           :   417    G: {}     L: {}
expander = PatternMatcher([                                                                                                                                                                              # codegen/uopgraph.py           :   459    G: {}     L: {}
  # create gate MUST BE BEFORE expander
  (NOp(UOps.STORE, name="root"), create_gate),
  # do expansion
  (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST, UOps.GEP, UOps.WMMA, UOps.LOAD, UOps.STORE,
         UOps.VECTORIZE, UOps.REDUCE, UOps.EXPAND, UOps.IF}, name="root"), do_expand),
  (NOp(UOps.CONTRACT, name="con"), do_contract),
  # remove EXPANDs from SINK
  (NOp(UOps.SINK, name="root"),
   lambda root: UOp(UOps.SINK, root.dtype, a, root.arg)
    if len(a:=tuple(flatten(x.src if x.op is UOps.EXPAND else (x,) for x in root.src))) != len(root.src) else None),
  # BARRIERs aren't actually expanded
  (NOp(UOps.BARRIER, src=(NOp(UOps.EXPAND, name="ex"),)), lambda ex: UOp(UOps.EXPAND, None, (UOp(UOps.BARRIER, None, ex.src),)*len(ex.src), ex.arg)),
  # empty EXPAND is NOOP
  (NOp(UOps.EXPAND, src=(NOp.var('x'),), arg=()), lambda x: x),
  # EXPAND GEP (needed for WMMA, generalize this) -> vectorized ALU
  (NOp(UOps.EXPAND, name="ex", src=tuple(NOp.var('x').gep(i)+NOp.var('y').gep(i) for i in range(8))),
    lambda ex,x,y: UOp(UOps.EXPAND, ex.dtype, tuple((x+y).gep(i) for i in range(8)), ex.arg)),
])

  @dataclass(frozen=True, eq=False)                                                                                                                                                                      # ops.py                        :   100    G: {}     L: {}
  class UOp:
    def gep(self, i:int): return type(self)(UOps.GEP, self.dtype.scalar() if self.dtype is not None else None, (self,), i)                                                                               # ops.py                        :   129    G: {}     L: {}

reducer = PatternMatcher([                                                                                                                                                                               # codegen/uopgraph.py           :   487    G: {}     L: {}
  (NOp(UOps.REDUCE, name="root"), do_reduce),
  # no ALU on vectorized dtypes
  (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST}, name="alu"), no_vectorized_alu),
  # delete_redundant_gates (after expand, is this still needed?)
  (NOp(UOps.STORE, name="root"), delete_redundant_gates),
  # late fixup of unfoldable image loads
  (UPat(UOps.LOAD, src=(UPat(name="buf"), UPat()), allow_any_len=True, name="load"), fix_unfoldable_image_load),
])

no_pyint = PatternMatcher([(UPat({UOps.CONST, UOps.ALU, UOps.SPECIAL, UOps.RANGE}, dtype=dtypes.pyint, name="x"),                                                                                        # codegen/uopgraph.py           :   497    G: {}     L: {}
    lambda x: UOp(x.op, dtypes.int32, x.src, x.arg))])

linearize_cnt = 0                                                                                                                                                                                        # codegen/uopgraph.py           :   524    G: {}     L: {}
render_ops: Any = { NumNode: lambda self, ops, ctx: UOp.const(dtypes.pyint, self.b),                                                                                                                     # shape/shapetracker.py         :    15    G: {}     L: {}
                    MulNode: lambda self, ops, ctx: self.a.render(ops, ctx)*variable_to_uop(self.b, ctx),
                    DivNode: lambda self, ops, ctx: self.a.render(ops, ctx)//variable_to_uop(self.b, ctx),
                    ModNode: lambda self, ops, ctx: self.a.render(ops, ctx)%variable_to_uop(self.b, ctx),
                    LtNode: lambda self, ops, ctx: self.a.render(ops, ctx).lt(variable_to_uop(self.b, ctx)),
  Variable: lambda self,ops,ctx: ctx[self] if ctx is not None and self in ctx else \
    UOp(UOps.DEFINE_VAR, dtypes.int, (UOp.const(dtypes.int, self.min), UOp.const(dtypes.int, self.max)), self),
  SumNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a+b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)),
  AndNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a*b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)) }

@dataclass(frozen=True)                                                                                                                                                                                  # shape/shapetracker.py         :    36    G: {}     L: {}
class ShapeTracker:
  views: Tuple[View, ...]                                                                                                                                                                                # shape/shapetracker.py         :    37    G: {}     L: {'__module__': 'tinygrad.shape.shapetracker', '__qualname__': 'ShapeTracker'}

from tinygrad.device import Buffer                                                                                                                                                                       # lazy.py                       :     8    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # device.py                     :     1    G: {'__name__': 'tinygrad.device', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929a1da710>, '__spec__': ModuleSpec(name='tinygrad.device', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929a1da710>, origin='/home/lorinbaum/code/tinygrad/tinygrad/device.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/device.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/device.cpython-310.pyc'}     L: {}
import multiprocessing, decimal, statistics, random                                                                                                                                                      # device.py                     :     2    G: {}     L: {}
from dataclasses import dataclass                                                                                                                                                                        # device.py                     :     3    G: {}     L: {}
from collections import defaultdict                                                                                                                                                                      # device.py                     :     4    G: {}     L: {}
from typing import List, Optional, Dict, Tuple, Any, cast, Protocol, Type                                                                                                                                # device.py                     :     5    G: {}     L: {}
import importlib, inspect, functools, pathlib, os, ctypes, atexit, time, contextlib, array                                                                                                               # device.py                     :     6    G: {}     L: {}
from tinygrad.helpers import SAVE_SCHEDULE, getenv, diskcache_get, diskcache_put, DEBUG, GlobalCounters, flat_mv, from_mv, ProfileLogger, PROFILE                                                        # device.py                     :     7    G: {}     L: {}
from tinygrad.dtype import DType, ImageDType                                                                                                                                                             # device.py                     :     8    G: {}     L: {}
from tinygrad.renderer import Renderer                                                                                                                                                                   # device.py                     :     9    G: {}     L: {}
from typing import Optional, List, Tuple, Dict, Callable, Any                                                                                                                                            # renderer/__init__.py          :     1    G: {'__name__': 'tinygrad.renderer', '__doc__': None, '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d39810>, '__spec__': ModuleSpec(name='tinygrad.renderer', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d39810>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad/renderer']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad/renderer'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/__init__.cpython-310.pyc'}     L: {}
import functools                                                                                                                                                                                         # renderer/__init__.py          :     2    G: {}     L: {}
from dataclasses import dataclass, field                                                                                                                                                                 # renderer/__init__.py          :     3    G: {}     L: {}
from tinygrad.helpers import to_function_name, dedup                                                                                                                                                     # renderer/__init__.py          :     4    G: {}     L: {}
from tinygrad.ops import Op, UOps, UOp, flops_mem                                                                                                                                                        # renderer/__init__.py          :     5    G: {}     L: {}
from tinygrad.shape.symbolic import sym_infer, sint, Variable                                                                                                                                            # renderer/__init__.py          :     6    G: {}     L: {}
from tinygrad.dtype import DType                                                                                                                                                                         # renderer/__init__.py          :     7    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # renderer/__init__.py          :    10    G: {}     L: {}
class TensorCore: # D = A * B + C, A is (M x K), B is (K x N), C and D are (M x N)
  dims: Tuple[int,int,int] # N, M, K                                                                                                                                                                     # renderer/__init__.py          :    11    G: {}     L: {'__module__': 'tinygrad.renderer', '__qualname__': 'TensorCore', '__annotations__': {}}
  dtype_in: DType # dtype for A and B                                                                                                                                                                    # renderer/__init__.py          :    12    G: {}     L: {}
  dtype_out: DType # dtype for C and D                                                                                                                                                                   # renderer/__init__.py          :    13    G: {}     L: {}
  threads: List[Tuple[int,int]] # list of (TC dim,amt) that construct the warp thread structure                                                                                                          # renderer/__init__.py          :    14    G: {}     L: {}

@dataclass                                                                                                                                                                                               # renderer/__init__.py          :    18    G: {}     L: {}
class Program:
  name:str                                                                                                                                                                                               # renderer/__init__.py          :    19    G: {}     L: {'__module__': 'tinygrad.renderer', '__qualname__': 'Program', '__annotations__': {}}
  src:str                                                                                                                                                                                                # renderer/__init__.py          :    20    G: {}     L: {}
  dname:str                                                                                                                                                                                              # renderer/__init__.py          :    21    G: {}     L: {}
  uops:Optional[List[UOp]]=None                                                                                                                                                                          # renderer/__init__.py          :    22    G: {}     L: {}
  mem_estimate:sint=0  # TODO: get this from the load/store uops once min/max are good                                                                                                                   # renderer/__init__.py          :    23    G: {}     L: {}
  global_size:Optional[List[int]]=None                                                                                                                                                                   # renderer/__init__.py          :    26    G: {}     L: {}
  local_size:Optional[List[int]]=None                                                                                                                                                                    # renderer/__init__.py          :    27    G: {}     L: {}
  vars:List[Variable]=field(default_factory=list)                                                                                                                                                        # renderer/__init__.py          :    28    G: {}     L: {}
  globals:List[int]=field(default_factory=list)                                                                                                                                                          # renderer/__init__.py          :    29    G: {}     L: {}
  outs:List[int]=field(default_factory=list)                                                                                                                                                             # renderer/__init__.py          :    30    G: {}     L: {}
  _ran_post_init:bool=False  # NOTE: this is needed if you call replace on the Program                                                                                                                   # renderer/__init__.py          :    31    G: {}     L: {}

class Renderer:                                                                                                                                                                                          # renderer/__init__.py          :    71    G: {}     L: {}
  device: str = ""                                                                                                                                                                                       # renderer/__init__.py          :    72    G: {}     L: {}
  suffix: str = ""                                                                                                                                                                                       # renderer/__init__.py          :    73    G: {}     L: {}
  supports_float4: bool = True                                                                                                                                                                           # renderer/__init__.py          :    75    G: {}     L: {}
  has_local: bool = True                                                                                                                                                                                 # renderer/__init__.py          :    76    G: {}     L: {}
  has_shared: bool = True                                                                                                                                                                                # renderer/__init__.py          :    77    G: {}     L: {}
  global_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now                                                                                               # renderer/__init__.py          :    79    G: {}     L: {}
  local_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now                                                                                                # renderer/__init__.py          :    80    G: {}     L: {}
  shared_max: int = 32768                                                                                                                                                                                # renderer/__init__.py          :    81    G: {}     L: {}
  tensor_cores: List[TensorCore] = []                                                                                                                                                                    # renderer/__init__.py          :    82    G: {}     L: {}
  extra_matcher: Any = None                                                                                                                                                                              # renderer/__init__.py          :    83    G: {}     L: {}
  code_for_op: Dict[Op, Callable] = {}                                                                                                                                                                   # renderer/__init__.py          :    84    G: {}     L: {}

Device = _Device()                                                                                                                                                                                       # device.py                     :    40    G: {}     L: {}

  class _Device:                                                                                                                                                                                         # device.py                     :    13    G: {}     L: {}
    def __init__(self) -> None: self._devices: List[str] = [x.stem[len("ops_"):].upper() for x in (pathlib.Path(__file__).parent/"runtime").iterdir() if x.stem.startswith("ops_")]  # noqa: E501        # device.py                     :    14    G: {}     L: {}

@dataclass(frozen=True, eq=True)                                                                                                                                                                         # device.py                     :    45    G: {}     L: {}
class BufferOptions:
  image: Optional[ImageDType] = None                                                                                                                                                                     # device.py                     :    46    G: {}     L: {'__module__': 'tinygrad.device', '__qualname__': 'BufferOptions', '__annotations__': {}}
  uncached: bool = False                                                                                                                                                                                 # device.py                     :    47    G: {}     L: {}
  cpu_access: bool = False                                                                                                                                                                               # device.py                     :    48    G: {}     L: {}
  host: bool = False                                                                                                                                                                                     # device.py                     :    49    G: {}     L: {}
  nolru: bool = False                                                                                                                                                                                    # device.py                     :    50    G: {}     L: {}

MallocAllocator = _MallocAllocator()                                                                                                                                                                     # device.py                     :   170    G: {}     L: {}

  class LRUAllocator(Allocator):  # pylint: disable=abstract-method                                                                                                                                      # device.py                     :   143    G: {}     L: {}
    def __init__(self): self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)                                                                                                   # device.py                     :   148    G: {}     L: {}

  def hcq_command(func):                                                                                                                                                                                 # device.py                     :   200    G: {}     L: {}
    return __wrapper                                                                                                                                                                                     # device.py                     :   216    G: {}     L: {}

class HCQCompiled(Compiled):                                                                                                                                                                             # device.py                     :   482    G: {}     L: {}
  """                                                                                                                                                                                                    # device.py                     :   483    G: {}     L: {}
  A base class for devices compatible with the HCQ (Hardware Command Queue) API.
  """
  devices: List[HCQCompiled] = []                                                                                                                                                                        # device.py                     :   486    G: {}     L: {}
  gpu2cpu_copy_time_diff: decimal.Decimal = decimal.Decimal('nan')                                                                                                                                       # device.py                     :   487    G: {}     L: {}
  gpu2cpu_compute_time_diff: decimal.Decimal = decimal.Decimal('nan')                                                                                                                                    # device.py                     :   488    G: {}     L: {}

# Protocol for hcq compatible allocators for allocated buffers to contain VA address and it's size.                                                                                                      # device.py                     :   600    G: {}     L: {}
class HCQBuffer(Protocol): va_addr:int; size:int # noqa: E702

from weakref import ref, ReferenceType, WeakValueDictionary                                                                                                                                              # lazy.py                       :     9    G: {}     L: {}
lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()                                                                                                                                  # lazy.py                       :    11    G: {}     L: {}

view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}                                                                                                                         # lazy.py                       :    25    G: {}     L: {}

from tinygrad.multi import MultiLazyBuffer                                                                                                                                                               # tensor.py                     :    14    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # multi.py                      :     1    G: {'__name__': 'tinygrad.multi', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c92997669e0>, '__spec__': ModuleSpec(name='tinygrad.multi', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c92997669e0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/multi.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/multi.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/multi.cpython-310.pyc'}     L: {}
from typing import Optional, Union, Any, Tuple, List, Dict                                                                                                                                               # multi.py                      :     2    G: {}     L: {}
import functools, itertools, operator                                                                                                                                                                    # multi.py                      :     3    G: {}     L: {}
from tinygrad.helpers import all_same, all_int, dedup, prod, DEBUG, RING, getenv                                                                                                                         # multi.py                      :     4    G: {}     L: {}
from tinygrad.dtype import DType, ConstType                                                                                                                                                              # multi.py                      :     5    G: {}     L: {}
from tinygrad.ops import BinaryOps, MetaOps, UnaryOps, TernaryOps, ReduceOps                                                                                                                             # multi.py                      :     6    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # multi.py                      :     7    G: {}     L: {}
from tinygrad.shape.shapetracker import sint                                                                                                                                                             # multi.py                      :     8    G: {}     L: {}

from tinygrad.ops import MetaOps, truncate                                                                                                                                                               # tensor.py                     :    15    G: {}     L: {}
from tinygrad.device import Device, Buffer, BufferOptions                                                                                                                                                # tensor.py                     :    16    G: {}     L: {}
from tinygrad.shape.symbolic import sint, Variable, MulNode, SumNode, NumNode, Node                                                                                                                      # tensor.py                     :    17    G: {}     L: {}
from tinygrad.engine.realize import run_schedule, memory_planner                                                                                                                                         # tensor.py                     :    18    G: {}     L: {}
from typing import List, Dict, Optional, cast, Generator, Tuple, Union                                                                                                                                   # engine/realize.py             :     1    G: {'__name__': 'tinygrad.engine.realize', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d682e0>, '__spec__': ModuleSpec(name='tinygrad.engine.realize', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d682e0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/realize.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/realize.cpython-310.pyc'}     L: {}
import time, pprint                                                                                                                                                                                      # engine/realize.py             :     2    G: {}     L: {}
from collections import defaultdict                                                                                                                                                                      # engine/realize.py             :     3    G: {}     L: {}
from dataclasses import dataclass, replace                                                                                                                                                               # engine/realize.py             :     4    G: {}     L: {}
from tinygrad.helpers import colored, getenv, DEBUG, GlobalCounters, ansilen, BEAM, NOOPT, all_int, CAPTURING, Metadata, Context, TRACEMETA, dedup                                                       # engine/realize.py             :     5    G: {}     L: {}
from tinygrad.ops import MetaOps, UOps, UOp                                                                                                                                                              # engine/realize.py             :     6    G: {}     L: {}
from tinygrad.dtype import dtypes                                                                                                                                                                        # engine/realize.py             :     7    G: {}     L: {}
from tinygrad.device import Device, Buffer                                                                                                                                                               # engine/realize.py             :     8    G: {}     L: {}
from tinygrad.shape.symbolic import Variable, sym_infer, sint                                                                                                                                            # engine/realize.py             :     9    G: {}     L: {}
from tinygrad.renderer import Renderer, Program                                                                                                                                                          # engine/realize.py             :    10    G: {}     L: {}
from tinygrad.codegen.kernel import Kernel                                                                                                                                                               # engine/realize.py             :    11    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # codegen/kernel.py             :     1    G: {'__name__': 'tinygrad.codegen.kernel', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d69bd0>, '__spec__': ModuleSpec(name='tinygrad.codegen.kernel', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d69bd0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/kernel.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/kernel.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/kernel.cpython-310.pyc'}     L: {}
import itertools, functools                                                                                                                                                                              # codegen/kernel.py             :     2    G: {}     L: {}
from dataclasses import dataclass, replace                                                                                                                                                               # codegen/kernel.py             :     3    G: {}     L: {}
from collections import defaultdict                                                                                                                                                                      # codegen/kernel.py             :     4    G: {}     L: {}
from typing import Literal, Optional, List, Tuple, Union, cast, Dict, Final, DefaultDict                                                                                                                 # codegen/kernel.py             :     5    G: {}     L: {}
from tinygrad.ops import BinaryOps, ReduceOps, UNSAFE_PAD_OPS, KernelInfo, BUFFER_UOPS, UOp, UOps, print_uops                                                                                            # codegen/kernel.py             :     7    G: {}     L: {}
from tinygrad.device import Device                                                                                                                                                                       # codegen/kernel.py             :     8    G: {}     L: {}
from tinygrad.renderer import Renderer, TensorCore, Program                                                                                                                                              # codegen/kernel.py             :     9    G: {}     L: {}
from tinygrad.dtype import DType, ImageDType, PtrDType                                                                                                                                                   # codegen/kernel.py             :    10    G: {}     L: {}
from tinygrad.helpers import all_same, colored, ansilen, dedup, getenv, prod, DEBUG, TC_OPT, USE_TC, round_up, all_int, \                                                                                # codegen/kernel.py             :    11    G: {}     L: {}
                             get_contraction, to_function_name, diskcache_put, ContextVar
from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                                     # codegen/kernel.py             :    13    G: {}     L: {}
from tinygrad.shape.symbolic import Variable, sint                                                                                                                                                       # codegen/kernel.py             :    14    G: {}     L: {}
from tinygrad.shape.view import strides_for_shape                                                                                                                                                        # codegen/kernel.py             :    15    G: {}     L: {}
from tinygrad.codegen.uopgraph import linearize_uop                                                                                                                                                      # codegen/kernel.py             :    16    G: {}     L: {}
from tinygrad.codegen.lowerer import ast_to_uop                                                                                                                                                          # codegen/kernel.py             :    17    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # codegen/lowerer.py            :     1    G: {'__name__': 'tinygrad.codegen.lowerer', '__doc__': None, '__package__': 'tinygrad.codegen', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d6b460>, '__spec__': ModuleSpec(name='tinygrad.codegen.lowerer', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d6b460>, origin='/home/lorinbaum/code/tinygrad/tinygrad/codegen/lowerer.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/lowerer.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/codegen/__pycache__/lowerer.cpython-310.pyc'}     L: {}
import functools                                                                                                                                                                                         # codegen/lowerer.py            :     2    G: {}     L: {}
from dataclasses import replace                                                                                                                                                                          # codegen/lowerer.py            :     3    G: {}     L: {}
from typing import List, Tuple, cast, Optional, Dict                                                                                                                                                     # codegen/lowerer.py            :     4    G: {}     L: {}
from tinygrad.shape.shapetracker import ShapeTracker, variable_to_uop                                                                                                                                    # codegen/lowerer.py            :     5    G: {}     L: {}
from tinygrad.shape.symbolic import sint                                                                                                                                                                 # codegen/lowerer.py            :     6    G: {}     L: {}
from tinygrad.dtype import dtypes, DType                                                                                                                                                                 # codegen/lowerer.py            :     7    G: {}     L: {}
from tinygrad.ops import ReduceOps, KernelInfo, BinaryOps, BUFFER_UOPS, UOp, UOps                                                                                                                        # codegen/lowerer.py            :     8    G: {}     L: {}
from tinygrad.renderer import Renderer                                                                                                                                                                   # codegen/lowerer.py            :     9    G: {}     L: {}
from tinygrad.helpers import all_int, get_contraction, prod, partition, flatten                                                                                                                          # codegen/lowerer.py            :    10    G: {}     L: {}

from enum import Enum, auto                                                                                                                                                                              # codegen/kernel.py             :    18    G: {}     L: {}

class OptOps(Enum):                                                                                                                                                                                      # codegen/kernel.py             :    20    G: {}     L: {}
  TC = auto(); UPCAST = auto(); UPCASTMID = auto(); UNROLL = auto(); LOCAL = auto() # noqa: E702                                                                                                         # codegen/kernel.py             :    21    G: {}     L: {}
  GROUP = auto(); GROUPTOP = auto(); NOLOCALS = auto(); PADTO = auto(); SWAP = auto() # noqa: E702                                                                                                       # codegen/kernel.py             :    22    G: {}     L: {}

@dataclass(frozen=True, order=True)                                                                                                                                                                      # codegen/kernel.py             :    31    G: {}     L: {}
class Opt:
  op: OptOps                                                                                                                                                                                             # codegen/kernel.py             :    32    G: {}     L: {'__module__': 'tinygrad.codegen.kernel', '__qualname__': 'Opt', '__annotations__': {}}
  axis: Optional[int] = None                                                                                                                                                                             # codegen/kernel.py             :    33    G: {}     L: {}
  amt: Optional[int] = None                                                                                                                                                                              # codegen/kernel.py             :    34    G: {}     L: {}

@dataclass                                                                                                                                                                                               # codegen/kernel.py             :    43    G: {}     L: {}
class TensorCoreOptions:
  axes: Tuple[int, ...] # the location of the original N and M axes if still in the shape                                                                                                                # codegen/kernel.py             :    44    G: {}     L: {'__module__': 'tinygrad.codegen.kernel', '__qualname__': 'TensorCoreOptions', '__annotations__': {}}
  axes_exist: Tuple[bool, ...] # true if the original N and M axes are still in the shape                                                                                                                # codegen/kernel.py             :    45    G: {}     L: {}
  axis_pads: Tuple[Tuple[int, int], ...]                                                                                                                                                                 # codegen/kernel.py             :    46    G: {}     L: {}

class Kernel:                                                                                                                                                                                            # codegen/kernel.py             :    54    G: {}     L: {}
  kernel_cnt: Final[DefaultDict[str, int]] = defaultdict(int)                                                                                                                                            # codegen/kernel.py             :   619    G: {}     L: {}

from tinygrad.engine.schedule import ScheduleItem                                                                                                                                                        # engine/realize.py             :    12    G: {}     L: {}
import sys, pickle, atexit, importlib, contextlib                                                                                                                                                        # engine/schedule.py            :     1    G: {'__name__': 'tinygrad.engine.schedule', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d6bfd0>, '__spec__': ModuleSpec(name='tinygrad.engine.schedule', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d6bfd0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/schedule.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/schedule.cpython-310.pyc'}     L: {}
from collections import defaultdict, deque                                                                                                                                                               # engine/schedule.py            :     2    G: {}     L: {}
from dataclasses import dataclass, field                                                                                                                                                                 # engine/schedule.py            :     3    G: {}     L: {}
from typing import Tuple, List, Dict, Optional, Set, DefaultDict, get_args                                                                                                                               # engine/schedule.py            :     4    G: {}     L: {}
from tinygrad.ops import MetaOps, ReduceOps, UNSAFE_PAD_OPS, UnaryOps, UOp, UOps                                                                                                                         # engine/schedule.py            :     5    G: {}     L: {}
from tinygrad.engine.graph import log_lazybuffer, realized_lazybuffer                                                                                                                                    # engine/schedule.py            :     6    G: {}     L: {}
import os, atexit, functools, contextlib                                                                                                                                                                 # engine/graph.py               :     1    G: {'__name__': 'tinygrad.engine.graph', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298dcd450>, '__spec__': ModuleSpec(name='tinygrad.engine.graph', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298dcd450>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/graph.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/graph.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/graph.cpython-310.pyc'}     L: {}
from collections import defaultdict                                                                                                                                                                      # engine/graph.py               :     2    G: {}     L: {}
from typing import List, Any, DefaultDict                                                                                                                                                                # engine/graph.py               :     3    G: {}     L: {}
from tinygrad.ops import UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps, UOps, UOp                                                                                                                  # engine/graph.py               :     4    G: {}     L: {}
from tinygrad.device import Device                                                                                                                                                                       # engine/graph.py               :     5    G: {}     L: {}
from tinygrad.helpers import GRAPHPATH, DEBUG, GlobalCounters                                                                                                                                            # engine/graph.py               :     6    G: {}     L: {}
from tinygrad.shape.symbolic import NumNode                                                                                                                                                              # engine/graph.py               :     7    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # engine/graph.py               :     8    G: {}     L: {}
with contextlib.suppress(ImportError): import networkx as nx                                                                                                                                             # engine/graph.py               :    10    G: {}     L: {}
if DEBUG >= 2: atexit.register(print_globalcounters)                                                                                                                                                     # engine/graph.py               :    18    G: {}     L: {}

  class ContextVar:                                                                                                                                                                                      # helpers.py                    :    92    G: {}     L: {}
    def __ge__(self, x): return self.value >= x                                                                                                                                                          # helpers.py                    :   102    G: {}     L: {}

G:Any = None                                                                                                                                                                                             # engine/graph.py               :    25    G: {}     L: {}
counts: DefaultDict[type, int] = defaultdict(int)                                                                                                                                                        # engine/graph.py               :    32    G: {}     L: {}
top_colors = {MetaOps: '#FFFFa0', UnaryOps: "#c0c0c0", ReduceOps: "#FFA0A0", BinaryOps: "#c0c0c0", TernaryOps: "#c0c0c0"}                                                                                # engine/graph.py               :    45    G: {}     L: {}
graph_uops_cnt = 0                                                                                                                                                                                       # engine/graph.py               :    74    G: {}     L: {}
from tinygrad.helpers import GRAPH, DEBUG, MULTIOUTPUT, SAVE_SCHEDULE, FUSE_CONV_BW, FUSE_ARANGE, \                                                                                                      # engine/schedule.py            :     7    G: {}     L: {}
                             GlobalCounters, colored, prod, dedup, all_int, merge_dicts, getenv, Metadata
from tinygrad.shape.symbolic import Variable, sint                                                                                                                                                       # engine/schedule.py            :     9    G: {}     L: {}
from tinygrad.dtype import ConstType, ImageDType, PtrDType, dtypes                                                                                                                                       # engine/schedule.py            :    10    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # engine/schedule.py            :    11    G: {}     L: {}
from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                                     # engine/schedule.py            :    12    G: {}     L: {}
from tinygrad.device import Buffer                                                                                                                                                                       # engine/schedule.py            :    13    G: {}     L: {}
from tinygrad.shape.view import View, strides_for_shape                                                                                                                                                  # engine/schedule.py            :    14    G: {}     L: {}
sys.setrecursionlimit(10000)                                                                                                                                                                             # engine/schedule.py            :    17    G: {}     L: {}
logops = open(getenv("LOGOPS", ""), "a") if getenv("LOGOPS", "") else None                                                                                                                               # engine/schedule.py            :    20    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # engine/schedule.py            :    25    G: {}     L: {}
class ScheduleItem:
  ast: UOp                                                                                                                                                                                               # engine/schedule.py            :    26    G: {}     L: {'__module__': 'tinygrad.engine.schedule', '__qualname__': 'ScheduleItem'}
  bufs: Tuple[Buffer, ...]                                                                                                                                                                               # engine/schedule.py            :    27    G: {}     L: {}
  metadata: Optional[List[Metadata]] = None                                                                                                                                                              # engine/schedule.py            :    28    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # engine/schedule.py            :    39    G: {}     L: {}
class LBScheduleItem:
  ast: UOp                                                                                                                                                                                               # engine/schedule.py            :    40    G: {}     L: {'__module__': 'tinygrad.engine.schedule', '__qualname__': 'LBScheduleItem'}
  outputs: List[LazyBuffer]                                                                                                                                                                              # engine/schedule.py            :    41    G: {}     L: {}
  inputs: List[LazyBuffer]                                                                                                                                                                               # engine/schedule.py            :    42    G: {}     L: {}
  var_vals: Dict[Variable, int] = field(default_factory=dict)                                                                                                                                            # engine/schedule.py            :    43    G: {}     L: {}
  metadata: List[Metadata] = field(default_factory=list)                                                                                                                                                 # engine/schedule.py            :    44    G: {}     L: {}

SCHEDULES: List[Tuple[DefaultDict[LBScheduleItem, List[LBScheduleItem]], DefaultDict[LBScheduleItem, int]]] = []                                                                                         # engine/schedule.py            :   262    G: {}     L: {}
logkerns, logkerns_level = open(getenv("LOGKERNS", ""), "a") if getenv("LOGKERNS", "") else None, getenv("LOGKERNS_LEVEL", 1)                                                                            # engine/realize.py             :    16    G: {}     L: {}

method_cache: Dict[Tuple[str, bytes, int, int, bool], CompiledRunner] = {}                                                                                                                               # engine/realize.py             :   149    G: {}     L: {}

@dataclass(frozen=True)                                                                                                                                                                                  # engine/realize.py             :   167    G: {}     L: {}
class ExecItem:
  prg: Runner                                                                                                                                                                                            # engine/realize.py             :   168    G: {}     L: {'__module__': 'tinygrad.engine.realize', '__qualname__': 'ExecItem'}
  bufs: List[Optional[Buffer]]                                                                                                                                                                           # engine/realize.py             :   169    G: {}     L: {}
  metadata: Optional[List[Metadata]] = None                                                                                                                                                              # engine/realize.py             :   170    G: {}     L: {}

capturing: List = []  # put classes with an add method in here                                                                                                                                           # engine/realize.py             :   218    G: {}     L: {}
from tinygrad.engine.schedule import ScheduleItem, create_schedule_with_vars                                                                                                                             # tensor.py                     :    19    G: {}     L: {}

import tinygrad.function as F                                                                                                                                                                            # tensor.py                     :    42    G: {}     L: {}
"""This is where the forwards and backwards passes live."""                                                                                                                                              # function.py                   :     1    G: {'__name__': 'tinygrad.function', '__doc__': None, '__package__': 'tinygrad', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9298d68b20>, '__spec__': ModuleSpec(name='tinygrad.function', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9298d68b20>, origin='/home/lorinbaum/code/tinygrad/tinygrad/function.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/function.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/__pycache__/function.cpython-310.pyc'}     L: {}
import math                                                                                                                                                                                              # function.py                   :     2    G: {}     L: {}
from typing import Tuple, Optional                                                                                                                                                                       # function.py                   :     3    G: {}     L: {}
from tinygrad.helpers import argsort                                                                                                                                                                     # function.py                   :     4    G: {}     L: {}
from tinygrad.dtype import dtypes, DType, sum_acc_dtype                                                                                                                                                  # function.py                   :     5    G: {}     L: {}
from tinygrad.ops import UnaryOps, BinaryOps, TernaryOps, ReduceOps                                                                                                                                      # function.py                   :     6    G: {}     L: {}
from tinygrad.tensor import Function                                                                                                                                                                     # function.py                   :     7    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # function.py                   :     8    G: {}     L: {}
from tinygrad.shape.symbolic import sint                                                                                                                                                                 # function.py                   :     9    G: {}     L: {}

class Tensor:                                                                                                                                                                                            # tensor.py                     :    92    G: {}     L: {}
  """                                                                                                                                                                                                    # tensor.py                     :    93    G: {}     L: {}
  A `Tensor` is a multi-dimensional matrix containing elements of a single data type.

  ```python exec="true" session="tensor"
  from tinygrad import Tensor, dtypes, nn
  import numpy as np
  import math
  np.set_printoptions(precision=4)
  ```
  """
  __slots__ = "lazydata", "requires_grad", "grad", "_ctx"                                                                                                                                                # tensor.py                     :   103    G: {}     L: {}
  __deletable__ = ('_ctx',)                                                                                                                                                                              # tensor.py                     :   104    G: {}     L: {}
  training: ClassVar[bool] = False                                                                                                                                                                       # tensor.py                     :   105    G: {}     L: {}
  no_grad: ClassVar[bool] = False                                                                                                                                                                        # tensor.py                     :   106    G: {}     L: {}

  _seed: int = int(time.time())                                                                                                                                                                          # tensor.py                     :   386    G: {}     L: {}
  _rng_counter: Optional[Tensor] = None                                                                                                                                                                  # tensor.py                     :   387    G: {}     L: {}

for device in Device._devices: setattr(Tensor, f"{device.lower()}", functools.partialmethod(Tensor.to, device))                                                                                          # tensor.py                     :  3221    G: {}     L: {}
if IMAGE:                                                                                                                                                                                                # tensor.py                     :  3223    G: {}     L: {}

  class ContextVar:                                                                                                                                                                                      # helpers.py                    :    92    G: {}     L: {}
    def __bool__(self): return bool(self.value)                                                                                                                                                          # helpers.py                    :   101    G: {}     L: {}

if TRACEMETA >= 1:                                                                                                                                                                                       # tensor.py                     :  3268    G: {}     L: {}

  for name, fn in inspect.getmembers(Tensor, inspect.isfunction):                                                                                                                                        # tensor.py                     :  3269    G: {}     L: {}
    if name in ["__class__", "__init__", "__new__", "__repr__", "backward", "sequential"]: continue                                                                                                      # tensor.py                     :  3270    G: {}     L: {}
    setattr(Tensor, name, functools.wraps(fn)(_metadata_wrapper(fn)))                                                                                                                                    # tensor.py                     :  3271    G: {}     L: {}

      def _metadata_wrapper(fn):                                                                                                                                                                         # tensor.py                     :  3236    G: {}     L: {}
        return _wrapper                                                                                                                                                                                  # tensor.py                     :  3266    G: {}     L: {}

from tinygrad.engine.jit import TinyJit                       # noqa: F401                                                                                                                               # __init__.py                   :     2    G: {}     L: {}
from __future__ import annotations                                                                                                                                                                       # engine/jit.py                 :     1    G: {'__name__': 'tinygrad.engine.jit', '__doc__': None, '__package__': 'tinygrad.engine', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287af8730>, '__spec__': ModuleSpec(name='tinygrad.engine.jit', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287af8730>, origin='/home/lorinbaum/code/tinygrad/tinygrad/engine/jit.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/jit.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/engine/__pycache__/jit.cpython-310.pyc'}     L: {}
from typing import TypeVar, Generic, Callable, List, Tuple, Union, Dict, cast, Optional, Any                                                                                                             # engine/jit.py                 :     2    G: {}     L: {}
import functools, itertools, collections                                                                                                                                                                 # engine/jit.py                 :     3    G: {}     L: {}
from tinygrad.tensor import Tensor                                                                                                                                                                       # engine/jit.py                 :     4    G: {}     L: {}
from tinygrad.lazy import LazyBuffer                                                                                                                                                                     # engine/jit.py                 :     5    G: {}     L: {}
from tinygrad.helpers import flatten, merge_dicts, DEBUG, Context, GRAPH, BEAM, getenv, all_int, colored, JIT, dedup                                                                                     # engine/jit.py                 :     6    G: {}     L: {}
from tinygrad.device import Buffer, Compiled, Device                                                                                                                                                     # engine/jit.py                 :     7    G: {}     L: {}
from tinygrad.dtype import DType                                                                                                                                                                         # engine/jit.py                 :     8    G: {}     L: {}
from tinygrad.shape.shapetracker import ShapeTracker                                                                                                                                                     # engine/jit.py                 :     9    G: {}     L: {}
from tinygrad.shape.symbolic import Variable, sint, sym_infer                                                                                                                                            # engine/jit.py                 :    10    G: {}     L: {}
from tinygrad.engine.realize import ExecItem, capturing, EmptyOp, ViewOp, BufferXfer, CompiledRunner, Runner, _internal_memory_planner                                                                   # engine/jit.py                 :    11    G: {}     L: {}
from tinygrad.nn.state import get_parameters                                                                                                                                                             # engine/jit.py                 :    12    G: {}     L: {}
import math                                                                                                                                                                                              # nn/__init__.py                :     1    G: {'__name__': 'tinygrad.nn', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9330>, '__spec__': ModuleSpec(name='tinygrad.nn', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9330>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/__init__.py', submodule_search_locations=['/home/lorinbaum/code/tinygrad/tinygrad/nn']), '__path__': ['/home/lorinbaum/code/tinygrad/tinygrad/nn'], '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__init__.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/__init__.cpython-310.pyc'}     L: {}
from typing import Optional, Union, Tuple                                                                                                                                                                # nn/__init__.py                :     2    G: {}     L: {}
from tinygrad.tensor import Tensor                                                                                                                                                                       # nn/__init__.py                :     3    G: {}     L: {}
from tinygrad.helpers import prod                                                                                                                                                                        # nn/__init__.py                :     4    G: {}     L: {}
from tinygrad.nn import optim, state, datasets  # noqa: F401                                                                                                                                             # nn/__init__.py                :     5    G: {}     L: {}
from typing import List                                                                                                                                                                                  # nn/optim.py                   :     2    G: {'__name__': 'tinygrad.nn.optim', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9f00>, '__spec__': ModuleSpec(name='tinygrad.nn.optim', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9f00>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/optim.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/optim.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/optim.cpython-310.pyc'}     L: {}
from tinygrad.helpers import dedup, flatten, getenv                                                                                                                                                      # nn/optim.py                   :     3    G: {}     L: {}
from tinygrad.tensor import Tensor                                                                                                                                                                       # nn/optim.py                   :     4    G: {}     L: {}
from tinygrad.dtype import dtypes, least_upper_dtype                                                                                                                                                     # nn/optim.py                   :     5    G: {}     L: {}

import os, json, pathlib, zipfile, pickle, tarfile, struct                                                                                                                                               # nn/state.py                   :     1    G: {'__name__': 'tinygrad.nn.state', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287afa830>, '__spec__': ModuleSpec(name='tinygrad.nn.state', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287afa830>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/state.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/state.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/state.cpython-310.pyc'}     L: {}
from typing import Dict, Union, List, Optional, Any, Tuple                                                                                                                                               # nn/state.py                   :     2    G: {}     L: {}
from tinygrad.tensor import Tensor                                                                                                                                                                       # nn/state.py                   :     3    G: {}     L: {}
from tinygrad.dtype import dtypes                                                                                                                                                                        # nn/state.py                   :     4    G: {}     L: {}
from tinygrad.helpers import prod, argsort, DEBUG, Timing, CI, unwrap, GlobalCounters, tqdm                                                                                                              # nn/state.py                   :     5    G: {}     L: {}
from tinygrad.shape.view import strides_for_shape                                                                                                                                                        # nn/state.py                   :     6    G: {}     L: {}
from tinygrad.multi import MultiLazyBuffer                                                                                                                                                               # nn/state.py                   :     7    G: {}     L: {}
safe_dtypes = {"BOOL":dtypes.bool, "I8":dtypes.int8, "U8":dtypes.uint8, "I16":dtypes.int16, "U16":dtypes.uint16, "I32":dtypes.int, "U32":dtypes.uint,                                                    # nn/state.py                   :     9    G: {}     L: {}
               "I64":dtypes.int64, "U64":dtypes.uint64, "F16":dtypes.float16, "BF16":dtypes.bfloat16, "F32":dtypes.float32, "F64":dtypes.float64}
inverse_safe_dtypes = {v:k for k,v in safe_dtypes.items()}                                                                                                                                               # nn/state.py                   :    11    G: {}     L: {}
from collections import OrderedDict                                                                                                                                                                      # nn/state.py                   :    62    G: {}     L: {}
import gzip                                                                                                                                                                                              # nn/datasets.py                :     1    G: {'__name__': 'tinygrad.nn.datasets', '__doc__': None, '__package__': 'tinygrad.nn', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287afbc40>, '__spec__': ModuleSpec(name='tinygrad.nn.datasets', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287afbc40>, origin='/home/lorinbaum/code/tinygrad/tinygrad/nn/datasets.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/datasets.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/nn/__pycache__/datasets.cpython-310.pyc'}     L: {}
from tinygrad.tensor import Tensor                                                                                                                                                                       # nn/datasets.py                :     2    G: {}     L: {}
from tinygrad.helpers import fetch                                                                                                                                                                       # nn/datasets.py                :     3    G: {}     L: {}

BatchNorm2d = BatchNorm3d = BatchNorm                                                                                                                                                                    # nn/__init__.py                :    62    G: {}     L: {}

from dataclasses import dataclass                                                                                                                                                                        # engine/jit.py                 :    13    G: {}     L: {}
from weakref import WeakKeyDictionary                                                                                                                                                                    # engine/jit.py                 :    14    G: {}     L: {}

ReturnType = TypeVar('ReturnType')                                                                                                                                                                       # engine/jit.py                 :   130    G: {}     L: {}

@dataclass                                                                                                                                                                                               # engine/jit.py                 :   132    G: {}     L: {}
class CapturedJit(Generic[ReturnType]):
  ret: Any  # includes the Tensors or any other returned object                                                                                                                                          # engine/jit.py                 :   133    G: {}     L: {'__module__': 'tinygrad.engine.jit', '__qualname__': 'CapturedJit', '__annotations__': {}}
  jit_cache: List[ExecItem]                                                                                                                                                                              # engine/jit.py                 :   134    G: {}     L: {}
  input_replace: Dict[Tuple[int, int], int]                                                                                                                                                              # engine/jit.py                 :   135    G: {}     L: {}
  extra_view_inputs: List[Tuple[int, int, str, int, DType]]                                                                                                                                              # engine/jit.py                 :   136    G: {}     L: {}
  expected_names: List[Union[int, str]]                                                                                                                                                                  # engine/jit.py                 :   137    G: {}     L: {}
  expected_st_vars_dtype_device: List[Tuple[ShapeTracker, Tuple[Variable, ...], DType, str]]                                                                                                             # engine/jit.py                 :   138    G: {}     L: {}

from tinygrad.shape.symbolic import Variable                  # noqa: F401                                                                                                                               # __init__.py                   :     3    G: {}     L: {}
from tinygrad.dtype import dtypes                             # noqa: F401                                                                                                                               # __init__.py                   :     4    G: {}     L: {}
from tinygrad.helpers import GlobalCounters, fetch, Context   # noqa: F401                                                                                                                               # __init__.py                   :     5    G: {}     L: {}
from tinygrad.device import Device                            # noqa: F401                                                                                                                               # __init__.py                   :     6    G: {}     L: {}
a = (Tensor([1,2,3], device="CLANG") + 2)                                                                                                                                                                # ...grad.tensor.tolist_CLANG.py:     2    G: {}     L: {}

  class Tensor:                                                                                                                                                                                          # tensor.py                     :    92    G: {}     L: {}
    def __init__(self, data:Union[None, ConstType, List, Tuple, LazyBuffer, np.ndarray, bytes, MultiLazyBuffer, Variable],                                                                               # tensor.py                     :   108    G: {}     L: {}
                 device:Optional[Union[str, tuple, list]]=None, dtype:Optional[DTypeLike]=None, requires_grad:Optional[bool]=None):
      if dtype is not None: dtype = to_dtype(dtype)                                                                                                                                                      # tensor.py                     :   110    G: {}     L: repr failed
      assert dtype is None or isinstance(dtype, DType), f"invalid dtype {dtype}"                                                                                                                         # tensor.py                     :   111    G: {}     L: {}
      device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)                                                                       # tensor.py                     :   112    G: {}     L: {}

        class _Device:                                                                                                                                                                                   # device.py                     :    13    G: {}     L: {}
          # NOTE: you can't cache canonicalize in case Device.DEFAULT changes                                                                                                                            # device.py                     :    18    G: {}     L: {}
          def canonicalize(self, device:Optional[str]) -> str: return self._canonicalize(device) if device is not None else Device.DEFAULT

            class _Device:                                                                                                                                                                               # device.py                     :    13    G: {}     L: {}
              @functools.lru_cache(maxsize=None)  # this class is a singleton, pylint: disable=method-cache-max-size-none                                                                                # device.py                     :    16    G: {}     L: {}
              def _canonicalize(self, device:str) -> str: return (device.split(":", 1)[0].upper() + ((":"+device.split(":", 1)[1]) if ':' in device else '')).replace(":0", "")   # noqa: E501

      self.grad: Optional[Tensor] = None                                                                                                                                                                 # tensor.py                     :   115    G: {}     L: {}
      self.requires_grad: Optional[bool] = requires_grad                                                                                                                                                 # tensor.py                     :   119    G: {}     L: {}
      self._ctx: Optional[Function] = None                                                                                                                                                               # tensor.py                     :   122    G: {}     L: {}
      if isinstance(data, LazyBuffer): assert dtype is None or dtype == data.dtype, "dtype doesn't match, and casting isn't supported"                                                                   # tensor.py                     :   125    G: {}     L: {}
      elif isinstance(data, get_args(ConstType)): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data), device, data)                                                                    # tensor.py                     :   126    G: {}     L: {}
      elif isinstance(data, Variable): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data.unbind()[1]), device, data)                                                                   # tensor.py                     :   127    G: {}     L: {}
      elif isinstance(data, bytes): data = _frompy(data, dtypes.uint8 if dtype is None else dtype)                                                                                                       # tensor.py                     :   128    G: {}     L: {}
      elif isinstance(data, (list, tuple)):                                                                                                                                                              # tensor.py                     :   129    G: {}     L: {}
        if dtype is None:                                                                                                                                                                                # tensor.py                     :   130    G: {}     L: {}
          if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool                                                                                                     # tensor.py                     :   131    G: {}     L: {}

            def fully_flatten(l): return [item for sublist in l for item in (fully_flatten(sublist) if isinstance(sublist, (tuple, list)) else [sublist])]                                               # helpers.py                    :    35    G: {}     L: {}

          else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float                                                                                                                 # tensor.py                     :   132    G: {}     L: {}

            def all_int(t: Sequence[Any]) -> TypeGuard[Tuple[int, ...]]: return all(isinstance(s, int) for s in t)                                                                                       # helpers.py                    :    27    G: {}     L: {}

        if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata                                                                    # tensor.py                     :   133    G: {}     L: {}
        else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))                                                                                                                                 # tensor.py                     :   134    G: {}     L: {}

          def _to_np_dtype(dtype:DType) -> Optional[type]: return np.dtype(dtype.fmt).type if dtype.fmt is not None else None                                                                            # tensor.py                     :    49    G: {}     L: {}

          def _fromnp(x: np.ndarray) -> LazyBuffer:                                                                                                                                                      # tensor.py                     :    51    G: {}     L: {}
            ret = LazyBuffer.metaop(MetaOps.EMPTY, x.shape, _from_np_dtype(x.dtype), "NPY")                                                                                                              # tensor.py                     :    52    G: {}     L: {'x': array([1, 2, 3], dtype=int32)}

              def _from_np_dtype(npdtype:np.dtype) -> DType: return dtypes.fields()[np.dtype(npdtype).name]                                                                                              # tensor.py                     :    48    G: {}     L: {}

                class dtypes:                                                                                                                                                                            # dtype.py                      :    38    G: {}     L: {}
                  @staticmethod                                                                                                                                                                          # dtype.py                      :    67    G: {}     L: {}
                  def fields() -> Dict[str, DType]: return DTYPES_DICT

              class LazyBuffer:                                                                                                                                                                          # lazy.py                       :    26    G: {}     L: {}
                @staticmethod                                                                                                                                                                            # lazy.py                       :    70    G: {}     L: {}
                def metaop(op, shape:Tuple[sint,...], dtype:DTypeLike, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:
                  assert isinstance(src, tuple)                                                                                                                                                          # lazy.py                       :    71    G: {}     L: {'op': <MetaOps.EMPTY: 1>, 'shape': (3,), 'dtype': dtypes.int, 'device': 'NPY', 'arg': None, 'src': (), 'enable_cache': False}
                  return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)                                                                       # lazy.py                       :    72    G: {}     L: {}

                    @dataclass(frozen=True)                                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                    class ShapeTracker:
                      @staticmethod                                                                                                                                                                      # shape/shapetracker.py         :    52    G: {}     L: {}
                      def from_shape(shape:Tuple[sint, ...]) -> ShapeTracker: return ShapeTracker((View.create(shape),))

                        @dataclass(frozen=True)                                                                                                                                                          # shape/view.py                 :    85    G: {}     L: {}
                        class View:
                          @staticmethod                                                                                                                                                                  # shape/view.py                 :   101    G: {}     L: {}
                          @functools.lru_cache(maxsize=None)
                          def create(shape:Tuple[sint, ...], strides:Optional[Tuple[sint, ...]]=None, offset:sint=0, mask:Optional[Tuple[Tuple[sint, sint], ...]]=None):
                            if not all(s >= 0 for s in shape): raise ValueError(f"Trying to create View with negative dimension: {shape=}")                                                              # shape/view.py                 :   102    G: {}     L: {'shape': (3,), 'offset': 0, 'mask': None, 'strides': None}
                            strides = canonicalize_strides(shape, strides) if strides else strides_for_shape(shape)                                                                                      # shape/view.py                 :   103    G: {}     L: {}

                              @functools.lru_cache(maxsize=None)                                                                                                                                         # shape/view.py                 :    13    G: {}     L: {}
                              def strides_for_shape(shape:Tuple[sint, ...]) -> Tuple[sint, ...]:
                                if not shape: return ()                                                                                                                                                  # shape/view.py                 :    14    G: {}     L: {}
                                strides = tuple(itertools.accumulate(reversed(shape[1:]), operator.mul, initial=1))[::-1]                                                                                # shape/view.py                 :    15    G: {}     L: {}
                                return canonicalize_strides(shape, strides)                                                                                                                              # shape/view.py                 :    16    G: {}     L: {}

                                  @functools.lru_cache(maxsize=None)                                                                                                                                     # shape/view.py                 :     9    G: {}     L: {}
                                  def canonicalize_strides(shape:Tuple[sint, ...], strides:Tuple[sint, ...]) -> Tuple[sint, ...]:
                                    return tuple(0 if s == 1 else st for s, st in zip(shape, strides))                                                                                                   # shape/view.py                 :    10    G: {}     L: {}

                            if 0 in shape: return View(shape, (0,) * len(shape), offset=0, mask=None, contiguous=True)                                                                                   # shape/view.py                 :   105    G: {}     L: {'offset': 0, 'mask': None, 'strides': (1,)}
                            if mask is not None and all(m == (0,s) for m,s in zip(mask, shape)): mask = None                                                                                             # shape/view.py                 :   107    G: {}     L: {}
                            if mask and any(elim := [not (b+1 < e) for b,e in mask]):                                                                                                                    # shape/view.py                 :   111    G: {}     L: {}
                            contiguous = offset == 0 and mask is None and strides == strides_for_shape(shape)                                                                                            # shape/view.py                 :   116    G: {}     L: {}
                            return View(shape, strides, offset, mask, contiguous)                                                                                                                        # shape/view.py                 :   117    G: {}     L: {}

                    def create_lazybuffer(device:str, st:ShapeTracker, dtype:DTypeLike, op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),                                              # lazy.py                       :    12    G: {}     L: {}
                                          base:Optional[LazyBuffer]=None, enable_cache=bool(getenv("LAZYCACHE", 1))):
                      if st.size == 0: op, arg, srcs, base = MetaOps.CONST, 0, (), None                                                                                                                  # lazy.py                       :    14    G: {}     L: {'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'srcs': (), 'base': None}

                        @dataclass(frozen=True)                                                                                                                                                          # shape/shapetracker.py         :    36    G: {}     L: {}
                        class ShapeTracker:
                          @property                                                                                                                                                                      # shape/shapetracker.py         :    64    G: {}     L: {}
                          def size(self) -> int: return self.views[-1].size()

                            @dataclass(frozen=True)                                                                                                                                                      # shape/view.py                 :    85    G: {}     L: {}
                            class View:
                              @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                                           # shape/view.py                 :    93    G: {}     L: {}
                              def size(self) -> int:
                                ret = prod([x.max if isinstance(x, Node) else x for x in self.shape])                                                                                                    # shape/view.py                 :    95    G: {}     L: {'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}

                                  # NOTE: it returns int 1 if x is empty regardless of the type of x                                                                                                     # helpers.py                    :    13    G: {}     L: {}
                                  def prod(x:Iterable[T]) -> Union[T,int]: return functools.reduce(operator.mul, x, 1)

                                assert isinstance(ret, int), f"{ret=} is not int"                                                                                                                        # shape/view.py                 :    96    G: {}     L: {}
                                return ret                                                                                                                                                               # shape/view.py                 :    97    G: {}     L: {}

                      dtype = to_dtype(dtype)                                                                                                                                                            # lazy.py                       :    15    G: {}     L: {}

                        def to_dtype(dtype:DTypeLike) -> DType: return dtype if isinstance(dtype, DType) else getattr(dtypes, dtype)                                                                     # dtype.py                      :   104    G: {}     L: {}

                      if op is MetaOps.CONST: arg, enable_cache = dtypes.as_const(arg, dtype) if not isinstance(arg, Variable) else arg, True                                                            # lazy.py                       :    16    G: {}     L: {}
                      cache_key = (device, st, dtype, op, arg, tuple(ref(x) for x in srcs)) if base is None else (st, ref(base))                                                                         # lazy.py                       :    18    G: {}     L: {}
                      if enable_cache and (rret := lazycache.get(cache_key, None)): return rret                                                                                                          # lazy.py                       :    19    G: {}     L: {}
                      ret = LazyBuffer(device, st, dtype, op, arg, srcs, base=base, metadata=_METADATA.get())                                                                                            # lazy.py                       :    21    G: {}     L: {}

                        class LazyBuffer:                                                                                                                                                                # lazy.py                       :    26    G: {}     L: {}
                          def __init__(self, device:str, st:ShapeTracker, dtype:DTypeLike,                                                                                                               # lazy.py                       :    27    G: {}     L: {}
                                       op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),
                                       base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):
                            self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, to_dtype(dtype), st.shape, st.size, metadata                                            # lazy.py                       :    30    G: {}     L: repr failed

                              @dataclass(frozen=True)                                                                                                                                                    # shape/shapetracker.py         :    36    G: {}     L: {}
                              class ShapeTracker:
                                @property                                                                                                                                                                # shape/shapetracker.py         :    61    G: {}     L: {}
                                def shape(self) -> Tuple[sint, ...]: return self.views[-1].shape

                            self._base: Optional[LazyBuffer] = None                                                                                                                                      # lazy.py                       :    31    G: {}     L: {}
                            if base is None:                                                                                                                                                             # lazy.py                       :    32    G: {}     L: {}
                              self.op, self.arg, self.srcs = op, arg, srcs  # this is a LazyOp, except the src is LazyBuffers and not LazyOps                                                            # lazy.py                       :    34    G: {}     L: {}
                              assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"                                                                # lazy.py                       :    35    G: {}     L: {}
                              if self.op is MetaOps.VIEW:                                                                                                                                                # lazy.py                       :    37    G: {}     L: {}
                                self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, self.dtype)                                                                # lazy.py                       :    41    G: {}     L: {}

                                  class Buffer:                                                                                                                                                          # device.py                     :    52    G: {}     L: {}
                                    def __init__(self, device:str, size:int, dtype:DType, opaque:Any=None, options:Optional[BufferOptions]=None,                                                         # device.py                     :    53    G: {}     L: {}
                                                 initial_value:Optional[bytes]=None, lb_refcount=0, base:Optional[Buffer]=None, offset:int=0, preallocate=False):
                                      assert isinstance(dtype, DType)                                                                                                                                    # device.py                     :    55    G: {}     L: repr failed
                                      if isinstance(dtype, ImageDType): options = BufferOptions(image=dtype) # TODO: image hack shouldn't be here. where should it be?                                   # device.py                     :    56    G: {}     L: {}
                                      self.device, self.size, self.dtype, self.options, self.offset = device, size, dtype, options, offset                                                               # device.py                     :    57    G: {}     L: {}
                                      if base is None:                                                                                                                                                   # device.py                     :    58    G: {}     L: {}
                                        assert offset == 0, "base buffers can't have offset"                                                                                                             # device.py                     :    59    G: {}     L: {}
                                        self._base = None                                                                                                                                                # device.py                     :    60    G: {}     L: {}
                                        self._lb_refcount = lb_refcount                                                                                                                                  # device.py                     :    61    G: {}     L: {}
                                        if opaque is not None: self.allocate(opaque)                                                                                                                     # device.py                     :    62    G: {}     L: {}
                                        if initial_value is not None:                                                                                                                                    # device.py                     :    63    G: {}     L: {}
                                      if preallocate: self.allocate()                                                                                                                                    # device.py                     :    70    G: {}     L: {}

                              self.buffer.ref(1)                                                                                                                                                         # lazy.py                       :    42    G: {}     L: {}

                                class Buffer:                                                                                                                                                            # device.py                     :    52    G: {}     L: {}
                                  def ref(self, cnt): self.base._lb_refcount += cnt                                                                                                                      # device.py                     :    75    G: {}     L: {}

                                    class Buffer:                                                                                                                                                        # device.py                     :    52    G: {}     L: {}
                                      @property                                                                                                                                                          # device.py                     :    72    G: {}     L: {}
                                      def base(self) -> Buffer: return self._base if self._base is not None else self

                              self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None                                                                                     # lazy.py                       :    43    G: {}     L: {}
                              self.forced_realize = False                                                                                                                                                # lazy.py                       :    44    G: {}     L: {}

                      if enable_cache: lazycache[cache_key] = ret                                                                                                                                        # lazy.py                       :    22    G: {}     L: {'enable_cache': False, 'cache_key': ('NPY', ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), dtypes.int, <MetaOps.EMPTY: 1>, None, ()), 'ret': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, None)>}
                      return ret                                                                                                                                                                         # lazy.py                       :    23    G: {}     L: {}

            ret.buffer.allocate(x)                                                                                                                                                                       # tensor.py                     :    54    G: {}     L: {'x': array([1, 2, 3], dtype=int32), 'ret': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, None)>}

              class Buffer:                                                                                                                                                                              # device.py                     :    52    G: {}     L: {}
                def allocate(self, opaque=None) -> Buffer:                                                                                                                                               # device.py                     :    78    G: {}     L: {}
                  assert not hasattr(self, '_buf'), "can't allocate already allocated buffer"                                                                                                            # device.py                     :    79    G: {}     L: {'opaque': array([1, 2, 3], dtype=int32)}
                  self.allocator = Device[self.device].allocator                                                                                                                                         # device.py                     :    80    G: {}     L: {}

                    class _Device:                                                                                                                                                                       # device.py                     :    13    G: {}     L: {}
                      def __getitem__(self, ix:str) -> Compiled: return self.__get_canonicalized_item(self.canonicalize(ix))                                                                             # device.py                     :    19    G: {}     L: {}

                        class _Device:                                                                                                                                                                   # device.py                     :    13    G: {}     L: {}
                          @functools.lru_cache(maxsize=None)  # this class is a singleton, pylint: disable=method-cache-max-size-none                                                                    # device.py                     :    21    G: {}     L: {}
                          def __get_canonicalized_item(self, ix:str) -> Compiled:
                            assert ((cpn:=multiprocessing.current_process().name) == "MainProcess") or ix.split(":")[0] in ["DISK", "NPY"], \                                                            # device.py                     :    22    G: {}     L: {'ix': 'NPY'}
                              f"can only open device {ix} from parent, not {cpn}"
                            x = ix.split(":")[0].upper()                                                                                                                                                 # device.py                     :    24    G: {}     L: {}
                            ret = [cls for cname, cls in inspect.getmembers(importlib.import_module(f'tinygrad.runtime.ops_{x.lower()}')) if (cname.lower() == x.lower() + "device") and x in self._devices][0](ix)  # noqa: E501 # device.py                     :    25    G: {}     L: {}
                        import numpy as np                                                                                                                                                               # runtime/ops_npy.py            :     1    G: {'__name__': 'tinygrad.runtime.ops_npy', '__doc__': None, '__package__': 'tinygrad.runtime', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9600>, '__spec__': ModuleSpec(name='tinygrad.runtime.ops_npy', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287af9600>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_npy.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_npy.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/__pycache__/ops_npy.cpython-310.pyc'}     L: {}
                        from tinygrad.helpers import flat_mv                                                                                                                                             # runtime/ops_npy.py            :     2    G: {}     L: {}
                        from tinygrad.device import Compiled, Allocator                                                                                                                                  # runtime/ops_npy.py            :     3    G: {}     L: {}

                              class NpyDevice(Compiled):                                                                                                                                                 # runtime/ops_npy.py            :     8    G: {}     L: {}
                                def __init__(self, device:str): super().__init__(device, NpyAllocator(), None, None, None)                                                                               # runtime/ops_npy.py            :     9    G: {}     L: {}

                                  class Compiled:                                                                                                                                                        # device.py                     :   186    G: {}     L: {}
                                    def __init__(self, device:str, allocator:Allocator, renderer:Optional[Renderer], compiler:Optional[Compiler], runtime, graph=None):                                  # device.py                     :   187    G: {}     L: {}
                                      self.dname, self.allocator, self.compiler, self.runtime, self.graph = device, allocator, compiler or Compiler(), runtime, graph                                    # device.py                     :   188    G: {}     L: {'self': <tinygrad.runtime.ops_npy.NpyDevice object at 0x7c929e74f850>, 'device': 'NPY', 'allocator': <tinygrad.runtime.ops_npy.NpyAllocator object at 0x7c9287af96f0>, 'renderer': None, 'compiler': None, 'runtime': None, 'graph': None}

                                        class Compiler:                                                                                                                                                  # device.py                     :   176    G: {}     L: {}
                                          def __init__(self, cachekey:Optional[str]=None): self.cachekey = None if getenv("DISABLE_COMPILER_CACHE") else cachekey                                        # device.py                     :   177    G: {}     L: {}

                                      self.renderer = renderer or Renderer()                                                                                                                             # device.py                     :   189    G: {}     L: {'self': <tinygrad.runtime.ops_npy.NpyDevice object at 0x7c929e74f850>, 'device': 'NPY', 'allocator': <tinygrad.runtime.ops_npy.NpyAllocator object at 0x7c9287af96f0>, 'renderer': None, 'compiler': None, 'runtime': None, 'graph': None}

                            if DEBUG >= 1: print(f"opened device {ix} from pid:{os.getpid()}")                                                                                                           # device.py                     :    26    G: {}     L: {'ix': 'NPY', 'self': <tinygrad.device._Device object at 0x7c9298d3ae90>, 'cpn': 'MainProcess', 'x': 'NPY', 'ret': <tinygrad.runtime.ops_npy.NpyDevice object at 0x7c929e74f850>}

                            return ret                                                                                                                                                                   # device.py                     :    27    G: {}     L: {}

                  if self._base is not None:                                                                                                                                                             # device.py                     :    81    G: {}     L: {'self': <buf real:False device:NPY size:3 dtype:dtypes.int offset:0>, 'opaque': array([1, 2, 3], dtype=int32)}
                    self._buf = opaque if opaque is not None else self.allocator.alloc(self.nbytes, self.options)                                                                                        # device.py                     :    86    G: {}     L: {}
                    if not self.device.startswith("DISK"): GlobalCounters.mem_used += self.nbytes                                                                                                        # device.py                     :    87    G: {}     L: {}

                      class Buffer:                                                                                                                                                                      # device.py                     :    52    G: {}     L: {}
                        @property                                                                                                                                                                        # device.py                     :    99    G: {}     L: {}
                        def nbytes(self): return self.size*self.dtype.itemsize

                  return self                                                                                                                                                                            # device.py                     :    88    G: {}     L: {'opaque': array([1, 2, 3], dtype=int32)}

            del ret.srcs                                                                                                                                                                                 # tensor.py                     :    55    G: {}     L: {}
            return ret                                                                                                                                                                                   # tensor.py                     :    56    G: {}     L: {}

      if not isinstance(data, (LazyBuffer, MultiLazyBuffer)):                                                                                                                                            # tensor.py                     :   141    G: {}     L: repr failed
      if isinstance(device, tuple):                                                                                                                                                                      # tensor.py                     :   145    G: {}     L: {}
        self.lazydata = data if data.device == device else data.copy_to_device(device)                                                                                                                   # tensor.py                     :   153    G: {}     L: {}

          class LazyBuffer:                                                                                                                                                                              # lazy.py                       :    26    G: {}     L: {}
            def copy_to_device(self, device:str, force: bool = False) -> LazyBuffer:                                                                                                                     # lazy.py                       :   119    G: {}     L: {}
              if self.device == device: return self                                                                                                                                                      # lazy.py                       :   121    G: {}     L: {'self': <LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>, 'device': 'CLANG', 'force': False}
              if not force and self.st.contiguous and self.size == self.base.size and not self.base.realized and self.base.op is MetaOps.COPY:                                                           # lazy.py                       :   124    G: {}     L: {}

                @dataclass(frozen=True)                                                                                                                                                                  # shape/shapetracker.py         :    36    G: {}     L: {}
                class ShapeTracker:
                  @property                                                                                                                                                                              # shape/shapetracker.py         :    55    G: {}     L: {}
                  def contiguous(self) -> bool: return len(self.views) == 1 and self.views[0].contiguous

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  # NOTE: this has to be a function to prevent self reference                                                                                                                            # lazy.py                       :    63    G: {}     L: {}
                  @property
                  def base(self) -> LazyBuffer: return self._base if self._base is not None else self

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  @property                                                                                                                                                                              # lazy.py                       :    57    G: {}     L: {}
                  def realized(self) -> Optional[Buffer]:
                    return self.buffer if self._base is None and not hasattr(self, 'srcs') else None                                                                                                     # lazy.py                       :    59    G: {}     L: {}

              if self.is_unrealized_const():                                                                                                                                                             # lazy.py                       :   128    G: {}     L: {'device': 'CLANG', 'force': False}

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  def is_unrealized_const(self): return self.base.realized is None and self.base.op is MetaOps.CONST and not isinstance(self.base.arg, Variable)                                         # lazy.py                       :   113    G: {}     L: {}

              if prod(self.st.shape) < prod(self.base.st.shape): return self.contiguous()._copy(device)                                                                                                  # lazy.py                       :   132    G: {}     L: {'device': 'CLANG', 'force': False}

              return self.base._copy(device)._view(self.st)                                                                                                                                              # lazy.py                       :   135    G: {}     L: {}

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  def _copy(self, device:str) -> LazyBuffer:                                                                                                                                             # lazy.py                       :   116    G: {}     L: {}
                    return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, MetaOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)                                     # lazy.py                       :   117    G: {}     L: {}

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  def _view(self, new_st:ShapeTracker) -> LazyBuffer:                                                                                                                                    # lazy.py                       :   208    G: {}     L: {}
                    if self.st.size == 0 or (new_st.views[-1].mask is not None and any((x[1]-x[0]) == 0 for x in new_st.views[-1].mask)):                                                                # lazy.py                       :   209    G: {}     L: {'self': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'new_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}

                    if new_st.contiguous and self.base.shape == new_st.shape: return self.base                                                                                                           # lazy.py                       :   211    G: {}     L: {}

  def _metadata_wrapper(fn):                                                                                                                                                                             # tensor.py                     :  3236    G: {}     L: {}
    def _wrapper(*args, **kwargs):                                                                                                                                                                       # tensor.py                     :  3237    G: {}     L: {}
      if _METADATA.get() is not None: return fn(*args, **kwargs)                                                                                                                                         # tensor.py                     :  3238    G: {}     L: {'args': (<Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 2), 'kwargs': {}}
      if TRACEMETA >= 2:                                                                                                                                                                                 # tensor.py                     :  3240    G: {}     L: {}

      else: caller = ""                                                                                                                                                                                  # tensor.py                     :  3260    G: {}     L: {}
      token = _METADATA.set(Metadata(name=fn.__name__, caller=caller))                                                                                                                                   # tensor.py                     :  3262    G: {}     L: {}
      ret = fn(*args, **kwargs)                                                                                                                                                                          # tensor.py                     :  3263    G: {}     L: {}

        class Tensor:                                                                                                                                                                                    # tensor.py                     :    92    G: {}     L: {}
          def __add__(self, x) -> Tensor: return self.add(x)                                                                                                                                             # tensor.py                     :  2745    G: {}     L: {}

          class Tensor:                                                                                                                                                                                  # tensor.py                     :    92    G: {}     L: {}
            def add(self, x:Union[Tensor, ConstType], reverse=False) -> Tensor:                                                                                                                          # tensor.py                     :  2495    G: {}     L: {}
              return F.Add.apply(*self._broadcasted(x, reverse))                                                                                                                                         # tensor.py                     :  2513    G: {}     L: {'reverse': False}

            class Tensor:                                                                                                                                                                                # tensor.py                     :    92    G: {}     L: {}
              def _broadcasted(self, y:Union[Tensor, Node, ConstType], reverse:bool=False, match_dtype:bool=True) -> Tuple[Tensor, Tensor]:                                                              # tensor.py                     :  2471    G: {}     L: {}
                x: Tensor = self                                                                                                                                                                         # tensor.py                     :  2472    G: {}     L: {'y': 2, 'match_dtype': True}
                if not isinstance(y, Tensor):                                                                                                                                                            # tensor.py                     :  2473    G: {}     L: {}
                  assert isinstance(y, (*get_args(ConstType), Node)), f"{type(y)=}, {y=}"                                                                                                                # tensor.py                     :  2475    G: {}     L: {}
                  if isinstance(x.dtype, ImageDType) or dtypes.is_float(x.dtype) or (dtypes.is_int(x.dtype) and isinstance(y, int)): y_dtype = x.dtype                                                   # tensor.py                     :  2476    G: {}     L: {}

                    class Tensor:                                                                                                                                                                        # tensor.py                     :    92    G: {}     L: {}
                      @property                                                                                                                                                                          # tensor.py                     :   184    G: {}     L: {}
                      def dtype(self) -> DType: return self.lazydata.dtype

                  if isinstance(y, Node): y = Tensor.from_node(y, device=x.device)                                                                                                                       # tensor.py                     :  2478    G: {}     L: {'y': 2, 'reverse': False, 'match_dtype': True, 'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y_dtype': dtypes.int}
                  else: y = Tensor(dtypes.as_const(y, y_dtype), x.device, y_dtype, requires_grad=False)                                                                                                  # tensor.py                     :  2479    G: {}     L: {}

                    class Tensor:                                                                                                                                                                        # tensor.py                     :    92    G: {}     L: {}
                      @property                                                                                                                                                                          # tensor.py                     :   178    G: {}     L: {}
                      def device(self) -> Union[str, Tuple[str, ...]]: return self.lazydata.device

              def _metaop(op, shape:Tuple[sint,...], dtype:DType, device:Union[str, Tuple[str, ...]], arg=None, src:Tuple[LazyBuffer, ...]=()):                                                          # tensor.py                     :    44    G: {}     L: {}
                if isinstance(device, str): return LazyBuffer.metaop(op, shape, dtype, device, arg, src)                                                                                                 # tensor.py                     :    45    G: {}     L: {'arg': 2, 'dtype': dtypes.int, 'op': <MetaOps.CONST: 2>, 'shape': (), 'src': ()}

                if match_dtype and x.dtype != y.dtype:                                                                                                                                                   # tensor.py                     :  2481    G: {}     L: {'self': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>, 'reverse': False, 'match_dtype': True, 'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y_dtype': dtypes.int}

                if reverse: x, y = y, x                                                                                                                                                                  # tensor.py                     :  2485    G: {}     L: {}
                out_shape = _broadcast_shape(x.shape, y.shape)                                                                                                                                           # tensor.py                     :  2488    G: {}     L: {}

                  class Tensor:                                                                                                                                                                          # tensor.py                     :    92    G: {}     L: {}
                    @property                                                                                                                                                                            # tensor.py                     :   181    G: {}     L: {}
                    def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape

                  def _broadcast_shape(*shapes:Tuple[sint, ...]) -> Tuple[sint, ...]:                                                                                                                    # tensor.py                     :    89    G: {}     L: {}
                    return tuple(0 if 0 in nth_dim_sizes else max(nth_dim_sizes) for nth_dim_sizes in zip(*_pad_left(*shapes)))                                                                          # tensor.py                     :    90    G: {}     L: {'shapes': ((3,), ())}

                      def _pad_left(*shapes:Tuple[sint, ...]) -> Tuple[Tuple[sint, ...], ...]:                                                                                                           # tensor.py                     :    86    G: {}     L: {}
                        max_dim = max(len(shape) for shape in shapes)                                                                                                                                    # tensor.py                     :    87    G: {}     L: {'shapes': ((3,), ())}
                        return tuple((1,) * (max_dim - len(shape)) + shape for shape in shapes)                                                                                                          # tensor.py                     :    88    G: {}     L: {}

                return x._broadcast_to(out_shape), y._broadcast_to(out_shape)                                                                                                                            # tensor.py                     :  2489    G: {}     L: {'self': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>, 'reverse': False, 'match_dtype': True, 'x': <Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 'y_dtype': dtypes.int, 'out_shape': (3,)}

              class Tensor:                                                                                                                                                                              # tensor.py                     :    92    G: {}     L: {}
                # ***** broadcasted elementwise ops *****                                                                                                                                                # tensor.py                     :  2462    G: {}     L: {}
                def _broadcast_to(self, shape:Tuple[sint, ...]) -> Tensor:
                  if self.shape == shape: return self                                                                                                                                                    # tensor.py                     :  2463    G: {}     L: {'shape': (3,)}

              class Tensor:                                                                                                                                                                              # tensor.py                     :    92    G: {}     L: {}
                # ***** broadcasted elementwise ops *****                                                                                                                                                # tensor.py                     :  2462    G: {}     L: {}
                def _broadcast_to(self, shape:Tuple[sint, ...]) -> Tensor:
                  if self.shape == shape: return self                                                                                                                                              # OLD # tensor.py                     :  2463
                  if self.ndim > len(shape): raise ValueError(f"cannot broadcast tensor to fewer dimensions. shape={self.shape} to {shape=}")                                                            # tensor.py                     :  2464    G: {}     L: {'self': <Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>}

                    class Tensor:                                                                                                                                                                        # tensor.py                     :    92    G: {}     L: {}
                      @property                                                                                                                                                                          # tensor.py                     :  2962    G: {}     L: {}
                      def ndim(self) -> int:
                        return len(self.shape)                                                                                                                                                           # tensor.py                     :  2971    G: {}     L: {}

                  padded, _ = _pad_left(self.shape, shape)                                                                                                                                               # tensor.py                     :  2466    G: {}     L: {'shape': (3,)}

                  if any(from_ != 1 and from_ != to for from_,to in zip(padded, shape)): raise ValueError(f"cannot broadcast from shape={self.shape} to {shape=}")                                       # tensor.py                     :  2468    G: {}     L: {}
                  return F.Expand.apply(self.reshape(padded), shape=shape)                                                                                                                               # tensor.py                     :  2469    G: {}     L: {}

                class Tensor:                                                                                                                                                                            # tensor.py                     :    92    G: {}     L: {}
                  def reshape(self, shape, *args) -> Tensor:                                                                                                                                             # tensor.py                     :   786    G: {}     L: {}
                    new_shape = tuple([s if s is not None else self.shape[i] for i,s in enumerate(argfix(shape, *args))])                                                                                # tensor.py                     :   797    G: {}     L: {'shape': (1,), 'args': ()}

                      def argfix(*x):                                                                                                                                                                    # helpers.py                    :    20    G: {}     L: {}
                        if x and x[0].__class__ in (tuple, list):                                                                                                                                        # helpers.py                    :    21    G: {}     L: {'x': ((1,),)}
                          if len(x) != 1: raise ValueError(f"bad arg {x}")                                                                                                                               # helpers.py                    :    22    G: {}     L: {}
                          return tuple(x[0])                                                                                                                                                             # helpers.py                    :    23    G: {}     L: {}

                    if (c := new_shape.count(-1)) > 1: raise RuntimeError(f"only one dimension can be inferred using -1, getting {new_shape}")                                                           # tensor.py                     :   799    G: {}     L: {}
                    if c: new_shape = tuple([-prod(self.shape) // prod(new_shape) if s == -1 else s for s in new_shape])                                                                                 # tensor.py                     :   800    G: {}     L: {}
                    return F.Reshape.apply(self, shape=new_shape) if new_shape != self.shape else self                                                                                                   # tensor.py                     :   801    G: {}     L: {}

                      class Function:                                                                                                                                                                    # tensor.py                     :    23    G: {}     L: {}
                        @classmethod                                                                                                                                                                     # tensor.py                     :    35    G: {}     L: {}
                        def apply(fxn:Type[Function], *x:Tensor, **kwargs) -> Tensor:
                          ctx = fxn(x[0].device, *x, metadata=_METADATA.get())                                                                                                                           # tensor.py                     :    36    G: {}     L: {'fxn': <class 'tinygrad.function.Reshape'>, 'x': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,), 'kwargs': {'shape': (1,)}}

                            class Function:                                                                                                                                                              # tensor.py                     :    23    G: {}     L: {}
                              def __init__(self, device:Union[str, Tuple[str, ...]], *tensors:Tensor, metadata:Optional[Metadata]=None):                                                                 # tensor.py                     :    24    G: {}     L: {}
                                self.device = device                                                                                                                                                     # tensor.py                     :    25    G: {}     L: {'self': <tinygrad.function.Reshape object at 0x7c9287b4ac20>, 'metadata': __add__, 'tensors': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,)}
                                self.needs_input_grad = [t.requires_grad for t in tensors]                                                                                                               # tensor.py                     :    26    G: {}     L: {}
                                self.requires_grad = True if any(self.needs_input_grad) else None if None in self.needs_input_grad else False                                                            # tensor.py                     :    27    G: {}     L: {}
                                if self.requires_grad: self.parents = tensors                                                                                                                            # tensor.py                     :    28    G: {}     L: {}
                                self.metadata = metadata                                                                                                                                                 # tensor.py                     :    29    G: {}     L: {}

                          ret = Tensor.__new__(Tensor)                                                                                                                                                   # tensor.py                     :    37    G: {}     L: {'fxn': <class 'tinygrad.function.Reshape'>, 'x': (<Tensor <LB CLANG () int (<MetaOps.CONST: 2>, None)> on CLANG with grad None>,), 'kwargs': {'shape': (1,)}, 'ctx': <tinygrad.function.Reshape object at 0x7c9287b4ac20>}
                          ret.lazydata, ret.requires_grad, ret.grad = ctx.forward(*[t.lazydata for t in x], **kwargs), ctx.requires_grad, None                                                           # tensor.py                     :    38    G: {}     L: {}

                            class Reshape(Function):                                                                                                                                                     # function.py                   :   187    G: {}     L: {}
                              def forward(self, x:LazyBuffer, shape:Tuple[int, ...]) -> LazyBuffer:                                                                                                      # function.py                   :   188    G: {}     L: {}
                                self.input_shape = x.shape                                                                                                                                               # function.py                   :   189    G: {}     L: {'self': <tinygrad.function.Reshape object at 0x7c9287b4ac20>, 'x': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'shape': (1,)}
                                return x.reshape(shape)                                                                                                                                                  # function.py                   :   190    G: {}     L: {}

                                  class LazyBuffer:                                                                                                                                                      # lazy.py                       :    26    G: {}     L: {}
                                    def reshape(self, arg:Tuple[sint, ...]): return self._view(self.st.reshape(arg))                                                                                     # lazy.py                       :   214    G: {}     L: {}

                                      @dataclass(frozen=True)                                                                                                                                            # shape/shapetracker.py         :    36    G: {}     L: {}
                                      class ShapeTracker:
                                        def reshape(self, new_shape: Tuple[sint, ...]) -> ShapeTracker:                                                                                                  # shape/shapetracker.py         :   154    G: {}     L: {}
                                          if getenv("MERGE_VIEW", 1) and (new_view := self.views[-1].reshape(new_shape)) is not None: return ShapeTracker(self.views[0:-1] + (new_view,))                # shape/shapetracker.py         :   155    G: {}     L: {'self': ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),)), 'new_shape': (1,)}

                                            @dataclass(frozen=True)                                                                                                                                      # shape/view.py                 :    85    G: {}     L: {}
                                            class View:
                                              @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                           # shape/view.py                 :   267    G: {}     L: {}
                                              def reshape(self, new_shape: Tuple[sint, ...]) -> Optional[View]:
                                                if self.shape == new_shape: return self                                                                                                                  # shape/view.py                 :   268    G: {}     L: {'self': View(shape=(), strides=(), offset=0, mask=None, contiguous=True), 'new_shape': (1,)}
                                                assert all(x >= 0 for x in new_shape), f"shape can't contain negative numbers {new_shape}"                                                               # shape/view.py                 :   270    G: {}     L: {}
                                                if 0 in self.shape:                                                                                                                                      # shape/view.py                 :   271    G: {}     L: {}
                                                if (self_all_int := all_int(self.shape)):                                                                                                                # shape/view.py                 :   275    G: {}     L: {}

                                                  assert all(isinstance(s, (int, Variable)) for s in new_shape), f"{self.shape=} -> {new_shape=} contains non (int, Variable) dim"                       # shape/view.py                 :   276    G: {}     L: {}
                                                  if prod(self.shape) != prod([s if isinstance(s, int) else cast(Variable,s).val for s in new_shape]):                                                   # shape/view.py                 :   277    G: {}     L: {}

                                                if new_shape == () and self.mask and any(mx==my for (mx,my) in self.mask): return None                                                                   # shape/view.py                 :   280    G: {}     L: {}
                                                if self.contiguous: return View.create(new_shape)                                                                                                        # shape/view.py                 :   283    G: {}     L: {}

                                      class LazyBuffer:                                                                                                                                                  # lazy.py                       :    26    G: {}     L: {}
                                        def _view(self, new_st:ShapeTracker) -> LazyBuffer:                                                                                                              # lazy.py                       :   208    G: {}     L: {}
                                          if self.st.size == 0 or (new_st.views[-1].mask is not None and any((x[1]-x[0]) == 0 for x in new_st.views[-1].mask)):                                    # OLD # lazy.py                       :   209
                                          if new_st.contiguous and self.base.shape == new_st.shape: return self.base                                                                               # OLD # lazy.py                       :   211
                                          return create_lazybuffer(self.device, new_st, self.dtype, base=self.base)                                                                                      # lazy.py                       :   212    G: {}     L: {'new_st': ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))}

                                        class LazyBuffer:                                                                                                                                                # lazy.py                       :    26    G: {}     L: {}
                                          def __init__(self, device:str, st:ShapeTracker, dtype:DTypeLike,                                                                                               # lazy.py                       :    27    G: {}     L: {}
                                                       op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),
                                                       base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):
                                            self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, to_dtype(dtype), st.shape, st.size, metadata                      # OLD # lazy.py                       :    30
                                            self._base: Optional[LazyBuffer] = None                                                                                                                # OLD # lazy.py                       :    31
                                            if base is None:                                                                                                                                       # OLD # lazy.py                       :    32
                                              self.op, self.arg, self.srcs = op, arg, srcs  # this is a LazyOp, except the src is LazyBuffers and not LazyOps                                      # OLD # lazy.py                       :    34
                                              assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"                                          # OLD # lazy.py                       :    35
                                              if self.op is MetaOps.VIEW:                                                                                                                          # OLD # lazy.py                       :    37
                                                self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, self.dtype)                                          # OLD # lazy.py                       :    41
                                              self.buffer.ref(1)                                                                                                                                   # OLD # lazy.py                       :    42
                                              self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None                                                               # OLD # lazy.py                       :    43
                                              self.forced_realize = False                                                                                                                          # OLD # lazy.py                       :    44
                                              assert base.base == base, "base must be a base itself"                                                                                                     # lazy.py                       :    47    G: {}     L: repr failed

                                              self._base = base                                                                                                                                          # lazy.py                       :    48    G: {}     L: {}

                          ret._ctx = ctx if ctx.requires_grad and not Tensor.no_grad else None  # used by autograd engine                                                                                # tensor.py                     :    39    G: {}     L: {}
                          return ret                                                                                                                                                                     # tensor.py                     :    40    G: {}     L: {}

                # NOTE: this is sum in reverse                                                                                                                                                           # function.py                   :   179    G: {}     L: {}
                class Expand(Function):
                  def forward(self, x:LazyBuffer, shape:Tuple[int, ...]) -> LazyBuffer:                                                                                                                  # function.py                   :   180    G: {}     L: {}
                    self.expanded_axis = tuple(i for i, (si, so) in enumerate(zip(x.shape, shape)) if si != so)                                                                                          # function.py                   :   181    G: {}     L: {'self': <tinygrad.function.Expand object at 0x7c9287b4ab30>, 'x': <LB CLANG (1,) int ShapeTracker(views=(View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True),))>, 'shape': (3,)}
                    return x.expand(shape)                                                                                                                                                               # function.py                   :   182    G: {}     L: {}

                      class LazyBuffer:                                                                                                                                                                  # lazy.py                       :    26    G: {}     L: {}
                        def expand(self, arg:Tuple[sint, ...]): return self._view(self.st.expand(arg))                                                                                                   # lazy.py                       :   216    G: {}     L: {}

                          @dataclass(frozen=True)                                                                                                                                                        # shape/shapetracker.py         :    36    G: {}     L: {}
                          class ShapeTracker:
                            def expand(self, new_shape: Tuple[sint, ...]) -> ShapeTracker: return ShapeTracker(self.views[0:-1] + (self.views[-1].expand(new_shape), ))                                  # shape/shapetracker.py         :   150    G: {}     L: {}

                              @dataclass(frozen=True)                                                                                                                                                    # shape/view.py                 :    85    G: {}     L: {}
                              class View:
                                @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                                         # shape/view.py                 :   239    G: {}     L: {}
                                def expand(self, new_shape: Tuple[sint, ...]) -> View:
                                  if len(new_shape) != len(self.shape): raise ValueError(f"expand arg {new_shape=} must have same number of dimensions as shape {self.shape=}")                          # shape/view.py                 :   240    G: {}     L: {'self': View(shape=(1,), strides=(0,), offset=0, mask=None, contiguous=True), 'new_shape': (3,)}
                                  if 0 in self.shape:                                                                                                                                                    # shape/view.py                 :   241    G: {}     L: {}
                                  assert all((s == x or (s == 1 and st == 0)) for s,x,st in zip(self.shape, new_shape, self.strides)), f"can't expand {self.shape} into {new_shape}"                     # shape/view.py                 :   244    G: {}     L: {}
                                  mask = tuple([(((0,0) if m != (0,1) else (0,ns)) if s != ns else m) for m,s,ns in zip(self.mask, self.shape, new_shape)]) if self.mask else None                       # shape/view.py                 :   246    G: {}     L: {}
                                  return View.create(new_shape, self.strides, self.offset, mask)                                                                                                         # shape/view.py                 :   247    G: {}     L: {}

            class Add(Function):                                                                                                                                                                         # function.py                   :   118    G: {}     L: {}
              def forward(self, x:LazyBuffer, y:LazyBuffer) -> LazyBuffer: return x.e(BinaryOps.ADD, y)                                                                                                  # function.py                   :   119    G: {}     L: {}

                class LazyBuffer:                                                                                                                                                                        # lazy.py                       :    26    G: {}     L: {}
                  def e(self, op:Union[MetaOps, UnaryOps, BinaryOps, TernaryOps], *in_srcs:LazyBuffer, arg:Optional[Any]=None) -> LazyBuffer:                                                            # lazy.py                       :   137    G: {}     L: {}
                    srcs: List[LazyBuffer] = []                                                                                                                                                          # lazy.py                       :   138    G: {}     L: {'self': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'op': <BinaryOps.ADD: 1>, 'arg': None, 'in_srcs': (<LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>,)}
                    for s in (self,)+in_srcs:                                                                                                                                                            # lazy.py                       :   139    G: {}     L: {}
                      if s == s.base and s.base.contiguous_child and (root:=s.base.contiguous_child[0]()) is not None:                                                                                   # lazy.py                       :   140    G: {}     L: {}

                        srcs.append(s)                                                                                                                                                                   # lazy.py                       :   143    G: {}     L: {}

                    assert all_same(dts:=[x.dtype.scalar() for x in (srcs[1:] if op is TernaryOps.WHERE else srcs)]), f"all dtypes must match {dts} on {op}"                                             # lazy.py                       :   144    G: {}     L: {}

                      def all_same(items:Union[Tuple[T, ...], List[T]]): return all(x == items[0] for x in items)                                                                                        # helpers.py                    :    26    G: {}     L: {}

                    assert all_same([x.shape for x in srcs]), f"all shapes must be the same {[x.shape for x in srcs]}"                                                                                   # lazy.py                       :   145    G: {}     L: {}

                    if op is TernaryOps.WHERE: assert srcs[0].dtype == dtypes.bool, "TernaryOps.WHERE must have the first arg be bool"                                                                   # lazy.py                       :   146    G: {}     L: {}
                    if op is UnaryOps.NEG: assert srcs[0].dtype != dtypes.bool, "UnaryOps.NEG does not accept dtype bool"                                                                                # lazy.py                       :   147    G: {}     L: {}
                    out_dtype = dtypes.bool if op in (BinaryOps.CMPLT, BinaryOps.CMPNE) else srcs[-1].dtype                                                                                              # lazy.py                       :   149    G: {}     L: {}
                    if op in python_alu and all(s.is_unrealized_unmasked_const() for s in srcs):                                                                                                         # lazy.py                       :   152    G: {}     L: {}

                  class LazyBuffer:                                                                                                                                                                      # lazy.py                       :    26    G: {}     L: {}
                    def is_unrealized_unmasked_const(self): return self.is_unrealized_const() and all(v.mask is None for v in self.st.views)                                                             # lazy.py                       :   114    G: {}     L: {}

                    if op is UnaryOps.NEG and self.base.op is UnaryOps.NEG and self.base.realized is None: return self.base.srcs[0]                                                                      # lazy.py                       :   154    G: {}     L: {'op': <BinaryOps.ADD: 1>, 'arg': None, 'in_srcs': (<LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>,), 'srcs': [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>], 's': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>, 'dts': [dtypes.int, dtypes.int], 'out_dtype': dtypes.int}
                    if op in BinaryOps:                                                                                                                                                                  # lazy.py                       :   155    G: {}     L: {}
                      x, y = self, in_srcs[0]                                                                                                                                                            # lazy.py                       :   156    G: {}     L: {}
                      if op is BinaryOps.ADD:                                                                                                                                                            # lazy.py                       :   157    G: {}     L: {}
                        if y.is_unrealized_unmasked_const() and y.base.arg == 0: return x                                                                                                                # lazy.py                       :   158    G: {}     L: {}

                        if x.is_unrealized_unmasked_const() and x.base.arg == 0: return y                                                                                                                # lazy.py                       :   159    G: {}     L: {}

                      if op is BinaryOps.MUL:                                                                                                                                                            # lazy.py                       :   160    G: {}     L: {}
                    return create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))                                                                          # lazy.py                       :   166    G: {}     L: {}

      _METADATA.reset(token)                                                                                                                                                                             # tensor.py                     :  3264    G: {}     L: {'args': (<Tensor <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)> on CLANG with grad None>, 2), 'kwargs': {}, 'caller': '', 'token': <Token var=<ContextVar name='_METADATA' default=None at 0x7c929a144450> at 0x7c9287b5bd00>, 'ret': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}
      return ret                                                                                                                                                                                         # tensor.py                     :  3265    G: {}     L: {}

a.tolist()                                                                                                                                                                                               # ...grad.tensor.tolist_CLANG.py:     3    G: {}     L: {}

  class Tensor:                                                                                                                                                                                          # tensor.py                     :    92    G: {}     L: {}
    # TODO: should be Tensor.tolist() -> Union[List[ConstType], ConstType]. The List is Sequence because mypy expects memoryview.tolist() -> list[int]                                                   # tensor.py                     :   278    G: {}     L: {}
    # src: https://github.com/python/mypy/blob/release-1.6/mypy/typeshed/stdlib/builtins.pyi#L803
    def tolist(self) -> Union[Sequence[ConstType], ConstType]:
      return self.data().tolist()                                                                                                                                                                        # tensor.py                     :   287    G: {}     L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}

    class Tensor:                                                                                                                                                                                        # tensor.py                     :    92    G: {}     L: {}
      def data(self) -> memoryview:                                                                                                                                                                      # tensor.py                     :   250    G: {}     L: {}
        assert self.dtype.fmt is not None, f"no fmt dtype for {self.dtype}"                                                                                                                              # tensor.py                     :   259    G: {}     L: {}

        assert all_int(self.shape), f"no data if shape is symbolic, {self.shape=}"                                                                                                                       # tensor.py                     :   260    G: {}     L: {}

        return self._data().cast(self.dtype.fmt, self.shape)                                                                                                                                             # tensor.py                     :   261    G: {}     L: {}

      class Tensor:                                                                                                                                                                                      # tensor.py                     :    92    G: {}     L: {}
        def _data(self) -> memoryview:                                                                                                                                                                   # tensor.py                     :   242    G: {}     L: {}
          if 0 in self.shape: return memoryview(bytearray(0))                                                                                                                                            # tensor.py                     :   243    G: {}     L: {}

          cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()                                                                                                                        # tensor.py                     :   245    G: {}     L: {}

        class Tensor:                                                                                                                                                                                    # tensor.py                     :    92    G: {}     L: {}
          def cast(self, dtype:DTypeLike) -> Tensor:                                                                                                                                                     # tensor.py                     :  3039    G: {}     L: {}
            return self if self.dtype == (dt:=to_dtype(dtype)) else F.Cast.apply(self, dtype=dt)                                                                                                         # tensor.py                     :  3052    G: {}     L: {'dtype': dtypes.int}

        class Tensor:                                                                                                                                                                                    # tensor.py                     :    92    G: {}     L: {}
          def contiguous(self):                                                                                                                                                                          # tensor.py                     :  1995    G: {}     L: {}
            return F.Contiguous.apply(self)                                                                                                                                                              # tensor.py                     :  1999    G: {}     L: {}

          class Contiguous(Function):                                                                                                                                                                    # function.py                   :    11    G: {}     L: {}
            def forward(self, x:LazyBuffer) -> LazyBuffer: return x.contiguous()                                                                                                                         # function.py                   :    12    G: {}     L: {}

              class LazyBuffer:                                                                                                                                                                          # lazy.py                       :    26    G: {}     L: {}
                def contiguous(self, allow_buffer_view=True):                                                                                                                                            # lazy.py                       :    87    G: {}     L: {}
                  if not self.st.contiguous or self.size != self.base.size or self.is_unrealized_const():                                                                                                # lazy.py                       :    88    G: {}     L: {'self': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'allow_buffer_view': True}

                  self.base.forced_realize = True                                                                                                                                                        # lazy.py                       :    92    G: {}     L: {}

                  return self                                                                                                                                                                            # lazy.py                       :    93    G: {}     L: {}

        class Tensor:                                                                                                                                                                                    # tensor.py                     :    92    G: {}     L: {}
          def to(self, device:Optional[Union[str, Tuple[str, ...]]]) -> Tensor:                                                                                                                          # tensor.py                     :   303    G: {}     L: {}
            device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)                                                                 # tensor.py                     :   307    G: {}     L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)> on CLANG with grad None>}

            if device == self.device: return self                                                                                                                                                        # tensor.py                     :   308    G: {}     L: {}

        class Tensor:                                                                                                                                                                                    # tensor.py                     :    92    G: {}     L: {}
          def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:                                                                                                                                # tensor.py                     :   202    G: {}     L: {}
            run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)                                                                                                                # tensor.py                     :   204    G: {}     L: {'do_update_stats': True, 'lst': ()}

          class Tensor:                                                                                                                                                                                  # tensor.py                     :    92    G: {}     L: {}
            def schedule_with_vars(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:                                                            # tensor.py                     :   188    G: {}     L: {}
              if getenv("FUZZ_SCHEDULE"):                                                                                                                                                                # tensor.py                     :   190    G: {}     L: {'seen': None}

              schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)                                                                                       # tensor.py                     :   193    G: {}     L: {}

            class LazyBuffer:                                                                                                                                                                            # lazy.py                       :    26    G: {}     L: {}
              # same API as multi                                                                                                                                                                        # lazy.py                       :    67    G: {}     L: {}
              @property
              def lbs(self) -> List[LazyBuffer]: return [self]

                def flatten(l:Iterable[Iterable[T]]): return [item for sublist in l for item in sublist]                                                                                                 # helpers.py                    :    34    G: {}     L: {}

                def create_schedule_with_vars(outs:List[LazyBuffer], seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:                                             # engine/schedule.py            :   384    G: {}     L: {}
                  if seen is None: seen = set()                                                                                                                                                          # engine/schedule.py            :   385    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': None}
                  graph, in_degree = _graph_schedule(outs, seen)                                                                                                                                         # engine/schedule.py            :   386    G: {}     L: {}

                    def _graph_schedule(outs:List[LazyBuffer], seen:Set[LazyBuffer]) -> \                                                                                                                # engine/schedule.py            :   263    G: {}     L: {}
                      Tuple[DefaultDict[LBScheduleItem, List[LBScheduleItem]],  # this is the graph
                            DefaultDict[LBScheduleItem, int]]:                  # this is the in-degree of the graph
                      realizes: Dict[LazyBuffer, None] = {x.base:None for x in outs if x.base.realized is None}                                                                                          # engine/schedule.py            :   268    G: {}     L: {}

                      allbufs: Dict[LazyBuffer, None] = {}                                                                                                                                               # engine/schedule.py            :   269    G: {}     L: {}
                      simple_pads: Dict[LazyBuffer, None] = {}                                                                                                                                           # engine/schedule.py            :   270    G: {}     L: {}
                      children: DefaultDict[LazyBuffer, Dict[LazyBuffer, None]] = defaultdict(dict)                                                                                                      # engine/schedule.py            :   271    G: {}     L: {}
                      assign_targets: Dict[LazyBuffer, LazyBuffer] = {}                                                                                                                                  # engine/schedule.py            :   272    G: {}     L: {}
                      double_reduces: Dict[LazyBuffer, None] = {}                                                                                                                                        # engine/schedule.py            :   273    G: {}     L: {}
                      for out in outs: _recurse_lb(out.base, realizes, allbufs, simple_pads, children, assign_targets, double_reduces, scheduled=True)                                                   # engine/schedule.py            :   274    G: {}     L: {}

                        def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                                             # engine/schedule.py            :   188    G: {}     L: {}
                                        children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                        double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:
                          if buf in allbufs or buf.base.realized is not None: return                                                                                                                     # engine/schedule.py            :   192    G: {}     L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'scheduled': True}

                          if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                                       # engine/schedule.py            :   193    G: {}     L: {}

                          if buf is not buf.base:                                                                                                                                                        # engine/schedule.py            :   195    G: {}     L: {}

                          if buf.op in ReduceOps and buf.srcs[0].base.op is buf.op and buf.srcs[0] is not buf.srcs[0].base: double_reduces[buf] = None                                                   # engine/schedule.py            :   209    G: {}     L: {}
                          allbufs[buf] = None                                                                                                                                                            # engine/schedule.py            :   210    G: {}     L: {}
                          if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None                                                                                                               # engine/schedule.py            :   211    G: {}     L: {}
                          if buf.op is MetaOps.ASSIGN:                                                                                                                                                   # engine/schedule.py            :   212    G: {}     L: {}
                          if buf.op is MetaOps.COPY:                                                                                                                                                     # engine/schedule.py            :   216    G: {}     L: {}
                          if buf.op is MetaOps.VIEW: realizes[buf.srcs[0].base] = None                                                                                                                   # engine/schedule.py            :   219    G: {}     L: {}
                          for x in buf.srcs:                                                                                                                                                             # engine/schedule.py            :   220    G: {}     L: {}
                            if x.base.realized is None: children[x.base][buf] = None                                                                                                                     # engine/schedule.py            :   221    G: {}     L: {}

                            _recurse_lb(x, realizes, allbufs, simple_pads, children, assign_targets, double_reduces)                                                                                     # engine/schedule.py            :   222    G: {}     L: {}

                              def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                                       # engine/schedule.py            :   188    G: {}     L: {}
                                              children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                              double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:
                                if buf in allbufs or buf.base.realized is not None: return                                                                                                         # OLD # engine/schedule.py            :   192
                                if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                           # OLD # engine/schedule.py            :   193
                                if buf is not buf.base:                                                                                                                                            # OLD # engine/schedule.py            :   195
                                if buf.op in ReduceOps and buf.srcs[0].base.op is buf.op and buf.srcs[0] is not buf.srcs[0].base: double_reduces[buf] = None                                       # OLD # engine/schedule.py            :   209
                                allbufs[buf] = None                                                                                                                                                # OLD # engine/schedule.py            :   210
                                if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None                                                                                                   # OLD # engine/schedule.py            :   211
                                if buf.op is MetaOps.ASSIGN:                                                                                                                                       # OLD # engine/schedule.py            :   212
                                if buf.op is MetaOps.COPY:                                                                                                                                         # OLD # engine/schedule.py            :   216
                                  assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"                                                                 # engine/schedule.py            :   217    G: {}     L: {'buf': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>, 'scheduled': False}

                                  realizes[buf.srcs[0].base] = None                                                                                                                                      # engine/schedule.py            :   218    G: {}     L: {}

                              def _recurse_lb(buf:LazyBuffer, realizes:Dict[LazyBuffer, None], allbufs:Dict[LazyBuffer, None], simple_pads:Dict[LazyBuffer, None],                                       # engine/schedule.py            :   188    G: {}     L: {}
                                              children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                              double_reduces:Dict[LazyBuffer, None], scheduled=False) -> None:
                                if buf in allbufs or buf.base.realized is not None: return                                                                                                         # OLD # engine/schedule.py            :   192
                                if GRAPH: log_lazybuffer(buf, scheduled)                                                                                                                           # OLD # engine/schedule.py            :   193
                                if buf is not buf.base:                                                                                                                                            # OLD # engine/schedule.py            :   195
                                  if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \                                                                   # engine/schedule.py            :   197    G: {}     L: {'buf': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}
                                      prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):
                                  elif prod(buf.base.st.shape) < prod(buf.st.shape):                                                                                                                     # engine/schedule.py            :   201    G: {}     L: {}

                                    if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):                                         # engine/schedule.py            :   203    G: {}     L: {}

                                    else: realizes[buf.base] = None                                                                                                                                      # engine/schedule.py            :   205    G: {}     L: {}

                                  return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children, assign_targets, double_reduces)                                                                 # engine/schedule.py            :   208    G: {}     L: {}

                      for p in simple_pads:                                                                                                                                                              # engine/schedule.py            :   277    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>}
                      reduce_for_op: Dict[LazyBuffer, LazyBuffer] = {}                                                                                                                                   # engine/schedule.py            :   282    G: {}     L: {}
                      reduce_of_const: List[LazyBuffer] = []                                                                                                                                             # engine/schedule.py            :   283    G: {}     L: {}
                      for r in allbufs:                                                                                                                                                                  # engine/schedule.py            :   284    G: {}     L: {}
                        if r.op not in ReduceOps or r in realizes: continue                                                                                                                              # engine/schedule.py            :   285    G: {}     L: {}
                      if FUSE_CONV_BW:                                                                                                                                                                   # engine/schedule.py            :   325    G: {}     L: {}

                      for r in reduce_of_const:                                                                                                                                                          # engine/schedule.py            :   330    G: {}     L: {}
                      output_groups: DefaultDict[LazyBuffer, List[LazyBuffer]] = defaultdict(list)                                                                                                       # engine/schedule.py            :   339    G: {}     L: {}
                      for buf in realizes:                                                                                                                                                               # engine/schedule.py            :   340    G: {}     L: {}
                        if buf.realized is not None or buf.op is MetaOps.CONST or buf in seen: continue                                                                                                  # engine/schedule.py            :   341    G: {}     L: {}

                        output_groups[reduce_for_op[buf] if buf in reduce_for_op and MULTIOUTPUT else buf].append(buf)                                                                                   # engine/schedule.py            :   342    G: {}     L: {}
                        if isinstance(buf.dtype, ImageDType) and (prod(buf.shape) != prod(buf.dtype.shape) or                                                                                            # engine/schedule.py            :   345    G: {}     L: {}
                                                                  not any(buf.shape[x]%4 == 0 for x in buf.st.unit_stride_axes())):

                      prescheduled = [_lower_lazybuffer(group, realizes) for group in output_groups.values()]                                                                                            # engine/schedule.py            :   356    G: {}     L: {}

                      def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None]) -> LBScheduleItem:                                                                                   # engine/schedule.py            :   146    G: {}     L: {}
                        if (out:=outs[0]).op is MetaOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:                                             # engine/schedule.py            :   148    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>]}
                        if out.op in {MetaOps.CUSTOM, MetaOps.COPY, MetaOps.EMPTY, MetaOps.VIEW}:                                                                                                        # engine/schedule.py            :   153    G: {}     L: {}
                        reduce_info: Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]] = {}                                                                                    # engine/schedule.py            :   156    G: {}     L: {}
                        seen_ops: Dict[Tuple[LazyBuffer, ShapeTracker], Optional[Tuple[LazyBuffer, ShapeTracker]]] = {}                                                                                  # engine/schedule.py            :   157    G: {}     L: {}
                        for out in outs: _recurse_reduceops(out, out.st, realizes, outs, reduce_info, seen_ops)                                                                                          # engine/schedule.py            :   158    G: {}     L: {}

                          def _recurse_reduceops(buf:LazyBuffer, st:ShapeTracker, realizes:Dict[LazyBuffer, None], outs:List[LazyBuffer],                                                                # engine/schedule.py            :   106    G: {}     L: {}
                                                 reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],
                                                 cache:Dict[Tuple[LazyBuffer, ShapeTracker], Optional[Tuple[LazyBuffer, ShapeTracker]]]) -> \
                                                   Optional[Tuple[LazyBuffer, ShapeTracker]]:
                            if (buf, st) in cache: return cache[(buf, st)]                                                                                                                               # engine/schedule.py            :   110    G: {}     L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'cache': {}}
                            if buf.base.realized is not None or (buf.base in realizes and buf.base not in outs): return None                                                                             # engine/schedule.py            :   111    G: {}     L: {}

                            if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                                        # engine/schedule.py            :   112    G: {}     L: {}

                            input_st = ShapeTracker.from_shape(buf.srcs[0].shape) if buf.op in ReduceOps else st                                                                                         # engine/schedule.py            :   113    G: {}     L: {}
                            reduce_srcs = [r for x in buf.srcs if (r:=_recurse_reduceops(x, input_st, realizes, outs, reduce_info, cache)) is not None]                                                  # engine/schedule.py            :   114    G: {}     L: {}

                            top_reduce = reduce_srcs[-1] if len(reduce_srcs) != 0 else None                                                                                                              # engine/schedule.py            :   115    G: {}     L: {}
                            if buf.op in ReduceOps:                                                                                                                                                      # engine/schedule.py            :   116    G: {}     L: {}
                            return cache.setdefault((buf, st), top_reduce)                                                                                                                               # engine/schedule.py            :   144    G: {}     L: {}

                        shape_dims = [sorted(dedup(dims)) for dims in zip(*[input_st.shape for input_st,_ in reduce_info.values()])]                                                                     # engine/schedule.py            :   160    G: {}     L: {'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'seen_ops': {(<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))): None}}
                        for i,dims in enumerate(shape_dims):                                                                                                                                             # engine/schedule.py            :   161    G: {}     L: {}
                        var_vals = merge_dicts([out.st.var_vals.copy() for out in outs])                                                                                                                 # engine/schedule.py            :   168    G: {}     L: {}

                        @dataclass(frozen=True)                                                                                                                                                          # shape/shapetracker.py         :    36    G: {}     L: {}
                        class ShapeTracker:
                          @property                                                                                                                                                                      # shape/shapetracker.py         :    97    G: {}     L: {}
                          def var_vals(self) -> Dict[Variable, int]: return merge_dicts([dict([v.unbind()]) for v in self.vars()])

                            @dataclass(frozen=True)                                                                                                                                                      # shape/shapetracker.py         :    36    G: {}     L: {}
                            class ShapeTracker:
                              def vars(self) -> Set[Variable]: return set().union(*[v.vars() for v in self.views])                                                                                       # shape/shapetracker.py         :    94    G: {}     L: {}

                              @dataclass(frozen=True)                                                                                                                                                    # shape/view.py                 :    85    G: {}     L: {}
                              class View:
                                @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                                                 # shape/view.py                 :   120    G: {}     L: {}
                                def vars(self) -> Set[Variable]:
                                  flatten_mask = tuple(x for m in self.mask for x in m) if self.mask is not None else tuple()                                                                            # shape/view.py                 :   121    G: {}     L: {'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}
                                  return functools.reduce(operator.or_, [x.vars() for x in self.shape+self.strides+(self.offset,)+flatten_mask if isinstance(x, Node)], set())                           # shape/view.py                 :   122    G: {}     L: {}

                            def merge_dicts(ds:Iterable[Dict[T,U]]) -> Dict[T,U]:                                                                                                                        # helpers.py                    :    41    G: {}     L: {}
                              assert len(kvs:=set([(k,v) for d in ds for k,v in d.items()])) == len(set(kv[0] for kv in kvs)), f"cannot merge, {kvs} contains different values for the same key"  # noqa: E501 # helpers.py                    :    42    G: {}     L: {'ds': []}
                              return {k:v for d in ds for k,v in d.items()}                                                                                                                              # helpers.py                    :    43    G: {}     L: {}

                        assign_targets = {x.srcs[1]:x for x in outs if x.op is MetaOps.ASSIGN}                                                                                                           # engine/schedule.py            :   169    G: {}     L: {}
                        cache: Dict[Tuple[LazyBuffer, ShapeTracker], UOp] = {}                                                                                                                           # engine/schedule.py            :   170    G: {}     L: {}
                        ast: List[UOp] = []                                                                                                                                                              # engine/schedule.py            :   171    G: {}     L: {}
                        inputs: Dict[LazyBuffer, int] = {}                                                                                                                                               # engine/schedule.py            :   172    G: {}     L: {}
                        for i, out in enumerate(outs):                                                                                                                                                   # engine/schedule.py            :   173    G: {}     L: {}
                          output_st = ShapeTracker.from_shape(ShapeTracker.reduce(*deque(reduce_info.values(), 1).pop()) if reduce_info else out.shape)                                                  # engine/schedule.py            :   174    G: {}     L: {}

                          src = _recursive_uop(out, output_st, tuple(outs), var_vals, inputs, realizes, assign_targets, reduce_info, cache=cache)                                                        # engine/schedule.py            :   175    G: {}     L: {}

                            def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],                              # engine/schedule.py            :    51    G: {}     L: {}
                                                  realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                                  reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],
                                                  cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:
                              if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                                      # engine/schedule.py            :    56    G: {}     L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'outputs': (<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>,), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}

                              if (buf, st) in cache: return cache[(buf, st)]                                                                                                                             # engine/schedule.py            :    57    G: {}     L: {}
                              assert buf.op is not None, "base must be a base itself"                                                                                                                    # engine/schedule.py            :    58    G: {}     L: {}
                              dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                                                 # engine/schedule.py            :    59    G: {}     L: {}
                              if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                                                   # engine/schedule.py            :    62    G: {}     L: {}

                              if buf.op in ReduceOps:                                                                                                                                                    # engine/schedule.py            :    83    G: {}     L: {}
                              in_ops = tuple(_recursive_uop(x, st, outputs, var_vals, inputs, realizes, assign_targets, reduce_info, cache) for x in buf.srcs)                                           # engine/schedule.py            :    93    G: {}     L: {}

                              def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],                            # engine/schedule.py            :    51    G: {}     L: {}
                                                    realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                                    reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],
                                                    cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:
                                if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                              # OLD # engine/schedule.py            :    56
                                if (buf, st) in cache: return cache[(buf, st)]                                                                                                                     # OLD # engine/schedule.py            :    57
                                assert buf.op is not None, "base must be a base itself"                                                                                                            # OLD # engine/schedule.py            :    58
                                dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                                         # OLD # engine/schedule.py            :    59
                                if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                                           # OLD # engine/schedule.py            :    62
                                  unbound_st, st_var_vals = st.simplify().unbind()                                                                                                                       # engine/schedule.py            :    63    G: {}     L: {'buf': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def simplify(self) -> ShapeTracker:                                                                                                                                # shape/shapetracker.py         :   141    G: {}     L: {}
                                        if len(self.views) >= 2 and (new_view := self.views[-2] + self.views[-1]) is not None:                                                                           # shape/shapetracker.py         :   142    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}
                                        return self                                                                                                                                                      # shape/shapetracker.py         :   144    G: {}     L: {}

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def unbind(self) -> Tuple[ShapeTracker, Dict[Variable, int]]:                                                                                                      # shape/shapetracker.py         :    99    G: {}     L: {}
                                        unbound_views, var_vals = zip(*[v.unbind() for v in self.views])                                                                                                 # shape/shapetracker.py         :   100    G: {}     L: {}

                                      @dataclass(frozen=True)                                                                                                                                            # shape/view.py                 :    85    G: {}     L: {}
                                      class View:
                                        @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                                         # shape/view.py                 :   125    G: {}     L: {}
                                        def unbind(self) -> Tuple[View, Dict[Variable, int]]:
                                          var_unboundvar_val = [(v, v.unbind()) for v in self.vars()]                                                                                                    # shape/view.py                 :   126    G: {}     L: {}
                                          unbound_vars = {v:uv for v,(uv,_) in var_unboundvar_val}                                                                                                       # shape/view.py                 :   127    G: {}     L: {}
                                          new_shape = tuple(map(substitute, self.shape))                                                                                                                 # shape/view.py                 :   129    G: {}     L: {}

                                            @dataclass(frozen=True)                                                                                                                                      # shape/view.py                 :    85    G: {}     L: {}
                                            class View:
                                              @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                                   # shape/view.py                 :   125    G: {}     L: {}
                                              def unbind(self) -> Tuple[View, Dict[Variable, int]]:
                                                def substitute(x): return x if isinstance(x, int) else x.substitute(unbound_vars)                                                                        # shape/view.py                 :   128    G: {}     L: {}

                                          new_strides = tuple(map(substitute, self.strides))                                                                                                             # shape/view.py                 :   130    G: {}     L: {'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True), 'var_unboundvar_val': [], 'substitute': <function View.unbind.<locals>.substitute at 0x7c929e7dd7e0>, 'new_shape': (3,)}

                                          new_offset = substitute(self.offset)                                                                                                                           # shape/view.py                 :   131    G: {}     L: {}

                                          new_mask = tuple((substitute(x[0]), substitute(x[1])) for x in self.mask) if self.mask is not None else None                                                   # shape/view.py                 :   132    G: {}     L: {}
                                          return View.create(new_shape, new_strides, new_offset, new_mask), dict(x[1] for x in var_unboundvar_val)                                                       # shape/view.py                 :   133    G: {}     L: {}

                                        return ShapeTracker(tuple(unbound_views)), merge_dicts(var_vals)                                                                                                 # shape/shapetracker.py         :   101    G: {}     L: {}

                                  var_vals.update(st_var_vals)                                                                                                                                           # engine/schedule.py            :    64    G: {}     L: {}
                                  if buf.op is MetaOps.CONST:                                                                                                                                            # engine/schedule.py            :    66    G: {}     L: {}
                                  if buf in assign_targets and not (unbound_st.contiguous or (len(unbound_st.views) == 1 and unbound_st.views[0].mask is not None and \                                  # engine/schedule.py            :    73    G: {}     L: {}
                                      ShapeTracker.from_shape(unbound_st.shape).shrink(unbound_st.views[0].mask) == unbound_st.shrink(unbound_st.views[0].mask))):
                                  ubuf = UOp(UOps.DEFINE_GLOBAL, buf.dtype if isinstance(buf.dtype, ImageDType) else PtrDType(buf.dtype), (),                                                            # engine/schedule.py            :    78    G: {}     L: {}
                                             outputs.index(assign_targets[buf]) if buf in assign_targets else len(outputs)+inputs.setdefault(buf, len(inputs)))

                                    # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                                          # dtype.py                      :    31    G: {}     L: {}
                                    class PtrDType(DType):
                                      def __init__(self, dt:DType): super().__init__(dt.priority, dt.itemsize, dt.name, dt.fmt, dt.count)                                                                # dtype.py                      :    32    G: {}     L: {}

                                  return UOp(UOps.LOAD, dtype, (ubuf, unbound_st.to_uop()))                                                                                                              # engine/schedule.py            :    80    G: {}     L: {}

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def to_uop(self) -> UOp: return UOp(UOps.SHAPETRACKER, None, (), self)                                                                                             # shape/shapetracker.py         :    68    G: {}     L: {}

                              @dataclass(frozen=True)                                                                                                                                                    # shape/shapetracker.py         :    36    G: {}     L: {}
                              class ShapeTracker:
                                def __add__(self, st:ShapeTracker) -> ShapeTracker:                                                                                                                      # shape/shapetracker.py         :    39    G: {}     L: {}
                                  ret = self                                                                                                                                                             # shape/shapetracker.py         :    40    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}
                                  for v in st.views: ret = ShapeTracker(ret.views + (v,)).simplify() # one view at a time = better simplification                                                        # shape/shapetracker.py         :    41    G: {}     L: {}

                                @dataclass(frozen=True)                                                                                                                                                  # shape/view.py                 :    85    G: {}     L: {}
                                class View:
                                  @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                                       # shape/view.py                 :   136    G: {}     L: {}
                                  def __add__(self, vm1:View) -> Optional[View]:
                                    vm2 = self                                                                                                                                                           # shape/view.py                 :   137    G: {}     L: {'self': View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False), 'vm1': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}
                                    if vm2.contiguous: return vm1                                                                                                                                        # shape/view.py                 :   138    G: {}     L: {}
                                    if vm1.contiguous and vm1.shape == vm2.shape: return vm2                                                                                                             # shape/view.py                 :   139    G: {}     L: {}

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def simplify(self) -> ShapeTracker:                                                                                                                                # shape/shapetracker.py         :   141    G: {}     L: {}
                                        if len(self.views) >= 2 and (new_view := self.views[-2] + self.views[-1]) is not None:                                                                     # OLD # shape/shapetracker.py         :   142
                                          return ShapeTracker(self.views[:-2] + (new_view,)).simplify()                                                                                                  # shape/shapetracker.py         :   143    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False), View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True))), 'new_view': View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False)}

                                  return ret                                                                                                                                                             # shape/shapetracker.py         :    42    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'ret': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'v': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}

                              def _recursive_uop(buf:LazyBuffer, st:ShapeTracker, outputs:Tuple[LazyBuffer, ...], var_vals:Dict[Variable, int], inputs:Dict[LazyBuffer, int],                            # engine/schedule.py            :    51    G: {}     L: {}
                                                    realizes:Dict[LazyBuffer, None], assign_targets:Dict[LazyBuffer, LazyBuffer],
                                                    reduce_info:Dict[Tuple[LazyBuffer, ShapeTracker], Tuple[ShapeTracker, Tuple[int, ...]]],
                                                    cache:Dict[Tuple[LazyBuffer, ShapeTracker], UOp]) -> UOp:
                                if buf is not buf.base: st, buf = buf.st+st, buf.base                                                                                                              # OLD # engine/schedule.py            :    56
                                if (buf, st) in cache: return cache[(buf, st)]                                                                                                                     # OLD # engine/schedule.py            :    57
                                assert buf.op is not None, "base must be a base itself"                                                                                                            # OLD # engine/schedule.py            :    58
                                dtype = buf.dtype.base if isinstance(buf.dtype, ImageDType) else buf.dtype                                                                                         # OLD # engine/schedule.py            :    59
                                if buf.realized is not None or (buf in realizes and buf not in outputs):                                                                                           # OLD # engine/schedule.py            :    62
                                    if isinstance(val:=buf.arg, Variable):                                                                                                                               # engine/schedule.py            :    67    G: {}     L: {'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'unbound_st': ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), 'st_var_vals': {}}
                                    else: assert isinstance(val, get_args(ConstType)), f"cannot create ConstBuffer with value {val}"                                                                     # engine/schedule.py            :    70    G: {}     L: {}
                                    return UOp(UOps.CONST, dtype, (unbound_st.to_uop(),), val)                                                                                                           # engine/schedule.py            :    71    G: {}     L: {}

                              if buf.op in {MetaOps.CONTIGUOUS, MetaOps.ASSIGN}:                                                                                                                         # engine/schedule.py            :    94    G: {}     L: {'buf': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'in_ops': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)))}
                              if buf.op is UnaryOps.CAST: return cache.setdefault((buf, st), UOp(UOps.CAST, dtype, in_ops))                                                                              # engine/schedule.py            :    97    G: {}     L: {}
                              if buf.op is UnaryOps.BITCAST: return cache.setdefault((buf, st), UOp(UOps.BITCAST, dtype, in_ops))                                                                        # engine/schedule.py            :    98    G: {}     L: {}
                              return cache.setdefault((buf, st), UOp(UOps.ALU, dtype, in_ops, buf.op))                                                                                                   # engine/schedule.py            :    99    G: {}     L: {}

                          if out.op is MetaOps.ASSIGN and out.arg:                                                                                                                                       # engine/schedule.py            :   176    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'seen_ops': {(<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))): None}, 'shape_dims': [], 'ast': [], 'i': 0, 'output_st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'src': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),))}
                          output_st, vv = output_st.simplify().unbind()                                                                                                                                  # engine/schedule.py            :   179    G: {}     L: {}

                          if vv: var_vals.update(vv)                                                                                                                                                     # engine/schedule.py            :   180    G: {}     L: {}
                          ubuf = UOp(UOps.DEFINE_GLOBAL, out.dtype if isinstance(out.dtype, ImageDType) else PtrDType(out.dtype), (), i)                                                                 # engine/schedule.py            :   181    G: {}     L: {}

                          ast.append(UOp(UOps.STORE, None, (ubuf, output_st.to_uop(), src)))                                                                                                             # engine/schedule.py            :   182    G: {}     L: {}

                        return LBScheduleItem(UOp(UOps.SINK, None, tuple(ast)), outs, list(inputs), var_vals,                                                                                            # engine/schedule.py            :   183    G: {}     L: {}
                                              dedup([x[0].metadata for x in cache if x[0].metadata and x[0] not in inputs]))

                          def dedup(x:Iterable[T]): return list(dict.fromkeys(x))   # retains list order                                                                                                 # helpers.py                    :    19    G: {}     L: {}

                            @dataclass(frozen=True)                                                                                                                                                      # helpers.py                    :   115    G: {}     L: {}
                            class Metadata:
                              def __hash__(self): return hash(self.name)                                                                                                                                 # helpers.py                    :   119    G: {}     L: {}

                      def _lower_lazybuffer(outs:List[LazyBuffer], realizes:Dict[LazyBuffer, None]) -> LBScheduleItem:                                                                                   # engine/schedule.py            :   146    G: {}     L: {}
                        if (out:=outs[0]).op is MetaOps.COPY and getenv("USE_COPY_KERNEL") and out.device.split(":")[0] == out.srcs[0].device.split(":")[0]:                                       # OLD # engine/schedule.py            :   148
                        if out.op in {MetaOps.CUSTOM, MetaOps.COPY, MetaOps.EMPTY, MetaOps.VIEW}:                                                                                                  # OLD # engine/schedule.py            :   153
                          return LBScheduleItem(UOp(UOps.EXT, out.dtype, (), (out.op, out.arg)), outs, [x.base for x in out.srcs])                                                                       # engine/schedule.py            :   154    G: {}     L: {'outs': [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], 'out': <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>}

                      schedule_targets = {out:lsi for lsi in prescheduled for out in lsi.outputs}                                                                                                        # engine/schedule.py            :   357    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'allbufs': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'simple_pads': {}, 'children': defaultdict(<class 'dict'>, {<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}}), 'assign_targets': {}, 'double_reduces': {}, 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'reduce_for_op': {}, 'reduce_of_const': [], 'r': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'output_groups': defaultdict(<class 'list'>, {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>]}), 'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'prescheduled': [LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])]}
                      graph: DefaultDict[LBScheduleItem, List[LBScheduleItem]] = defaultdict(list)                                                                                                       # engine/schedule.py            :   359    G: {}     L: {}
                      in_degree: DefaultDict[LBScheduleItem, int] = defaultdict(int)                                                                                                                     # engine/schedule.py            :   360    G: {}     L: {}
                      for lsi in prescheduled:                                                                                                                                                           # engine/schedule.py            :   361    G: {}     L: {}
                        if lsi not in in_degree: in_degree[lsi] = 0                                                                                                                                      # engine/schedule.py            :   362    G: {}     L: {}

                          @dataclass(frozen=True)                                                                                                                                                        # engine/schedule.py            :    39    G: {}     L: {}
                          class LBScheduleItem:
                            def __hash__(self):                                                                                                                                                          # engine/schedule.py            :    45    G: {}     L: {}
                              return hash(self.outputs[0])                                                                                                                                               # engine/schedule.py            :    47    G: {}     L: {'self': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__])}

                        scheduled_parents = dedup(schedule_targets[x] for x in lsi.inputs if x in schedule_targets)                                                                                      # engine/schedule.py            :   364    G: {}     L: {'outs': [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], 'seen': set(), 'realizes': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'allbufs': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None, <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: None, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: None}, 'simple_pads': {}, 'children': defaultdict(<class 'dict'>, {<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}, <LB CLANG () int (<MetaOps.CONST: 2>, None)>: {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: None}}), 'assign_targets': {}, 'double_reduces': {}, 'out': <LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>, 'reduce_for_op': {}, 'reduce_of_const': [], 'r': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'output_groups': defaultdict(<class 'list'>, {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: [<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: [<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>]}), 'buf': <LB CLANG () int (<MetaOps.CONST: 2>, None)>, 'prescheduled': [LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])], 'schedule_targets': {<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>: LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]), <LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>: LBScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), outputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], inputs=[<LB NPY (3,) int (<MetaOps.EMPTY: 1>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>)>], var_vals={}, metadata=[])}, 'graph': defaultdict(<class 'list'>, {}), 'in_degree': defaultdict(<class 'int'>, {LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__]): 0}), 'lsi': LBScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), outputs=[<LB CLANG (3,) int (<BinaryOps.ADD: 1>, None)>], inputs=[<LB CLANG (3,) int (<MetaOps.COPY: 3>, None)>], var_vals={}, metadata=[__add__])}

                        for x in scheduled_parents:                                                                                                                                                      # engine/schedule.py            :   365    G: {}     L: {}
                          graph[x].append(lsi)                                                                                                                                                           # engine/schedule.py            :   366    G: {}     L: {}

                          in_degree[lsi] += 1                                                                                                                                                            # engine/schedule.py            :   367    G: {}     L: {}

                        parents_assigns = dedup(schedule_targets[assign_targets[x]] for x in lsi.inputs if x in assign_targets)                                                                          # engine/schedule.py            :   369    G: {}     L: {}

                        for assign in parents_assigns:                                                                                                                                                   # engine/schedule.py            :   370    G: {}     L: {}

                      if SAVE_SCHEDULE:                                                                                                                                                                  # engine/schedule.py            :   374    G: {}     L: {}

                      return graph, in_degree                                                                                                                                                            # engine/schedule.py            :   380    G: {}     L: {}

                  if getenv("RUN_PROCESS_REPLAY") and getenv("COMPARE_SCHEDULE", 1):                                                                                                                     # engine/schedule.py            :   387    G: {}     L: {}

                  queue = deque(lsi for lsi,deg in in_degree.items() if deg == 0)                                                                                                                        # engine/schedule.py            :   391    G: {}     L: {}
                  schedule: List[ScheduleItem] = []                                                                                                                                                      # engine/schedule.py            :   392    G: {}     L: {}
                  var_vals: Dict[Variable, int] = {}                                                                                                                                                     # engine/schedule.py            :   393    G: {}     L: {}
                  kernel_number = GlobalCounters.kernel_count                                                                                                                                            # engine/schedule.py            :   394    G: {}     L: {}
                  while queue:                                                                                                                                                                           # engine/schedule.py            :   395    G: {}     L: {}
                    lsi = queue.popleft()                                                                                                                                                                # engine/schedule.py            :   396    G: {}     L: {}
                    for buf in lsi.outputs: seen.add(buf)                                                                                                                                                # engine/schedule.py            :   397    G: {}     L: {}
                    if GRAPH:                                                                                                                                                                            # engine/schedule.py            :   398    G: {}     L: {}

                    var_vals = merge_dicts([var_vals, lsi.var_vals])                                                                                                                                     # engine/schedule.py            :   401    G: {}     L: {}

                    for out in lsi.outputs: del out.srcs  # can only schedule once                                                                                                                       # engine/schedule.py            :   402    G: {}     L: {}
                    schedule.append(si:=ScheduleItem(lsi.ast, tuple(x.buffer for x in lsi.outputs+lsi.inputs if x.size != 0), lsi.metadata))                                                             # engine/schedule.py            :   403    G: {}     L: {}
                    if logops and si.ast.op is UOps.SINK and not any(i.device.startswith("DISK:") for i in si.inputs): logops.write(str(si.ast)+"\n")                                                    # engine/schedule.py            :   404    G: {}     L: {}
                    for x in graph[lsi]:                                                                                                                                                                 # engine/schedule.py            :   405    G: {}     L: {}

                      in_degree[x] -= 1                                                                                                                                                                  # engine/schedule.py            :   406    G: {}     L: {}

                      if in_degree[x] == 0: queue.append(x)                                                                                                                                              # engine/schedule.py            :   407    G: {}     L: {}

                        class LazyBuffer:                                                                                                                                                                # lazy.py                       :    26    G: {}     L: {}
                          def __del__(self):                                                                                                                                                             # lazy.py                       :    50    G: {}     L: {}
                            if hasattr(self, 'buffer'): self.buffer.ref(-1)                                                                                                                              # lazy.py                       :    51    G: {}     L: {'self': <LB CLANG (3,) int ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))>}

                  if any(degree != 0 for degree in in_degree.values()) or len(in_degree) != len(schedule):                                                                                               # engine/schedule.py            :   410    G: {}     L: {}
                  if DEBUG >= 1 and len(schedule) >= 10: print(f"scheduled {len(schedule)} kernels")                                                                                                     # engine/schedule.py            :   412    G: {}     L: {}

                  return schedule, var_vals                                                                                                                                                              # engine/schedule.py            :   413    G: {}     L: {}

              return memory_planner(schedule), var_vals                                                                                                                                                  # tensor.py                     :   194    G: {}     L: {}

                def memory_planner(schedule:List[ScheduleItem]) -> List[ScheduleItem]:                                                                                                                   # engine/realize.py             :   264    G: {}     L: {}
                  assigned = _internal_memory_planner([si.bufs for si in schedule],                                                                                                                      # engine/realize.py             :   266    G: {}     L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])]}
                                                      noopt_buffers={b for si in schedule if si.ast.op is not UOps.SINK for b in si.bufs})

                    def _internal_memory_planner(buffers:List[Union[List[Buffer], Tuple[Buffer, ...]]], noopt_buffers=None, debug_prefix="") -> Dict[Buffer, Buffer]:                                    # engine/realize.py             :   227    G: {}     L: {}
                      if getenv("NO_MEMORY_PLANNER"): return {}                                                                                                                                          # engine/realize.py             :   228    G: {}     L: {'noopt_buffers': {<buf real:True device:NPY size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>}, 'debug_prefix': '', 'buffers': [(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), (<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>)]}

                      first_appearance, last_appearance = {}, {}                                                                                                                                         # engine/realize.py             :   229    G: {}     L: {}
                      for i,u in enumerate(buffers):                                                                                                                                                     # engine/realize.py             :   230    G: {}     L: {}
                        for buf in u:                                                                                                                                                                    # engine/realize.py             :   231    G: {}     L: {}
                          if buf.is_allocated() or buf.lb_refcount > 0 or (noopt_buffers is not None and buf.base in noopt_buffers): continue                                                            # engine/realize.py             :   232    G: {}     L: {}

                            class Buffer:                                                                                                                                                                # device.py                     :    52    G: {}     L: {}
                              def is_allocated(self) -> bool: return hasattr(self, '_buf')                                                                                                               # device.py                     :    76    G: {}     L: {}

                            class Buffer:                                                                                                                                                                # device.py                     :    52    G: {}     L: {}
                              @property                                                                                                                                                                  # device.py                     :    74    G: {}     L: {}
                              def lb_refcount(self): return self.base._lb_refcount

                      free_segs: Dict[Tuple, List[Tuple[int, int, Buffer]]] = defaultdict(list) # Dict[buffer key, Tuple[start, end, buffer to reuse on the seg]]                                        # engine/realize.py             :   238    G: {}     L: {}
                      buffer_requests = sorted([(first_appearance[buf], last_appearance[buf], buf) for buf in first_appearance.keys()], key=lambda x: -x[2].nbytes)                                      # engine/realize.py             :   250    G: {}     L: {}
                      assigned = {buf:find_replace_buffer(buf, st, en) for st, en, buf in buffer_requests}                                                                                               # engine/realize.py             :   251    G: {}     L: {}
                      for i,u in enumerate(buffers):                                                                                                                                                     # engine/realize.py             :   253    G: {}     L: {}
                        for buf in u:                                                                                                                                                                    # engine/realize.py             :   254    G: {}     L: {}
                          if buf.is_allocated() or buf.lb_refcount > 0 or (noopt_buffers is not None and buf.base in noopt_buffers): continue                                                            # engine/realize.py             :   255    G: {}     L: {}

                      if DEBUG >= 1 and len(ak:=dedup(x for x in assigned.keys() if x._base is None)) != len(av:=dedup(x for x in assigned.values() if x._base is None)):                                # engine/realize.py             :   259    G: {}     L: {}

                      return assigned                                                                                                                                                                    # engine/realize.py             :   262    G: {}     L: {}

                  return [ScheduleItem(si.ast, tuple(assigned.get(x, x) for x in si.bufs), si.metadata) for si in schedule]                                                                              # engine/realize.py             :   268    G: {}     L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])]}

              def run_schedule(schedule:List[ScheduleItem], var_vals:Optional[Dict[Variable, int]]=None, do_update_stats=True):                                                                          # engine/realize.py             :   220    G: {}     L: {}
                for ei in lower_schedule(schedule):                                                                                                                                                      # engine/realize.py             :   221    G: {}     L: {'schedule': [ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[]), ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])], 'var_vals': {}, 'do_update_stats': True}

                  def lower_schedule(schedule:List[ScheduleItem]) -> Generator[ExecItem, None, None]:                                                                                                    # engine/realize.py             :   205    G: {}     L: {}
                    while len(schedule):                                                                                                                                                                 # engine/realize.py             :   206    G: {}     L: {}
                      si = schedule.pop(0)                                                                                                                                                               # engine/realize.py             :   207    G: {}     L: {}
                      try: yield lower_schedule_item(si)                                                                                                                                                 # engine/realize.py             :   208    G: {}     L: {}

                        def lower_schedule_item(si:ScheduleItem) -> ExecItem:                                                                                                                            # engine/realize.py             :   189    G: {}     L: {}
                          assert len(set(x.device for x in si.bufs)) == 1 or (si.ast.op is UOps.EXT and si.ast.arg[0] is MetaOps.COPY)                                                                   # engine/realize.py             :   190    G: {}     L: {}
                          if si.ast.op is UOps.SINK:                                                                                                                                                     # engine/realize.py             :   191    G: {}     L: {}
                          out, (op, arg) = si.outputs[0], si.ast.arg                                                                                                                                     # engine/realize.py             :   194    G: {}     L: {}

                            @dataclass(frozen=True)                                                                                                                                                      # engine/schedule.py            :    25    G: {}     L: {}
                            class ScheduleItem:
                              @property                                                                                                                                                                  # engine/schedule.py            :    30    G: {}     L: {}
                              def outputs(self) -> Tuple[Buffer, ...]:
                                return self.bufs[:len(self.ast.src)] if self.ast.op is UOps.SINK else self.bufs[0:1]                                                                                     # engine/schedule.py            :    32    G: {}     L: {'self': ScheduleItem(ast=UOp(UOps.EXT, dtypes.int, arg=(<MetaOps.COPY: 3>, 12), src=()), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>), metadata=[])}

                          if op is MetaOps.COPY:                                                                                                                                                         # engine/realize.py             :   195    G: {}     L: {}
                            kernel_type = BufferCopy                                                                                                                                                     # engine/realize.py             :   196    G: {}     L: {}
                            if hasattr(Device[out.device].allocator, 'transfer') and out.device.split(":")[0] == si.inputs[0].device.split(":")[0]:                                                      # engine/realize.py             :   197    G: {}     L: {}

                        import ctypes, subprocess, pathlib, tempfile                                                                                                                                     # runtime/ops_clang.py          :     1    G: {'__name__': 'tinygrad.runtime.ops_clang', '__doc__': None, '__package__': 'tinygrad.runtime', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f370>, '__spec__': ModuleSpec(name='tinygrad.runtime.ops_clang', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c929e74f370>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_clang.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/ops_clang.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/__pycache__/ops_clang.cpython-310.pyc'}     L: {}
                        from tinygrad.device import Compiled, Compiler, MallocAllocator                                                                                                                  # runtime/ops_clang.py          :     2    G: {}     L: {}
                        from tinygrad.helpers import cpu_time_execution, DEBUG, cpu_objdump                                                                                                              # runtime/ops_clang.py          :     3    G: {}     L: {}
                        from tinygrad.renderer.cstyle import ClangRenderer                                                                                                                               # runtime/ops_clang.py          :     4    G: {}     L: {}
                        from typing import Dict, List, Optional, Tuple, Union, DefaultDict, cast, Literal, Callable                                                                                      # renderer/cstyle.py            :     1    G: {'__name__': 'tinygrad.renderer.cstyle', '__doc__': None, '__package__': 'tinygrad.renderer', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287b4bfa0>, '__spec__': ModuleSpec(name='tinygrad.renderer.cstyle', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287b4bfa0>, origin='/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/cstyle.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/renderer/__pycache__/cstyle.cpython-310.pyc'}     L: {}
                        import os, math                                                                                                                                                                  # renderer/cstyle.py            :     2    G: {}     L: {}
                        from collections import defaultdict, Counter                                                                                                                                     # renderer/cstyle.py            :     3    G: {}     L: {}
                        from tinygrad.ops import UnaryOps, BinaryOps, TernaryOps, UOps, UOp                                                                                                              # renderer/cstyle.py            :     4    G: {}     L: {}
                        from tinygrad.helpers import strip_parens, getenv, prod, dedup                                                                                                                   # renderer/cstyle.py            :     5    G: {}     L: {}
                        from tinygrad.dtype import ImageDType, dtypes, DType, PtrDType, ConstType                                                                                                        # renderer/cstyle.py            :     6    G: {}     L: {}
                        from tinygrad.renderer import Renderer, TensorCore                                                                                                                               # renderer/cstyle.py            :     7    G: {}     L: {}

                        class CStyleLanguage(Renderer):                                                                                                                                                  # renderer/cstyle.py            :     9    G: {}     L: {}
                          kernel_prefix: str = ""                                                                                                                                                        # renderer/cstyle.py            :    10    G: {}     L: {}
                          buffer_prefix: str = ""                                                                                                                                                        # renderer/cstyle.py            :    11    G: {}     L: {}
                          buffer_suffix: str = ""                                                                                                                                                        # renderer/cstyle.py            :    12    G: {}     L: {}
                          smem_align: str = ""                                                                                                                                                           # renderer/cstyle.py            :    13    G: {}     L: {}
                          smem_prefix: str = ""                                                                                                                                                          # renderer/cstyle.py            :    14    G: {}     L: {}
                          smem_prefix_for_cast: bool = True                                                                                                                                              # renderer/cstyle.py            :    15    G: {}     L: {}
                          arg_int_prefix: str = "const int"                                                                                                                                              # renderer/cstyle.py            :    16    G: {}     L: {}
                          barrier: str = ""                                                                                                                                                              # renderer/cstyle.py            :    17    G: {}     L: {}
                          code_for_workitem: Dict[Union[Literal["g"], Literal["l"], Literal["i"]], Callable] = {}                                                                                        # renderer/cstyle.py            :    18    G: {}     L: {}
                          extra_args: List[str] = []                                                                                                                                                     # renderer/cstyle.py            :    19    G: {}     L: {}
                          float4: Optional[str] = None                                                                                                                                                   # renderer/cstyle.py            :    20    G: {}     L: {}
                          uses_vload: bool = False                                                                                                                                                       # renderer/cstyle.py            :    21    G: {}     L: {}
                          uses_ptr_arithmetic: bool = False                                                                                                                                              # renderer/cstyle.py            :    22    G: {}     L: {}
                          type_map: Dict[DType, str] = {}                                                                                                                                                # renderer/cstyle.py            :    23    G: {}     L: {}
                          infinity: str = "INFINITY"                                                                                                                                                     # renderer/cstyle.py            :    24    G: {}     L: {}
                          nan: str = "NAN"                                                                                                                                                               # renderer/cstyle.py            :    25    G: {}     L: {}
                          code_for_op: Dict = {                                                                                                                                                          # renderer/cstyle.py            :    26    G: {}     L: {}
                            UnaryOps.NEG: lambda x,dtype: f"(!{x})" if dtype == dtypes.bool else f"(-{x})", UnaryOps.SQRT: lambda x,dtype: f"sqrt({x})",
                            UnaryOps.RECIP: lambda x,dtype: f"(1/{x})",
                            UnaryOps.EXP2: lambda x,dtype: f"exp2({x})", UnaryOps.LOG2: lambda x,dtype: f"log2({x})", UnaryOps.SIN: lambda x,dtype: f"sin({x})",
                            BinaryOps.ADD: lambda a,b,dtype: f"({a}+{b})", BinaryOps.MAX: lambda a,b,dtype: f"max({a},{b})",
                            BinaryOps.IDIV: lambda a,b,dtype: f"({a}/{b})", BinaryOps.MUL: lambda a,b,dtype: f"({a}*{b})", BinaryOps.MOD: lambda a,b,dtype: f"({a}%{b})",
                            BinaryOps.CMPLT: lambda a,b,dtype: f"({a}<{b})", BinaryOps.CMPNE: lambda a,b,dtype: f"({a}!={b})", BinaryOps.XOR: lambda a,b,dtype: f"({a}^{b})",
                            BinaryOps.AND: lambda a,b,dtype: f"({a}&{b})", BinaryOps.OR: lambda a,b,dtype: f"({a}|{b})",
                            TernaryOps.WHERE: lambda a,b,c,dtype: f"({a}?{b}:{c})"}

                        class ClangRenderer(CStyleLanguage):                                                                                                                                             # renderer/cstyle.py            :   194    G: {}     L: {}
                          device = "CLANG"                                                                                                                                                               # renderer/cstyle.py            :   195    G: {}     L: {}
                          float4 = "(float4)"                                                                                                                                                            # renderer/cstyle.py            :   196    G: {}     L: {}
                          has_local = False                                                                                                                                                              # renderer/cstyle.py            :   197    G: {}     L: {}
                          global_max = None                                                                                                                                                              # renderer/cstyle.py            :   198    G: {}     L: {}
                          infinity = "__builtin_inff()"                                                                                                                                                  # renderer/cstyle.py            :   199    G: {}     L: {}
                          nan = '__builtin_nanf("")'                                                                                                                                                     # renderer/cstyle.py            :   200    G: {}     L: {}
                          buffer_suffix = " restrict"                                                                                                                                                    # renderer/cstyle.py            :   203    G: {}     L: {}
                          type_map = {dtypes.bool:"_Bool", dtypes.half:"__fp16"}                                                                                                                         # renderer/cstyle.py            :   204    G: {}     L: {}
                          code_for_op = {**({k:v for k,v in CStyleLanguage().code_for_op.items() if k not in [UnaryOps.EXP2, UnaryOps.SIN, UnaryOps.LOG2]}),                                             # renderer/cstyle.py            :   205    G: {}     L: {}
                                         UnaryOps.SQRT: lambda x,dtype: f"__builtin_sqrtl({x})" if dtype == dtypes.float64 else f"__builtin_sqrtf({x})",
                                         BinaryOps.MAX: lambda a,b,dtype: f"(({a}>{b})?{a}:{b})"}

                        class OpenCLRenderer(CStyleLanguage):                                                                                                                                            # renderer/cstyle.py            :   213    G: {}     L: {}
                          device = "GPU"                                                                                                                                                                 # renderer/cstyle.py            :   214    G: {}     L: {}
                          kernel_prefix = "__kernel "                                                                                                                                                    # renderer/cstyle.py            :   217    G: {}     L: {}
                          buffer_prefix = "__global "                                                                                                                                                    # renderer/cstyle.py            :   218    G: {}     L: {}
                          smem_align = "__attribute__ ((aligned (16))) "                                                                                                                                 # renderer/cstyle.py            :   219    G: {}     L: {}
                          smem_prefix = "__local "                                                                                                                                                       # renderer/cstyle.py            :   220    G: {}     L: {}
                          barrier = "barrier(CLK_LOCAL_MEM_FENCE);"                                                                                                                                      # renderer/cstyle.py            :   221    G: {}     L: {}
                          float4 = "(float4)"                                                                                                                                                            # renderer/cstyle.py            :   222    G: {}     L: {}
                          code_for_workitem = {"g": lambda x: f"get_group_id({x})", "l": lambda x: f"get_local_id({x})", "i": lambda x: f"get_global_id({x})"}                                           # renderer/cstyle.py            :   223    G: {}     L: {}
                          uses_vload = True                                                                                                                                                              # renderer/cstyle.py            :   224    G: {}     L: {}
                          type_map = { dtypes.uint8: "uchar", dtypes.uint32: "uint", dtypes.uint16: "ushort", dtypes.uint64: "ulong", dtypes.bfloat16: "ushort" }                                        # renderer/cstyle.py            :   225    G: {}     L: {}

                        class IntelRenderer(OpenCLRenderer):                                                                                                                                             # renderer/cstyle.py            :   233    G: {}     L: {}
                          device, suffix, kernel_prefix = "GPU", "INTEL", "__attribute__((intel_reqd_sub_group_size(8)))\n" + "__kernel "                                                                # renderer/cstyle.py            :   234    G: {}     L: {}
                          tensor_cores = [TensorCore(dims=(8,8,16), threads=[(0,8)], dtype_in=di, dtype_out=do) for di, do in [(dtypes.half, dtypes.float), (dtypes.bfloat16, dtypes.float)]]  # noqa: E501 # renderer/cstyle.py            :   235    G: {}     L: {}

                        class MetalRenderer(CStyleLanguage):                                                                                                                                             # renderer/cstyle.py            :   251    G: {}     L: {}
                          device = "METAL"                                                                                                                                                               # renderer/cstyle.py            :   252    G: {}     L: {}
                          shared_max = 32768                                                                                                                                                             # renderer/cstyle.py            :   253    G: {}     L: {}
                          tensor_cores = [TensorCore(dims=(8,8,8), threads=[(0,2),(1,4),(0,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in [(dtypes.float, dtypes.float), (dtypes.half, dtypes.float), (dtypes.half, dtypes.half)]] # noqa: E501 # renderer/cstyle.py            :   254    G: {}     L: {}
                          kernel_prefix = "kernel "                                                                                                                                                      # renderer/cstyle.py            :   258    G: {}     L: {}
                          buffer_prefix = "device "                                                                                                                                                      # renderer/cstyle.py            :   259    G: {}     L: {}
                          smem_prefix = "threadgroup "                                                                                                                                                   # renderer/cstyle.py            :   260    G: {}     L: {}
                          arg_int_prefix = "constant int&"                                                                                                                                               # renderer/cstyle.py            :   261    G: {}     L: {}
                          barrier = "threadgroup_barrier(mem_flags::mem_threadgroup);"                                                                                                                   # renderer/cstyle.py            :   262    G: {}     L: {}
                          float4 = "float4"                                                                                                                                                              # renderer/cstyle.py            :   263    G: {}     L: {}
                          uses_ptr_arithmetic = True                                                                                                                                                     # renderer/cstyle.py            :   264    G: {}     L: {}
                          code_for_workitem = {"g": lambda x: f"gid.{chr(120+int(x))}", "l": lambda x: f"lid.{chr(120+int(x))}"}                                                                         # renderer/cstyle.py            :   265    G: {}     L: {}
                          extra_args = ['uint3 gid [[threadgroup_position_in_grid]]', 'uint3 lid [[thread_position_in_threadgroup]]']                                                                    # renderer/cstyle.py            :   267    G: {}     L: {}
                          type_map = {dtypes.bfloat16: "bfloat"}                                                                                                                                         # renderer/cstyle.py            :   268    G: {}     L: {}
                          code_for_op = {**CStyleLanguage().code_for_op,                                                                                                                                 # renderer/cstyle.py            :   269    G: {}     L: {}
                            BinaryOps.MAX: lambda a,b,dtype: f"(bfloat)max((float){a},(float){b})" if dtype == dtypes.bfloat16 else f"max({a},{b})",
                            UnaryOps.SQRT: lambda x,dtype: f"(bfloat)sqrt({x})" if dtype == dtypes.bfloat16 else f"sqrt({x})",
                            UnaryOps.EXP2: lambda x,dtype: f"(bfloat)exp2({x})" if dtype == dtypes.bfloat16 else f"exp2({x})",
                            UnaryOps.LOG2: lambda x,dtype: f"(bfloat)log2({x})" if dtype == dtypes.bfloat16 else f"log2({x})",
                            UnaryOps.SIN: lambda x,dtype: f"(bfloat)precise::sin({x})" if dtype == dtypes.bfloat16 else f"precise::sin({x})",}

                        code_for_op_half = {UnaryOps.RECIP: lambda x,dtype: f"hrcp({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"1/{x}",                                                       # renderer/cstyle.py            :   287    G: {}     L: {}
                                            BinaryOps.MAX: lambda a,b,dtype: f"__hmax({a},{b})" if dtype in (dtypes.half, dtypes.bfloat16) else f"max({a},{b})",
                                            UnaryOps.SQRT: lambda x,dtype: f"hsqrt({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"sqrt({x})",
                                            UnaryOps.SIN: lambda x,dtype: f"hsin({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"sin({x})",
                                            UnaryOps.LOG2: lambda x,dtype: f"hlog2({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"log2({x})",
                                            UnaryOps.EXP2: lambda x,dtype: f"hexp2({x})" if dtype in (dtypes.half, dtypes.bfloat16) else f"exp2({x})",}
                        _nms = "xyzwabcdefghijkl"                                                                                                                                                        # renderer/cstyle.py            :   294    G: {}     L: {}

                        class CUDARenderer(CStyleLanguage):                                                                                                                                              # renderer/cstyle.py            :   300    G: {}     L: {}
                          device = "CUDA"                                                                                                                                                                # renderer/cstyle.py            :   301    G: {}     L: {}
                          global_max = (2147483647, 65535, 65535)                                                                                                                                        # renderer/cstyle.py            :   302    G: {}     L: {}
                          local_max = (1024, 1024, 64)                                                                                                                                                   # renderer/cstyle.py            :   303    G: {}     L: {}
                          shared_max = 49152                                                                                                                                                             # renderer/cstyle.py            :   304    G: {}     L: {}
                          tensor_cores = [TensorCore(dims=(8,16,16), threads=[(0,2),(0,2),(1,2),(1,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in ([(dtypes.half, dtypes.float), (dtypes.bfloat16, dtypes.float)])]  # noqa: E501 # renderer/cstyle.py            :   305    G: {}     L: {}
                          kernel_prefix = "extern \"C\" __global__ "                                                                                                                                     # renderer/cstyle.py            :   309    G: {}     L: {}
                          smem_prefix = "__shared__ "                                                                                                                                                    # renderer/cstyle.py            :   310    G: {}     L: {}
                          smem_prefix_for_cast = False                                                                                                                                                   # renderer/cstyle.py            :   311    G: {}     L: {}
                          barrier = "__syncthreads();"                                                                                                                                                   # renderer/cstyle.py            :   312    G: {}     L: {}
                          float4 = "make_float4"                                                                                                                                                         # renderer/cstyle.py            :   313    G: {}     L: {}
                          code_for_workitem = {"g": lambda x: f"blockIdx.{chr(120+int(x))}", "l": lambda x: f"threadIdx.{chr(120+int(x))}",                                                              # renderer/cstyle.py            :   314    G: {}     L: {}
                                               "i": lambda x: f"(blockIdx.{chr(120+int(x))}*blockDim.{chr(120+x)}+threadIdx.{chr(120+int(x))})"}
                          code_for_op = {**CStyleLanguage().code_for_op, **code_for_op_half}                                                                                                             # renderer/cstyle.py            :   316    G: {}     L: {}
                          type_map = {dtypes.bfloat16: "nv_bfloat16"}                                                                                                                                    # renderer/cstyle.py            :   317    G: {}     L: {}

                        code_for_op_hip = { UnaryOps.SQRT: lambda x,dtype: f"__ocml_sqrt_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",                                                    # renderer/cstyle.py            :   342    G: {}     L: {}
                                            UnaryOps.SIN: lambda x,dtype: f"__ocml_sin_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",
                                            UnaryOps.LOG2: lambda x,dtype: f"__ocml_log2_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",
                                            UnaryOps.EXP2: lambda x,dtype: f"__ocml_exp2_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32)}({x})",
                                            # TODO: MAX with int uses fmax_f32?
                                            BinaryOps.MAX: lambda a,b,dtype: f"__ocml_fmax_f{ {dtypes.half:16, dtypes.double:64}.get(dtype, 32) }({a},{b})",}

                        class AMDRenderer(CStyleLanguage):                                                                                                                                               # renderer/cstyle.py            :   364    G: {}     L: {}
                          device = "AMD"                                                                                                                                                                 # renderer/cstyle.py            :   365    G: {}     L: {}
                          shared_max = 65536                                                                                                                                                             # renderer/cstyle.py            :   366    G: {}     L: {}
                          tensor_cores = [TensorCore(dims=(16,16,16), threads=[(0,8),(0,2),(1,2)], dtype_in=di, dtype_out=do) for (di, do) in [(dtypes.half, dtypes.float), (dtypes.half, dtypes.half)]] # noqa: E501 # renderer/cstyle.py            :   367    G: {}     L: {}
                          kernel_prefix = """extern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_local_id(unsigned int);                                                         # renderer/cstyle.py            :   370    G: {}     L: {}
                        extern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_group_id(unsigned int);
                        extern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_local_size(unsigned int);
                        extern "C" {\n""" + "".join([
                        f"""  __attribute__((device)) __attribute__((const)) {dt} __ocml_fmax_f{n}({dt}, {dt});
                          __attribute__((device)) __attribute__((pure)) {dt} __ocml_exp2_f{n}({dt});
                          __attribute__((device)) __attribute__((pure)) {dt} __ocml_log2_f{n}({dt});
                          __attribute__((device)) __attribute__((const)) {dt} __ocml_sqrt_f{n}({dt});
                          __attribute__((device)) {dt} __ocml_sin_f{n}({dt});\n""" for dt,n in [("float",32), ("double",64), ("_Float16",16)]]) +\
                        '}\nextern "C" __attribute__((global))'
                          code_for_workitem = {"g": lambda x: f"__ockl_get_group_id({x})", "l": lambda x: f"__ockl_get_local_id({x})",                                                                   # renderer/cstyle.py            :   380    G: {}     L: {}
                                               "i": lambda x: f"(__ockl_get_group_id({x})*__ockl_get_local_size({x})+__ockl_get_local_id({x}))"}
                          code_for_op = _make_hip_code_for_op()                                                                                                                                          # renderer/cstyle.py            :   382    G: {}     L: {}

                            def _make_hip_code_for_op():                                                                                                                                                 # renderer/cstyle.py            :   349    G: {}     L: {}
                              return { k:wrapper(k,v) for k,v in {**CStyleLanguage().code_for_op, **code_for_op_hip}.items() }                                                                           # renderer/cstyle.py            :   357    G: {}     L: {}

                              def _make_hip_code_for_op():                                                                                                                                               # renderer/cstyle.py            :   349    G: {}     L: {}
                                def wrapper(key, func):                                                                                                                                                  # renderer/cstyle.py            :   350    G: {}     L: {}
                                  return cast_bf16                                                                                                                                                       # renderer/cstyle.py            :   356    G: {}     L: {}

                          smem_prefix = "__attribute__((shared))"                                                                                                                                        # renderer/cstyle.py            :   383    G: {}     L: {'__module__': 'tinygrad.renderer.cstyle', '__qualname__': 'AMDRenderer', 'device': 'AMD', 'shared_max': 65536, 'tensor_cores': [TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.float, threads=[(0, 8), (0, 2), (1, 2)]), TensorCore(dims=(16, 16, 16), dtype_in=dtypes.half, dtype_out=dtypes.half, threads=[(0, 8), (0, 2), (1, 2)])], 'kernel_prefix': 'extern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_local_id(unsigned int);\nextern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_group_id(unsigned int);\nextern "C" __attribute__((device)) __attribute__((const)) size_t __ockl_get_local_size(unsigned int);\nextern "C" {\n  __attribute__((device)) __attribute__((const)) float __ocml_fmax_f32(float, float);\n  __attribute__((device)) __attribute__((pure)) float __ocml_exp2_f32(float);\n  __attribute__((device)) __attribute__((pure)) float __ocml_log2_f32(float);\n  __attribute__((device)) __attribute__((const)) float __ocml_sqrt_f32(float);\n  __attribute__((device)) float __ocml_sin_f32(float);\n  __attribute__((device)) __attribute__((const)) double __ocml_fmax_f64(double, double);\n  __attribute__((device)) __attribute__((pure)) double __ocml_exp2_f64(double);\n  __attribute__((device)) __attribute__((pure)) double __ocml_log2_f64(double);\n  __attribute__((device)) __attribute__((const)) double __ocml_sqrt_f64(double);\n  __attribute__((device)) double __ocml_sin_f64(double);\n  __attribute__((device)) __attribute__((const)) _Float16 __ocml_fmax_f16(_Float16, _Float16);\n  __attribute__((device)) __attribute__((pure)) _Float16 __ocml_exp2_f16(_Float16);\n  __attribute__((device)) __attribute__((pure)) _Float16 __ocml_log2_f16(_Float16);\n  __attribute__((device)) __attribute__((const)) _Float16 __ocml_sqrt_f16(_Float16);\n  __attribute__((device)) _Float16 __ocml_sin_f16(_Float16);\n}\nextern "C" __attribute__((global))', 'code_for_workitem': {'g': <function AMDRenderer.<lambda> at 0x7c9287b9c040>, 'l': <function AMDRenderer.<lambda> at 0x7c9287b9e7a0>, 'i': <function AMDRenderer.<lambda> at 0x7c9287b9e830>}, 'code_for_op': {<UnaryOps.NEG: 7>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9e950>, <UnaryOps.SQRT: 6>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9e9e0>, <UnaryOps.RECIP: 8>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ea70>, <UnaryOps.EXP2: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9eb00>, <UnaryOps.LOG2: 2>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9eb90>, <UnaryOps.SIN: 5>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ec20>, <BinaryOps.ADD: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ecb0>, <BinaryOps.MAX: 4>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ed40>, <BinaryOps.IDIV: 3>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9edd0>, <BinaryOps.MUL: 2>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ee60>, <BinaryOps.MOD: 5>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9eef0>, <BinaryOps.CMPLT: 6>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9ef80>, <BinaryOps.CMPNE: 7>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9f010>, <BinaryOps.XOR: 8>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9f0a0>, <BinaryOps.AND: 12>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9f130>, <BinaryOps.OR: 11>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9f1c0>, <TernaryOps.WHERE: 1>: <function _make_hip_code_for_op.<locals>.wrapper.<locals>.cast_bf16 at 0x7c9287b9f250>}}
                          barrier = '__builtin_amdgcn_fence(__ATOMIC_RELEASE, "workgroup");' + '__builtin_amdgcn_s_barrier();' + \                                                                       # renderer/cstyle.py            :   384    G: {}     L: {}
                                    '__builtin_amdgcn_fence(__ATOMIC_ACQUIRE, "workgroup");'
                          float4 = "make_float4"                                                                                                                                                         # renderer/cstyle.py            :   386    G: {}     L: {}
                          uses_ptr_arithmetic = False  # NOTE: this fixes TestLinearizerOverflowAlt                                                                                                      # renderer/cstyle.py            :   387    G: {}     L: {}
                          type_map = {dtypes.bfloat16: "hip_bfloat16"}                                                                                                                                   # renderer/cstyle.py            :   388    G: {}     L: {}

                        class NVRenderer(CUDARenderer): device = "NV"                                                                                                                                    # renderer/cstyle.py            :   432    G: {}     L: {}

                        class HIPRenderer(AMDRenderer): device = "HIP"                                                                                                                                   # renderer/cstyle.py            :   433    G: {}     L: {}

                          class ClangDevice(Compiled):                                                                                                                                                   # runtime/ops_clang.py          :    25    G: {}     L: {}
                            def __init__(self, device:str):                                                                                                                                              # runtime/ops_clang.py          :    26    G: {}     L: {}
                              from tinygrad.runtime.graph.clang import ClangGraph                                                                                                                        # runtime/ops_clang.py          :    27    G: {}     L: {'self': <tinygrad.runtime.ops_clang.ClangDevice object at 0x7c929e74f3a0>, 'device': 'CLANG', '__class__': <class 'tinygrad.runtime.ops_clang.ClangDevice'>}
                          from typing import List, Dict, cast                                                                                                                                            # runtime/graph/clang.py        :     1    G: {'__name__': 'tinygrad.runtime.graph.clang', '__doc__': None, '__package__': 'tinygrad.runtime.graph', '__loader__': <_frozen_importlib_external.SourceFileLoader object at 0x7c9287b98e20>, '__spec__': ModuleSpec(name='tinygrad.runtime.graph.clang', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7c9287b98e20>, origin='/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/clang.py'), '__file__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/clang.py', '__cached__': '/home/lorinbaum/code/tinygrad/tinygrad/runtime/graph/__pycache__/clang.cpython-310.pyc'}     L: {}
                          import ctypes                                                                                                                                                                  # runtime/graph/clang.py        :     2    G: {}     L: {}
                          from tinygrad.helpers import dedup, cpu_time_execution, DEBUG                                                                                                                  # runtime/graph/clang.py        :     3    G: {}     L: {}
                          from tinygrad.engine.jit import GraphRunner, GraphException                                                                                                                    # runtime/graph/clang.py        :     4    G: {}     L: {}
                          from tinygrad.device import Buffer, Device                                                                                                                                     # runtime/graph/clang.py        :     5    G: {}     L: {}
                          from tinygrad.engine.realize import ExecItem, CompiledRunner                                                                                                                   # runtime/graph/clang.py        :     6    G: {}     L: {}
                          from tinygrad.shape.symbolic import Variable                                                                                                                                   # runtime/graph/clang.py        :     7    G: {}     L: {}
                          from tinygrad.runtime.ops_clang import ClangProgram                                                                                                                            # runtime/graph/clang.py        :     8    G: {}     L: {}
                          from tinygrad.renderer.cstyle import ClangRenderer                                                                                                                             # runtime/graph/clang.py        :     9    G: {}     L: {}
                          render_dtype = ClangRenderer().render_dtype                                                                                                                                    # runtime/graph/clang.py        :    10    G: {}     L: {}

                              super().__init__(device, MallocAllocator, ClangRenderer(), ClangCompiler("compile_clang"), ClangProgram, ClangGraph)                                                       # runtime/ops_clang.py          :    28    G: {}     L: {}

                            return ExecItem(kernel_type(arg, out.device, si.inputs[0].device), list(si.bufs))                                                                                            # engine/realize.py             :   199    G: {}     L: {}

                              @dataclass(frozen=True)                                                                                                                                                    # engine/schedule.py            :    25    G: {}     L: {}
                              class ScheduleItem:
                                @property                                                                                                                                                                # engine/schedule.py            :    34    G: {}     L: {}
                                def inputs(self) -> Tuple[Buffer, ...]:
                                  return self.bufs[len(self.ast.src):] if self.ast.op is UOps.SINK else self.bufs[1:]                                                                                    # engine/schedule.py            :    36    G: {}     L: {}

                              class BufferCopy(Runner):                                                                                                                                                  # engine/realize.py             :   121    G: {}     L: {}
                                def __init__(self, total_sz, dest_device, src_device):                                                                                                                   # engine/realize.py             :   122    G: {}     L: {}
                                  if total_sz >= 1e6: name = f"{type(self).__name__[6:].lower()} {total_sz/1e6:7.2f}M, {dest_device[:7]:>7s} <- {src_device[:7]:7s}"                                     # engine/realize.py             :   123    G: {}     L: {'self': <tinygrad.engine.realize.BufferCopy object at 0x7c9287af9840>, 'total_sz': 12, 'dest_device': 'CLANG', 'src_device': 'NPY', '__class__': <class 'tinygrad.engine.realize.BufferCopy'>}
                                  else: name = f"{type(self).__name__[6:].lower()} {total_sz:8d}, {dest_device[:7]:>7s} <- {src_device[:7]:7s}"                                                          # engine/realize.py             :   124    G: {}     L: {}
                                  super().__init__(colored(name, "yellow"), dest_device, 0, total_sz)                                                                                                    # engine/realize.py             :   125    G: {}     L: {}

                                    def colored(st, color:Optional[str], background=False): return f"\u001b[{10*background+60*(color.upper() == color)+30+['black', 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'].index(color.lower())}m{st}\u001b[0m" if color is not None else st  # replace the termcolor library with one line  # noqa: E501 # helpers.py                    :    28    G: {}     L: {}

                                    class Runner:                                                                                                                                                        # engine/realize.py             :    68    G: {}     L: {}
                                      def __init__(self, display_name:str, dname:str, op_estimate:sint=0, mem_estimate:sint=0, lds_estimate:Optional[sint]=None):                                        # engine/realize.py             :    69    G: {}     L: {}
                                        self.first_run, self.display_name, self.dname, self.op_estimate, self.mem_estimate, self.lds_estimate = \                                                        # engine/realize.py             :    70    G: {}     L: {'display_name': '\x1b[33mcopy       12,   CLANG <- NPY    \x1b[0m', 'dname': 'CLANG', 'op_estimate': 0, 'mem_estimate': 12, 'lds_estimate': None}
                                          True, display_name, dname, op_estimate, mem_estimate, mem_estimate if lds_estimate is None else lds_estimate

                  if len(capturing) and CAPTURING: capturing[0].add(ei)                                                                                                                                  # engine/realize.py             :   222    G: {}     L: {'schedule': [ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])], 'var_vals': {}, 'do_update_stats': True, 'ei': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x7c9287af9840>, bufs=[<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None)}
                  ei.run(var_vals, do_update_stats=do_update_stats)                                                                                                                                      # engine/realize.py             :   223    G: {}     L: {}

                    @dataclass(frozen=True)                                                                                                                                                              # engine/realize.py             :   167    G: {}     L: {}
                    class ExecItem:
                      def run(self, var_vals:Optional[Dict[Variable, int]]=None, wait=False, jit=False, do_update_stats=True) -> Optional[float]:                                                        # engine/realize.py             :   171    G: {}     L: {}
                        bufs = [cast(Buffer, x) for x in self.bufs] if jit else [cast(Buffer, x).ensure_allocated() for x in self.bufs]                                                                  # engine/realize.py             :   172    G: {}     L: {'self': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x7c9287af9840>, bufs=[<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None), 'wait': False, 'jit': False}

                      class Buffer:                                                                                                                                                                      # device.py                     :    52    G: {}     L: {}
                        def ensure_allocated(self) -> Buffer: return self.allocate() if not hasattr(self, '_buf') else self                                                                              # device.py                     :    77    G: {}     L: {}

                        class LRUAllocator(Allocator):  # pylint: disable=abstract-method                                                                                                                # device.py                     :   143    G: {}     L: {}
                          def alloc(self, size:int, options:Optional[BufferOptions]=None):                                                                                                               # device.py                     :   149    G: {}     L: {}
                            if len(c := self.cache[(size, options)]): return c.pop()                                                                                                                     # device.py                     :   150    G: {}     L: {'self': <tinygrad.device._MallocAllocator object at 0x7c9298d3a5f0>, 'size': 12, 'options': None, '__class__': <class 'tinygrad.device.LRUAllocator'>}
                            try: return super().alloc(size, options)                                                                                                                                     # device.py                     :   151    G: {}     L: {}

                              # TODO: size, dest, src are the same type. can we enforce this?                                                                                                            # device.py                     :   132    G: {}     L: {}
                              class Allocator:
                                def alloc(self, size:int, options:Optional[BufferOptions]=None):                                                                                                         # device.py                     :   133    G: {}     L: {}
                                  assert not isinstance(size, int) or size > 0, f"alloc size must be positve, getting {size}"                                                                            # device.py                     :   134    G: {}     L: {}
                                  return self._alloc(size, options if options is not None else BufferOptions())                                                                                          # device.py                     :   135    G: {}     L: {}

                                    class _MallocAllocator(LRUAllocator):                                                                                                                                # device.py                     :   163    G: {}     L: {}
                                      def _alloc(self, size:int, options:BufferOptions): return (ctypes.c_uint8 * size)()                                                                                # device.py                     :   164    G: {}     L: {}

                        et = self.prg(bufs, var_vals if var_vals is not None else {}, wait=wait or DEBUG >= 2)                                                                                           # engine/realize.py             :   173    G: {}     L: {}

                          class BufferCopy(Runner):                                                                                                                                                      # engine/realize.py             :   121    G: {}     L: {}
                            def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False):                                                                                          # engine/realize.py             :   135    G: {}     L: {}
                              dest, src = rawbufs[0:2]                                                                                                                                                   # engine/realize.py             :   136    G: {}     L: {'self': <tinygrad.engine.realize.BufferCopy object at 0x7c9287af9840>, 'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>]}
                              assert dest.size == src.size and dest.dtype == src.dtype, f"buffer copy mismatch, {dest.size} != {src.size}, {dest.dtype} != {src.dtype}"                                  # engine/realize.py             :   137    G: {}     L: {}
                              st = time.perf_counter()                                                                                                                                                   # engine/realize.py             :   138    G: {}     L: {}
                              self.copy(dest, src)                                                                                                                                                       # engine/realize.py             :   139    G: {}     L: {}

                                class BufferCopy(Runner):                                                                                                                                                # engine/realize.py             :   121    G: {}     L: {}
                                  def copy(self, dest, src):                                                                                                                                             # engine/realize.py             :   126    G: {}     L: {}
                                    disk_supports_fast_copyout = src.device.startswith("DISK") and hasattr(src.allocator.device, 'io_uring') and hasattr(src.allocator.device, 'fd')                     # engine/realize.py             :   127    G: {}     L: {}
                                    if src.device.startswith("DISK") and hasattr(dest.allocator, 'copy_from_disk') and disk_supports_fast_copyout and src.nbytes >= 4096:                                # engine/realize.py             :   128    G: {}     L: {}
                                    elif src.device.startswith("DISK") and hasattr(dest.allocator, 'as_buffer'):                                                                                         # engine/realize.py             :   130    G: {}     L: {}
                                      dest.copyin(src.as_buffer(allow_zero_copy=True))  # may allocate a CPU buffer depending on allow_zero_copy                                                         # engine/realize.py             :   134    G: {}     L: {}

                                        class Buffer:                                                                                                                                                    # device.py                     :    52    G: {}     L: {}
                                          def as_buffer(self, allow_zero_copy=False, force_zero_copy=False) -> memoryview:                                                                               # device.py                     :   109    G: {}     L: {}
                                            if (force_zero_copy or allow_zero_copy) and hasattr(self.allocator, 'as_buffer'): return self.allocator.as_buffer(self._buf)                                 # device.py                     :   111    G: {}     L: {'self': <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>, 'allow_zero_copy': True, 'force_zero_copy': False}
                                            assert not force_zero_copy, "force zero copy was passed, but copy is required"                                                                               # device.py                     :   112    G: {}     L: {}
                                            return self.copyout(memoryview(bytearray(self.nbytes)))                                                                                                      # device.py                     :   113    G: {}     L: {}

                                              class Buffer:                                                                                                                                              # device.py                     :    52    G: {}     L: {}
                                                def copyout(self, mv:memoryview) -> memoryview:                                                                                                          # device.py                     :   120    G: {}     L: {}
                                                  mv = flat_mv(mv)                                                                                                                                       # device.py                     :   121    G: {}     L: {'mv': <memory at 0x7c9287b33700>}

                                                    def flat_mv(mv:memoryview): return mv if len(mv) == 0 else mv.cast("B", shape=(mv.nbytes,))                                                          # helpers.py                    :   309    G: {}     L: {}

                                                  assert len(mv) == self.nbytes, f"size mismatch, {len(mv)=} != {self.dtype=} {self.size=}"                                                              # device.py                     :   122    G: {}     L: {}

                                                  assert self.is_allocated(), "can't copyout unallocated buffer"                                                                                         # device.py                     :   123    G: {}     L: {}

                                                  self.allocator.copyout(mv, self._buf)                                                                                                                  # device.py                     :   124    G: {}     L: {}

                                                    class NpyAllocator(Allocator):  # pylint: disable=abstract-method                                                                                    # runtime/ops_npy.py            :     5    G: {}     L: {}
                                                      def copyout(self, dest:memoryview, src:np.ndarray): dest[:] = flat_mv(np.require(src, requirements='C').data)                                      # runtime/ops_npy.py            :     6    G: {}     L: {}

                                                  return mv                                                                                                                                              # device.py                     :   125    G: {}     L: {}

                                        class Buffer:                                                                                                                                                    # device.py                     :    52    G: {}     L: {}
                                          def copyin(self, mv:memoryview):                                                                                                                               # device.py                     :   114    G: {}     L: {}
                                            mv = flat_mv(mv)                                                                                                                                             # device.py                     :   115    G: {}     L: {'self': <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>}

                                            assert len(mv) == self.nbytes, f"size mismatch, {len(mv)=} != {self.dtype=} {self.size=}"                                                                    # device.py                     :   116    G: {}     L: {}

                                            assert self.is_allocated(), "can't copyin to unallocated buffer"                                                                                             # device.py                     :   117    G: {}     L: {}

                                            self.allocator.copyin(self._buf, mv)                                                                                                                         # device.py                     :   118    G: {}     L: {}

                                              class _MallocAllocator(LRUAllocator):                                                                                                                      # device.py                     :   163    G: {}     L: {}
                                                def copyin(self, dest, src:memoryview): ctypes.memmove(dest, from_mv(src), len(src))                                                                     # device.py                     :   166    G: {}     L: {}

                                                  # TODO: make this work with read only memoryviews (if possible)                                                                                        # helpers.py                    :   298    G: {}     L: {}
                                                  def from_mv(mv:memoryview, to_type=ctypes.c_char):
                                                    return ctypes.cast(ctypes.addressof(to_type.from_buffer(mv)), ctypes.POINTER(to_type * len(mv))).contents                                            # helpers.py                    :   299    G: {}     L: {'mv': <memory at 0x7c9287b32440>, 'to_type': <class 'ctypes.c_char'>}

                                            return self                                                                                                                                                  # device.py                     :   119    G: {}     L: {'self': <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, 'mv': <memory at 0x7c9287b32440>}

                              if wait:                                                                                                                                                                   # engine/realize.py             :   140    G: {}     L: {'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], 'var_vals': {}, 'wait': False, 'st': 4270.324219432}

                        if do_update_stats:                                                                                                                                                              # engine/realize.py             :   174    G: {}     L: {'self': ExecItem(prg=<tinygrad.engine.realize.BufferCopy object at 0x7c9287af9840>, bufs=[<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], metadata=None), 'jit': False, 'do_update_stats': True, 'bufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:NPY size:3 dtype:dtypes.int offset:0>], 'et': None}
                          GlobalCounters.kernel_count += 1                                                                                                                                               # engine/realize.py             :   175    G: {}     L: {}
                          GlobalCounters.global_ops += (op_est:=sym_infer(self.prg.op_estimate, var_vals))                                                                                               # engine/realize.py             :   176    G: {}     L: {}

                            def sym_infer(a: Union[Node, int], var_vals: Optional[Dict[Variable, int]]) -> int:                                                                                          # shape/symbolic.py             :   297    G: {}     L: {}
                              if isinstance(a, (int, float)): return a                                                                                                                                   # shape/symbolic.py             :   298    G: {}     L: {'a': 0, 'var_vals': {}}

                          GlobalCounters.global_mem += (mem_est:=sym_infer(self.prg.mem_estimate, var_vals))                                                                                             # engine/realize.py             :   177    G: {}     L: {}

                          if et is not None: GlobalCounters.time_sum_s += et                                                                                                                             # engine/realize.py             :   178    G: {}     L: {}
                          if DEBUG >= 2:                                                                                                                                                                 # engine/realize.py             :   179    G: {}     L: {}

                          self.prg.first_run = False                                                                                                                                                     # engine/realize.py             :   186    G: {}     L: {}
                        return et                                                                                                                                                                        # engine/realize.py             :   187    G: {}     L: {}

                def lower_schedule_item(si:ScheduleItem) -> ExecItem:                                                                                                                                    # engine/realize.py             :   189    G: {}     L: {}
                  assert len(set(x.device for x in si.bufs)) == 1 or (si.ast.op is UOps.EXT and si.ast.arg[0] is MetaOps.COPY)                                                                     # OLD # engine/realize.py             :   190
                  if si.ast.op is UOps.SINK:                                                                                                                                                       # OLD # engine/realize.py             :   191
                    runner = get_runner(si.outputs[0].device, si.ast)                                                                                                                                    # engine/realize.py             :   192    G: {}     L: {'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__])}

                      def get_runner(dname:str, ast:UOp) -> CompiledRunner:                                                                                                                              # engine/realize.py             :   150    G: {}     L: {}
                        ckey = (dname, ast.key, BEAM.value, NOOPT.value, False)                                                                                                                          # engine/realize.py             :   151    G: {}     L: {'dname': 'CLANG', 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}

                          @dataclass(frozen=True, eq=False)                                                                                                                                              # ops.py                        :   100    G: {}     L: {}
                          class UOp:
                            @functools.cached_property                                                                                                                                                   # ops.py                        :   115    G: {}     L: {}
                            def key(self) -> bytes:
                              return hashlib.sha256(functools.reduce(lambda x,y: x+y, [s.key for s in self.src], str((self.op, self.dtype, self.arg)).encode())).digest()                                # ops.py                        :   116    G: {}     L: {'self': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}

                            # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                                                  # dtype.py                      :    31    G: {}     L: {}
                            class PtrDType(DType):
                              def __repr__(self): return f"PtrDType({super().__repr__()})"                                                                                                               # dtype.py                      :    36    G: {}     L: {}

                                @dataclass(frozen=True, order=True)                                                                                                                                      # dtype.py                      :     9    G: {}     L: {}
                                class DType:
                                  def __repr__(self): return f"dtypes.{'_'*(c:=self.count!=1)}{INVERSE_DTYPES_DICT[self.name if not c else self.scalar().name]}{str(self.count)*c}"                      # dtype.py                      :    15    G: {}     L: {}

                        if cret:=method_cache.get(ckey): return cret                                                                                                                                     # engine/realize.py             :   152    G: {}     L: {}
                        bkey = (dname.split(":")[0], ast.key, BEAM.value, NOOPT.value, True)                                                                                                             # engine/realize.py             :   153    G: {}     L: {}
                        if bret:=method_cache.get(bkey):                                                                                                                                                 # engine/realize.py             :   154    G: {}     L: {}
                          prg: Program = get_kernel(Device[dname].renderer, ast).to_program()                                                                                                            # engine/realize.py             :   157    G: {}     L: {}

                            def get_kernel(renderer:Renderer, ast:UOp) -> Kernel:                                                                                                                        # engine/realize.py             :    17    G: {}     L: {}
                              if DEBUG >= 5:                                                                                                                                                             # engine/realize.py             :    18    G: {}     L: {'renderer': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>}

                              k = Kernel(ast, opts=renderer).required_optimizations()                                                                                                                    # engine/realize.py             :    20    G: {}     L: {}

                                class Kernel:                                                                                                                                                            # codegen/kernel.py             :    54    G: {}     L: {}
                                  def __init__(self, ast:UOp, opts:Optional[Renderer]=None):                                                                                                             # codegen/kernel.py             :    55    G: {}     L: {}
                                    if ast.op is UOps.SINK: self.ast = ast                                                                                                                               # codegen/kernel.py             :    56    G: {}     L: {'self': <tinygrad.codegen.kernel.Kernel object at 0x7c9287b99fc0>, 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>}
                                    self.opts = opts if opts is not None else Device[Device.DEFAULT].renderer                                                                                            # codegen/kernel.py             :    62    G: {}     L: {}
                                    try: uop_sts_map = verify_ast(self.ast)                                                                                                                              # codegen/kernel.py             :    63    G: {}     L: {}

                                      def verify_ast(ast:UOp) -> Dict[UOp, ShapeTracker]:                                                                                                                # codegen/kernel.py             :   787    G: {}     L: {}
                                        assert ast.op is UOps.SINK and all(x.op is UOps.STORE for x in ast.src), "must be SINK"                                                                          # codegen/kernel.py             :   788    G: {}     L: {}
                                        assert len(set(x.st_arg.size for x in ast.src)) == 1, "outputs must be exactly the same size"                                                                    # codegen/kernel.py             :   789    G: {}     L: {}

                                        @dataclass(frozen=True, eq=False)                                                                                                                                # ops.py                        :   100    G: {}     L: {}
                                        class UOp:
                                          # *** uop syntactic sugar                                                                                                                                      # ops.py                        :   121    G: {}     L: {}
                                          @property
                                          def st_arg(self) -> ShapeTracker:
                                            assert self.op in BUFFER_UOPS, f"st_arg called on {self.op}"                                                                                                 # ops.py                        :   122    G: {}     L: {'self': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}
                                            ret = self.src[0 if self.op is UOps.CONST else 1]                                                                                                            # ops.py                        :   123    G: {}     L: {}
                                            assert ret.op is UOps.SHAPETRACKER, f"st_arg trying to return {ret}"                                                                                         # ops.py                        :   124    G: {}     L: {}
                                            return ret.arg                                                                                                                                               # ops.py                        :   125    G: {}     L: {}

                                        sts: Dict[UOp, ShapeTracker] = {}                                                                                                                                # codegen/kernel.py             :   790    G: {}     L: {}
                                        for out in ast.src: _assert_valid_uop(out, out.st_arg, sts)                                                                                                      # codegen/kernel.py             :   791    G: {}     L: {}

                                          # the living definition of UOp st_arg                                                                                                                          # codegen/kernel.py             :   764    G: {}     L: {}
                                          def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:
                                            if uop in sts: return                                                                                                                                        # codegen/kernel.py             :   765    G: {}     L: {'uop': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}
                                            op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                                        # codegen/kernel.py             :   766    G: {}     L: {}
                                            if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                                     # codegen/kernel.py             :   768    G: {}     L: {}
                                            if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                                       # codegen/kernel.py             :   770    G: {}     L: {}
                                            for x in src: _assert_valid_uop(x, st, sts)                                                                                                                  # codegen/kernel.py             :   774    G: {}     L: {}

                                              # the living definition of UOp st_arg                                                                                                                      # codegen/kernel.py             :   764    G: {}     L: {}
                                              def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:
                                                if uop in sts: return                                                                                                                              # OLD # codegen/kernel.py             :   765
                                                op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                              # OLD # codegen/kernel.py             :   766
                                                if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                           # OLD # codegen/kernel.py             :   768
                                                if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                             # OLD # codegen/kernel.py             :   770
                                                for x in src: _assert_valid_uop(x, st, sts)                                                                                                        # OLD # codegen/kernel.py             :   774
                                                if op is UOps.REDUCE_AXIS: st = ShapeTracker.from_shape(sts[src[0]].reduce(arg[1][-1] if arg[0] is ReduceOps.WMMA else arg[1]))                          # codegen/kernel.py             :   776    G: {}     L: {'uop': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()), 'op': <UOps.SHAPETRACKER: 5>, 'src': (), 'arg': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}
                                                  st = arg if op is UOps.SHAPETRACKER else sts[src[-1]]                                                                                                  # codegen/kernel.py             :   780    G: {}     L: {}
                                                  for x in (src[1:] if op in BUFFER_UOPS else src):                                                                                                      # codegen/kernel.py             :   781    G: {}     L: {}
                                                sts[uop] = st                                                                                                                                            # codegen/kernel.py             :   785    G: {}     L: {}

                                            # the living definition of UOp st_arg                                                                                                                        # codegen/kernel.py             :   764    G: {}     L: {}
                                            def _assert_valid_uop(uop:UOp, st:ShapeTracker, sts:Dict[UOp, ShapeTracker]) -> None:
                                              if uop in sts: return                                                                                                                                # OLD # codegen/kernel.py             :   765
                                              op, _, src, arg = uop.op, uop.dtype, uop.src, uop.arg                                                                                                # OLD # codegen/kernel.py             :   766
                                              if op in {UOps.DEFINE_LOCAL, UOps.DEFINE_GLOBAL}: return                                                                                             # OLD # codegen/kernel.py             :   768
                                              if op is UOps.LOAD and src[0].op is UOps.DEFINE_LOCAL:                                                                                               # OLD # codegen/kernel.py             :   770
                                              for x in src: _assert_valid_uop(x, st, sts)                                                                                                          # OLD # codegen/kernel.py             :   774
                                              if op is UOps.REDUCE_AXIS: st = ShapeTracker.from_shape(sts[src[0]].reduce(arg[1][-1] if arg[0] is ReduceOps.WMMA else arg[1]))                      # OLD # codegen/kernel.py             :   776
                                                st = arg if op is UOps.SHAPETRACKER else sts[src[-1]]                                                                                              # OLD # codegen/kernel.py             :   780
                                                for x in (src[1:] if op in BUFFER_UOPS else src):                                                                                                  # OLD # codegen/kernel.py             :   781
                                                  if sts[x].shape != st.shape:                                                                                                                           # codegen/kernel.py             :   782    G: {}     L: {'uop': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'op': <UOps.LOAD: 21>, '_': dtypes.int, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())), 'arg': None, 'x': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}

                                        shape_dims = [sorted(dedup(dims)) for dims in zip(*[x.shape for x in sts.values()])]                                                                             # codegen/kernel.py             :   792    G: {}     L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'out': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}

                                        assert all(len(x) == 1 or (len(x) == 2 and x[0] == 1) for x in shape_dims), f"shapes must have either 1 or n in each dimension, {shape_dims}"                    # codegen/kernel.py             :   793    G: {}     L: {}
                                        return sts                                                                                                                                                       # codegen/kernel.py             :   794    G: {}     L: {}

                                    self.reduceops = dedup([x for x in ordered_parents(self.ast) if x.op is UOps.REDUCE_AXIS])                                                                           # codegen/kernel.py             :    71    G: {}     L: {}

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        def __init__(self, ast:UOp, opts:Optional[Renderer]=None):                                                                                                       # codegen/kernel.py             :    55    G: {}     L: {}
                                          @functools.lru_cache(None)                                                                                                                                     # codegen/kernel.py             :    70    G: {}     L: {}
                                          def ordered_parents(op:UOp) -> List[UOp]: return dedup([item for x in op.src for item in ordered_parents(x)] + [op])

                                    self.vars: List[Variable] = self.ast.variables()                                                                                                                     # codegen/kernel.py             :    73    G: {}     L: {'self': <tinygrad.codegen.kernel.Kernel object at 0x7c9287b99fc0>, 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}}

                                      @dataclass(frozen=True, eq=False)                                                                                                                                  # ops.py                        :   100    G: {}     L: {}
                                      class UOp:
                                        def variables(self) -> List[Variable]:                                                                                                                           # ops.py                        :   177    G: {}     L: {}
                                          st_vars: List[Set[Variable]] = [x.st_arg.vars() for x in self.sparents if x.op in BUFFER_UOPS]                                                                 # ops.py                        :   178    G: {}     L: {'self': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}

                                            @dataclass(frozen=True, eq=False)                                                                                                                            # ops.py                        :   100    G: {}     L: {}
                                            class UOp:
                                              @property  # parents with self                                                                                                                             # ops.py                        :   170    G: {}     L: {}
                                              def sparents(self) -> Dict[UOp, None]: return {**self.parents, self:None}

                                                @dataclass(frozen=True, eq=False)                                                                                                                        # ops.py                        :   100    G: {}     L: {}
                                                class UOp:
                                                  @functools.cached_property                                                                                                                             # ops.py                        :   168    G: {}     L: {}
                                                  def parents(self) -> Dict[UOp, None]: return merge_dicts([{x:None for x in self.src}]+[x.parents for x in self.src])

                                          return sorted(set.union(*st_vars, set([x.arg for x in self.sparents if x.op is UOps.DEFINE_VAR])), key=lambda v: v.expr)                                       # ops.py                        :   179    G: {}     L: {'st_vars': [set(), set(), set()]}

                                    self.bufs: List[UOp] = [x for x in self.ast.parents if x.op in BUFFER_UOPS]                                                                                          # codegen/kernel.py             :    74    G: {}     L: {}
                                    earlybufs: List[UOp] = [x for reduceop in self.reduceops for x in reduceop.parents if x.op in BUFFER_UOPS]                                                           # codegen/kernel.py             :    77    G: {}     L: {}
                                    self.full_buf_index: int = self.bufs.index(earlybufs[0]) if earlybufs else 0                                                                                         # codegen/kernel.py             :    78    G: {}     L: {}
                                    self.sts: List[ShapeTracker] = [x.st_arg for x in self.bufs]                                                                                                         # codegen/kernel.py             :    82    G: {}     L: {}

                                    for x in self.reduceops:                                                                                                                                             # codegen/kernel.py             :    86    G: {}     L: {}
                                    reduce = list(enumerate(zip(self.full_shape, self.output_shape)))                                                                                                    # codegen/kernel.py             :    91    G: {}     L: {}

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        @property                                                                                                                                                        # codegen/kernel.py             :   156    G: {}     L: {}
                                        def full_shape(self) -> Tuple[sint, ...]: return self.sts[self.full_buf_index].shape

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        @property                                                                                                                                                        # codegen/kernel.py             :   153    G: {}     L: {}
                                        def output_shape(self) -> Tuple[sint, ...]: return self.sts[0].shape

                                    permute = tuple([i for i,(s,n) in reduce if s == n] + [i for i,(s,n) in reduce if s != n])                                                                           # codegen/kernel.py             :    92    G: {}     L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x7c92879d8460>, 'earlybufs': [], 'reduce': [(0, (3, 3))]}
                                    self.reshape_and_permute(None, permute)                                                                                                                              # codegen/kernel.py             :    93    G: {}     L: {}

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        # apply reshape and permute to all shapetrackers                                                                                                                 # codegen/kernel.py             :   203    G: {}     L: {}
                                        def reshape_and_permute(self, new_shape_fxn, axis):
                                          new_sts = []                                                                                                                                                   # codegen/kernel.py             :   204    G: {}     L: {'new_shape_fxn': None, 'axis': (0,)}
                                          for st in self.sts:                                                                                                                                            # codegen/kernel.py             :   205    G: {}     L: {}
                                            if new_shape_fxn is not None: st = st.reshape(tuple(new_shape_fxn(st.shape)))                                                                                # codegen/kernel.py             :   206    G: {}     L: {}
                                            if axis is not None: st = st.permute(tuple(axis))                                                                                                            # codegen/kernel.py             :   207    G: {}     L: {}

                                              @dataclass(frozen=True)                                                                                                                                    # shape/shapetracker.py         :    36    G: {}     L: {}
                                              class ShapeTracker:
                                                def permute(self, axis: Tuple[int, ...]) -> ShapeTracker: return ShapeTracker(self.views[0:-1] + (self.views[-1].permute(axis), ))                       # shape/shapetracker.py         :   151    G: {}     L: {}

                                                  @dataclass(frozen=True)                                                                                                                                # shape/view.py                 :    85    G: {}     L: {}
                                                  class View:
                                                    @functools.lru_cache(maxsize=None)  # pylint: disable=method-cache-max-size-none                                                                     # shape/view.py                 :   250    G: {}     L: {}
                                                    def permute(self, axis: Tuple[int, ...]) -> View:
                                                      assert sorted(axis) == list(range(len(self.shape))), f"invalid permutation {axis} of len {len(self.shape)}"                                        # shape/view.py                 :   251    G: {}     L: {'axis': (0,), 'self': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True)}
                                                      return View.create(tuple(self.shape[a] for a in axis), tuple(self.strides[a] for a in axis), self.offset,                                          # shape/view.py                 :   252    G: {}     L: {}
                                                                         tuple(self.mask[a] for a in axis) if self.mask is not None else None)

                                            new_sts.append(st)                                                                                                                                           # codegen/kernel.py             :   208    G: {}     L: {}

                                          self.sts = new_sts                                                                                                                                             # codegen/kernel.py             :   209    G: {}     L: {}

                                    self.applied_opts: List[Opt] = []                                                                                                                                    # codegen/kernel.py             :    96    G: {}     L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x7c92879d8460>, 'earlybufs': [], 'reduce': [(0, (3, 3))], 'permute': (0,)}
                                    self.group_for_reduces: int = 0                                                                                                                                      # codegen/kernel.py             :    97    G: {}     L: {}
                                    self.upcasted: int = 0                                                                                                                                               # codegen/kernel.py             :    98    G: {}     L: {}
                                    self.local_dims: int = 0                                                                                                                                             # codegen/kernel.py             :    99    G: {}     L: {}
                                    self.tensor_core: Optional[TensorCore] = None                                                                                                                        # codegen/kernel.py             :   100    G: {}     L: {}
                                    self.tensor_core_opts: Optional[TensorCoreOptions] = None                                                                                                            # codegen/kernel.py             :   101    G: {}     L: {}
                                    self.use_tensor_cores: int = 0                                                                                                                                       # codegen/kernel.py             :   102    G: {}     L: {}
                                    self.bufs_for_tensor_core: Dict[UOp, Tuple[int, int]] = {}                                                                                                           # codegen/kernel.py             :   104    G: {}     L: {}
                                    self.dont_use_locals: bool = False                                                                                                                                   # codegen/kernel.py             :   105    G: {}     L: {}
                                    self.simplify_ones()                                                                                                                                                 # codegen/kernel.py             :   108    G: {}     L: {}

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        def simplify_ones(self) -> bool:                                                                                                                                 # codegen/kernel.py             :   230    G: {}     L: {}
                                          if self.shape_len == 0: return False                                                                                                                           # codegen/kernel.py             :   233    G: {}     L: {}

                                            class Kernel:                                                                                                                                                # codegen/kernel.py             :    54    G: {}     L: {}
                                              @property                                                                                                                                                  # codegen/kernel.py             :   162    G: {}     L: {}
                                              def shape_len(self) -> int: return len(self.sts[0].shape)

                                          all_ones = [s==1 for s in self.full_shape]                                                                                                                     # codegen/kernel.py             :   234    G: {}     L: {}

                                          self.local_dims -= sum(all_ones[self.first_reduce-self.local_dims:self.first_reduce])                                                                          # codegen/kernel.py             :   235    G: {}     L: {}

                                            class Kernel:                                                                                                                                                # codegen/kernel.py             :    54    G: {}     L: {}
                                              @property                                                                                                                                                  # codegen/kernel.py             :   143    G: {}     L: {}
                                              def first_reduce(self) -> int:
                                                return [x!=y for x,y in zip(self.sts[0].shape[:self.first_upcast]+(0,), self.full_shape[:self.first_upcast]+(1,))].index(True)                           # codegen/kernel.py             :   144    G: {}     L: {}

                                                  class Kernel:                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                                    @property                                                                                                                                            # codegen/kernel.py             :   147    G: {}     L: {}
                                                    def first_upcast(self) -> int: return self.shape_len-self.upcasted

                                          self.upcasted -= sum(all_ones[self.first_upcast:]) # TODO: no necessary since upcasted axis can't be un-upcasted                                               # codegen/kernel.py             :   236    G: {}     L: {'all_ones': [False]}

                                          self.reshape_and_permute(lambda shape: [x for i,x in enumerate(shape) if not all_ones[i]], None)                                                               # codegen/kernel.py             :   237    G: {}     L: {}

                                          return any(all_ones)                                                                                                                                           # codegen/kernel.py             :   238    G: {}     L: {}

                                    self.simplify_merge_adjacent()                                                                                                                                       # codegen/kernel.py             :   109    G: {}     L: {'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'uop_sts_map': {UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)): ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),))}, 'ordered_parents': <functools._lru_cache_wrapper object at 0x7c92879d8460>, 'earlybufs': [], 'reduce': [(0, (3, 3))], 'permute': (0,)}

                                      class Kernel:                                                                                                                                                      # codegen/kernel.py             :    54    G: {}     L: {}
                                        def simplify_merge_adjacent(self):                                                                                                                               # codegen/kernel.py             :   240    G: {}     L: {}
                                          if self.shape_len == 0: return                                                                                                                                 # codegen/kernel.py             :   241    G: {}     L: {}

                                          shapes, strides = [x.shape for x in self.sts], [x.real_strides() for x in self.sts]                                                                            # codegen/kernel.py             :   242    G: {}     L: {}

                                        @dataclass(frozen=True)                                                                                                                                          # shape/shapetracker.py         :    36    G: {}     L: {}
                                        class ShapeTracker:
                                          # NOTE: if a stride is not always valid, it will be None                                                                                                       # shape/shapetracker.py         :   104    G: {}     L: {}
                                          def real_strides(self, ignore_valid=False) -> Tuple[Optional[sint], ...]:
                                            if len(self.views) == 1 and self.views[-1].mask is None: return self.views[-1].strides                                                                       # shape/shapetracker.py         :   105    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'ignore_valid': False}

                                          if isinstance(self.membufs[0].dtype, ImageDType):                                                                                                              # codegen/kernel.py             :   245    G: {}     L: {}

                                            class Kernel:                                                                                                                                                # codegen/kernel.py             :    54    G: {}     L: {}
                                              @property                                                                                                                                                  # codegen/kernel.py             :   131    G: {}     L: {}
                                              def membufs(self) -> List[UOp]: return list({x.src[0].key:x.src[0] for x in self.bufs if x.op in {UOps.LOAD, UOps.STORE}}.values())

                                          rets = [[(s[0], st[0])] for s,st in zip(shapes, strides)]                                                                                                      # codegen/kernel.py             :   261    G: {}     L: {'shapes': [(3,), (3,), (3,)], 'strides': [(1,), (1,), (0,)]}
                                          for i in range(1, len(shapes[0])):                                                                                                                             # codegen/kernel.py             :   262    G: {}     L: {}
                                          for i,x in enumerate(rets[:len(self.sts)]): self.sts[i] = self.sts[i].reshape(tuple([y[0] for y in x]))                                                        # codegen/kernel.py             :   275    G: {}     L: {}

                                class Kernel:                                                                                                                                                            # codegen/kernel.py             :    54    G: {}     L: {}
                                  def required_optimizations(self) -> Kernel:                                                                                                                            # codegen/kernel.py             :   482    G: {}     L: {}
                                    if isinstance(self.membufs[0].dtype, ImageDType):                                                                                                                    # codegen/kernel.py             :   483    G: {}     L: {}

                                    return self                                                                                                                                                          # codegen/kernel.py             :   488    G: {}     L: {}

                              if not NOOPT:                                                                                                                                                              # engine/realize.py             :    21    G: {}     L: {}

                                if not (used_tensor_cores:=k.apply_tensor_cores(getenv("TC", 1))): k.hand_coded_optimizations()                                                                          # engine/realize.py             :    22    G: {}     L: {}

                                  class Kernel:                                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                    def apply_tensor_cores(self, use_tensor_cores=1, extra_opts:Optional[List[Opt]]=None, axis:int=0, tc_opt:Optional[int]=None) -> bool:                                # codegen/kernel.py             :   347    G: {}     L: {}
                                      if tc_opt is None: tc_opt = TC_OPT.value                                                                                                                           # codegen/kernel.py             :   362    G: {}     L: {'use_tensor_cores': 1, 'extra_opts': None, 'axis': 0, 'tc_opt': None}
                                      if not self.opts.tensor_cores and use_tensor_cores != 2: return False                                                                                              # codegen/kernel.py             :   363    G: {}     L: {}

                                  class Kernel:                                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                    def hand_coded_optimizations(self) -> Kernel:                                                                                                                        # codegen/kernel.py             :   490    G: {}     L: {}
                                      self.required_optimizations()                                                                                                                                      # codegen/kernel.py             :   491    G: {}     L: {}

                                      MV_BLOCKSIZE, MV_THREADS_PER_ROW, MV_ROWS_PER_THREAD = getenv("MV_BLOCKSIZE", 4), getenv("MV_THREADS_PER_ROW", 8), getenv("MV_ROWS_PER_THREAD", 4)                 # codegen/kernel.py             :   494    G: {}     L: {}

                                      if self.opts.has_local and getenv("MV",1) != 0 and (MV_BLOCKSIZE > 1 or MV_THREADS_PER_ROW > 1 or MV_ROWS_PER_THREAD > 1) and  \                                   # codegen/kernel.py             :   495    G: {}     L: {}
                                          self.reduceop is not None and self.reduceop.arg[0] is ReduceOps.SUM and len(self.full_shape) >= 2 and self.opts.has_shared and \
                                          (mulop:=self.reduceop.src[0]).arg is BinaryOps.MUL and mulop.src[0].op is UOps.LOAD and mulop.src[1].op is UOps.LOAD:
                                      if self.opts.has_local and self.opts.has_shared and all_int(self.sts[0].shape[:self.first_reduce]):                                                                # codegen/kernel.py             :   511    G: {}     L: {}
                                      for buf_index,buf in enumerate(self.bufs):                                                                                                                         # codegen/kernel.py             :   530    G: {}     L: {}
                                        unit_stride_axes_mul_4 = [i for i in self.sts[buf_index].unit_stride_axes(ignore_valid=True) if self.sts[buf_index].shape[i]%4 == 0]                             # codegen/kernel.py             :   531    G: {}     L: {}

                                          @dataclass(frozen=True)                                                                                                                                        # shape/shapetracker.py         :    36    G: {}     L: {}
                                          class ShapeTracker:
                                            def unit_stride_axes(self, ignore_valid=False) -> List[int]: return [i for i,st in enumerate(self.real_strides(ignore_valid)) if st == 1]                    # shape/shapetracker.py         :   120    G: {}     L: {}

                                        if buf.src[0].dtype.__class__ is ImageDType:                                                                                                                     # codegen/kernel.py             :   532    G: {}     L: {}

                                      if self.group_for_reduces: return self                                                                                                                             # codegen/kernel.py             :   541    G: {}     L: {}
                                      to_upcast: List[int] = []                                                                                                                                          # codegen/kernel.py             :   550    G: {}     L: {}
                                      for axis in range(self.first_reduce):                                                                                                                              # codegen/kernel.py             :   552    G: {}     L: {}

                                        if isinstance(self.full_shape[axis], int) and self.full_shape[axis] <= 7 and any(st.axis_is_masked(axis) for st in self.sts) and \                               # codegen/kernel.py             :   555    G: {}     L: {}
                                          prod(self.full_shape[self.first_upcast:]) * prod(self.full_shape[j] for j in to_upcast) * self.full_shape[axis] <= 7 * 7:

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def axis_is_masked(self, axis:int) -> bool:                                                                                                                        # shape/shapetracker.py         :   137    G: {}     L: {}
                                        _, valid = self.to_indexed_uops()                                                                                                                                # shape/shapetracker.py         :   138    G: {}     L: {'axis': 0}

                                          @dataclass(frozen=True)                                                                                                                                        # shape/shapetracker.py         :    36    G: {}     L: {}
                                          class ShapeTracker:
                                            def to_indexed_uops(self, _idxs:Optional[List[UOp]]=None) -> Tuple[UOp, UOp]:                                                                                # shape/shapetracker.py         :    70    G: {}     L: {}
                                              idxs = [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(s)), i) for i,s in enumerate(self.shape)] \                             # shape/shapetracker.py         :    71    G: {}     L: {'_idxs': None}
                                                if _idxs is None else _idxs

                                            # TODO: this needs to be replaced, there shouldn't be variables in the shapetracker, only ints and UOps                                                      # shape/shapetracker.py         :    14    G: {}     L: {}
                                            def variable_to_uop(x, ctx=None) -> UOp: return UOp.const(dtypes.pyint, x) if isinstance(x, int) else x.render(render_ops, ctx)

                                              idx, valid = _uop_view(self.views[-1], idxs, UOp.const(dtypes.bool, True))                                                                                 # shape/shapetracker.py         :    73    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), '_idxs': None, 'idxs': [UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n  UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),))]}

                                                def _uop_view(view:View, idxs:List[UOp], vexpr:UOp) -> Tuple[UOp, UOp]:                                                                                  # shape/shapetracker.py         :    25    G: {}     L: {}
                                                  iexpr = variable_to_uop(view.offset)                                                                                                                   # shape/shapetracker.py         :    27    G: {}     L: {'view': View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True), 'vexpr': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}

                                                  for idx,sh,st,m in zip(idxs, view.shape, view.strides, view.mask if view.mask is not None else [None]*len(view.shape)):                                # shape/shapetracker.py         :    28    G: {}     L: {}
                                                    if sh != 1 and st != 0: iexpr = iexpr + idx*variable_to_uop(st)                                                                                      # shape/shapetracker.py         :    29    G: {}     L: {}

                                                    if m is not None:                                                                                                                                    # shape/shapetracker.py         :    30    G: {}     L: {}
                                                  return iexpr, vexpr                                                                                                                                    # shape/shapetracker.py         :    33    G: {}     L: {}

                                              for view in reversed(self.views[0:-1]):                                                                                                                    # shape/shapetracker.py         :    74    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), '_idxs': None, 'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'valid': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}
                                              return idx, valid                                                                                                                                          # shape/shapetracker.py         :    82    G: {}     L: {}

                                        return axis in [x.arg for x in graph_rewrite(valid).sparents if x.op is UOps.RANGE]                                                                              # shape/shapetracker.py         :   139    G: {}     L: {'axis': 0, '_': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=0, src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),))}

                                          def graph_rewrite(sink:UOp, pm:PatternMatcher=constant_folder) -> UOp:                                                                                         # codegen/uopgraph.py           :   513    G: {}     L: {}
                                            nodes: Dict[Tuple, UOp] = {}                                                                                                                                 # codegen/uopgraph.py           :   514    G: {}     L: {'sink': UOp(UOps.CONST, dtypes.bool, arg=True, src=()), 'pm': <tinygrad.ops.PatternMatcher object at 0x7c9299726020>}
                                            replace: Dict[UOp, UOp] = {}                                                                                                                                 # codegen/uopgraph.py           :   515    G: {}     L: {}
                                            return __inner_rewrite(sink)                                                                                                                                 # codegen/uopgraph.py           :   522    G: {}     L: {}

                                              def graph_rewrite(sink:UOp, pm:PatternMatcher=constant_folder) -> UOp:                                                                                     # codegen/uopgraph.py           :   513    G: {}     L: {}
                                                def __inner_rewrite(n:UOp) -> UOp:                                                                                                                       # codegen/uopgraph.py           :   516    G: {}     L: {}
                                                  if rn := replace.get(n): return rn                                                                                                                     # codegen/uopgraph.py           :   517    G: {}     L: {'n': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}
                                                  replace_source = (n.op, n.dtype, tuple(__inner_rewrite(y) for y in n.src), n.arg)                                                                      # codegen/uopgraph.py           :   518    G: {}     L: {}
                                                  if found := nodes.get(replace_source): replace[n] = found                                                                                              # codegen/uopgraph.py           :   519    G: {}     L: {}
                                                  else: nodes[replace_source] = replace[n] = found = __inner_rewrite(new_x) if (new_x := pm.rewrite(x:=UOp(*replace_source))) else x                     # codegen/uopgraph.py           :   520    G: {}     L: {}

                                                    class PatternMatcher:                                                                                                                                # ops.py                        :   280    G: {}     L: {}
                                                      def rewrite(self, uop:UOp) -> Optional[UOp]:                                                                                                       # ops.py                        :   293    G: {}     L: {}
                                                        for p,fxn in itertools.chain(self.pdict[(uop.op, uop.arg)], self.pdict[(uop.op, None)]):                                                         # ops.py                        :   294    G: {}     L: {'self': <tinygrad.ops.PatternMatcher object at 0x7c9299726020>, 'uop': UOp(UOps.CONST, dtypes.bool, arg=True, src=())}
                                                        return None                                                                                                                                      # ops.py                        :   296    G: {}     L: {}

                                                  return found                                                                                                                                           # codegen/uopgraph.py           :   521    G: {}     L: {}

                                      for axis in to_upcast[::-1]: self.apply_opt(Opt(OptOps.UPCAST, axis, 0))                                                                                           # codegen/kernel.py             :   559    G: {}     L: {}
                                      upcasted_axis = set()                                                                                                                                              # codegen/kernel.py             :   562    G: {}     L: {}
                                      while prod(self.sts[0].shape[:self.first_reduce]) >= 1024:                                                                                                         # codegen/kernel.py             :   563    G: {}     L: {}

                                      if self.first_reduce < self.first_upcast and (prod(self.full_shape[self.first_upcast:]) <= 4 or not any(r for _,_,r in self.upcasted_axis(self.full_buf_index))) and (self.upcasted == 0 or prod(self.full_shape[-self.upcasted:]) < 64):  # noqa: E501 # codegen/kernel.py             :   577    G: {}     L: {}

                                      for splits in [4]:                                                                                                                                                 # codegen/kernel.py             :   591    G: {}     L: {}
                                        if self.upcasted == 0 and self.full_unupcasted_shape and self.full_unupcasted_shape[-1] % splits == 0:                                                           # codegen/kernel.py             :   592    G: {}     L: {}

                                          class Kernel:                                                                                                                                                  # codegen/kernel.py             :    54    G: {}     L: {}
                                            @property                                                                                                                                                    # codegen/kernel.py             :   159    G: {}     L: {}
                                            def full_unupcasted_shape(self) -> Tuple[sint, ...]: return self.full_shape[:self.first_upcast]

                                      if self.opts.has_local:                                                                                                                                            # codegen/kernel.py             :   597    G: {}     L: {'MV_BLOCKSIZE': 4, 'MV_THREADS_PER_ROW': 8, 'MV_ROWS_PER_THREAD': 4, 'buf': UOp(UOps.CONST, dtypes.int, arg=2, src=(\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)), 'buf_index': 2, 'unit_stride_axes_mul_4': [], 'to_upcast': [], 'axis': 0, 'upcasted_axis': set(), 'splits': 4}
                                      return self                                                                                                                                                        # codegen/kernel.py             :   615    G: {}     L: {}

                                if BEAM >= 1:                                                                                                                                                            # engine/realize.py             :    23    G: {}     L: {}

                              if logkerns is not None: logkerns.writelines([f"{(k.ast, k.applied_opts)}\n"])                                                                                             # engine/realize.py             :    62    G: {}     L: {}
                              if DEBUG >= 5: print((k.ast, k.applied_opts)) # print here to show final applied_opts for all kernels instead of just in beam_search                                       # engine/realize.py             :    63    G: {}     L: {}

                              return k                                                                                                                                                                   # engine/realize.py             :    64    G: {}     L: {}

                            class Kernel:                                                                                                                                                                # codegen/kernel.py             :    54    G: {}     L: {}
                              def to_program(self, name_override:Optional[str]=None) -> Program:                                                                                                         # codegen/kernel.py             :   747    G: {}     L: {}
                                self.linearize()                                                                                                                                                         # codegen/kernel.py             :   748    G: {}     L: {'name_override': None}

                                  class Kernel:                                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                    def linearize(self) -> Kernel:                                                                                                                                       # codegen/kernel.py             :   730    G: {}     L: {}
                                      modified_ast = self.get_optimized_ast()                                                                                                                            # codegen/kernel.py             :   731    G: {}     L: {}

                                        class Kernel:                                                                                                                                                    # codegen/kernel.py             :    54    G: {}     L: {}
                                          def get_optimized_ast(self) -> UOp:                                                                                                                            # codegen/kernel.py             :   632    G: {}     L: {}
                                            return fixup_ast(self.ast)                                                                                                                                   # codegen/kernel.py             :   726    G: {}     L: {}

                                              class Kernel:                                                                                                                                              # codegen/kernel.py             :    54    G: {}     L: {}
                                                def get_optimized_ast(self) -> UOp:                                                                                                                      # codegen/kernel.py             :   632    G: {}     L: {}
                                                  # set the shapetrackers to the optimized ones, fixup reduceop                                                                                          # codegen/kernel.py             :   636    G: {}     L: {}
                                                  # transformed to the final UOp
                                                  @functools.lru_cache(None)
                                                  def fixup_ast(op:UOp, apply_to_st=None) -> UOp:
                                                    arg = op.arg                                                                                                                                         # codegen/kernel.py             :   637    G: {}     L: {'op': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'apply_to_st': None}
                                                    if op.op in BUFFER_UOPS:                                                                                                                             # codegen/kernel.py             :   638    G: {}     L: {}
                                                    if op.op is UOps.REDUCE_AXIS:                                                                                                                        # codegen/kernel.py             :   645    G: {}     L: {}
                                                    elif op.op is UOps.SINK:                                                                                                                             # codegen/kernel.py             :   723    G: {}     L: {}
                                                      arg = KernelInfo(self.local_dims, self.upcasted, self.dont_use_locals)                                                                             # codegen/kernel.py             :   724    G: {}     L: {}
                                                    return replace(op, src=tuple(fixup_ast(x, apply_to_st) for x in op.src), arg=arg)                                                                    # codegen/kernel.py             :   725    G: {}     L: {}

                                                class Kernel:                                                                                                                                            # codegen/kernel.py             :    54    G: {}     L: {}
                                                  def get_optimized_ast(self) -> UOp:                                                                                                                    # codegen/kernel.py             :   632    G: {}     L: {}
                                                    # set the shapetrackers to the optimized ones, fixup reduceop                                                                                        # codegen/kernel.py             :   636    G: {}     L: {}
                                                    # transformed to the final UOp
                                                    @functools.lru_cache(None)
                                                    def fixup_ast(op:UOp, apply_to_st=None) -> UOp:
                                                      arg = op.arg                                                                                                                                 # OLD # codegen/kernel.py             :   637
                                                      if op.op in BUFFER_UOPS:                                                                                                                     # OLD # codegen/kernel.py             :   638
                                                        st = op.st_arg if op.src[0].op is UOps.DEFINE_LOCAL else self.sts[self.bufs.index(op)]                                                           # codegen/kernel.py             :   640    G: {}     L: {'op': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)), 'arg': None}
                                                        st_uop = (st if apply_to_st is None else apply_to_st(st)).to_uop()                                                                               # codegen/kernel.py             :   641    G: {}     L: {}

                                                        if op.op is UOps.CONST: return replace(op, src=(st_uop,))                                                                                        # codegen/kernel.py             :   642    G: {}     L: {}
                                                        if op.op is UOps.STORE: return replace(op, src=(op.src[0], st_uop, fixup_ast(op.src[2], apply_to_st)))                                           # codegen/kernel.py             :   643    G: {}     L: {}

                                                  class Kernel:                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                                    def get_optimized_ast(self) -> UOp:                                                                                                                  # codegen/kernel.py             :   632    G: {}     L: {}
                                                      # set the shapetrackers to the optimized ones, fixup reduceop                                                                                      # codegen/kernel.py             :   636    G: {}     L: {}
                                                      # transformed to the final UOp
                                                      @functools.lru_cache(None)
                                                      def fixup_ast(op:UOp, apply_to_st=None) -> UOp:
                                                        arg = op.arg                                                                                                                               # OLD # codegen/kernel.py             :   637
                                                        if op.op in BUFFER_UOPS:                                                                                                                   # OLD # codegen/kernel.py             :   638
                                                          return replace(op, src=(op.src[0], st_uop, *[fixup_ast(x, apply_to_st) for x in op.src[2:]]))                                                  # codegen/kernel.py             :   644    G: {}     L: {'op': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'st': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), 'st_uop': UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=())}

                                      if DEBUG >= 3:                                                                                                                                                     # codegen/kernel.py             :   733    G: {}     L: {'modified_ast': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}

                                      verify_ast(modified_ast)                                                                                                                                           # codegen/kernel.py             :   738    G: {}     L: {}

                                      self.uops:List[UOp] = linearize_uop(ast_to_uop(modified_ast, self.opts), self.opts)                                                                                # codegen/kernel.py             :   740    G: {}     L: {}

                                        def ast_to_uop(ast:UOp, opts:Renderer) -> UOp: return IndependentLowerer().lower(ast, opts)                                                                      # codegen/lowerer.py            :   128    G: {}     L: {}

                                          class IndependentLowerer:                                                                                                                                      # codegen/lowerer.py            :    38    G: {}     L: {}
                                            def lower(self, ast:UOp, opts:Renderer) -> UOp:                                                                                                              # codegen/lowerer.py            :    39    G: {}     L: {}
                                              self.output_count = len(ast.src)                                                                                                                           # codegen/lowerer.py            :    40    G: {}     L: {'self': <tinygrad.codegen.lowerer.IndependentLowerer object at 0x7c9287b9a9b0>}
                                              ki = ast.arg if isinstance(ast.arg, KernelInfo) else KernelInfo()                                                                                          # codegen/lowerer.py            :    42    G: {}     L: {}
                                              full_shape = ast.full_shape                                                                                                                                # codegen/lowerer.py            :    44    G: {}     L: {}

                                                @dataclass(frozen=True, eq=False)                                                                                                                        # ops.py                        :   100    G: {}     L: {}
                                                class UOp:
                                                  @functools.cached_property                                                                                                                             # ops.py                        :   172    G: {}     L: {}
                                                  def full_shape(self) -> Tuple[sint, ...]:
                                                    if self.op is UOps.SHAPETRACKER: return self.arg.shape                                                                                               # ops.py                        :   173    G: {}     L: {'self': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}
                                                    return tuple(max(x) for x in zip(*[x.full_shape for x in self.src if x.op not in {UOps.DEFINE_GLOBAL, UOps.DEFINE_LOCAL}]))                          # ops.py                        :   175    G: {}     L: {}

                                              first_upcasted = len(full_shape)-ki.upcasted                                                                                                               # codegen/lowerer.py            :    45    G: {}     L: {}
                                              first_output_st: ShapeTracker = ast.src[0].st_arg                                                                                                          # codegen/lowerer.py            :    46    G: {}     L: {}

                                              first_reduce = [x!=y for x,y in zip(first_output_st.shape[:first_upcasted]+(0,), full_shape[:first_upcasted]+(1,))].index(True)                            # codegen/lowerer.py            :    48    G: {}     L: {}

                                              local_loads = [x for x in ast.parents if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL]                                                           # codegen/lowerer.py            :    49    G: {}     L: {}

                                              group_for_reduces = sum([x!=y for x,y in zip(                                                                                                              # codegen/lowerer.py            :    51    G: {}     L: {}
                                                local_loads[0].st_arg.shape[first_reduce:first_upcasted], first_output_st.shape[first_reduce:first_upcasted])]) if local_loads else 0
                                              global_dims = first_reduce-ki.local_dims                                                                                                                   # codegen/lowerer.py            :    53    G: {}     L: {}
                                              if opts.has_local:                                                                                                                                         # codegen/lowerer.py            :    55    G: {}     L: {}
                                                self.idxs = [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(g)), (i, False))                                                 # codegen/lowerer.py            :    65    G: {}     L: {}
                                                             for i,g in enumerate(full_shape[:first_reduce])]

                                              self.idxs += [UOp(UOps.RANGE, dtypes.pyint, (UOp.const(dtypes.pyint, 0), variable_to_uop(g)), (i, True))                                                   # codegen/lowerer.py            :    69    G: {}     L: {}
                                                for i,g in enumerate(full_shape[first_reduce+group_for_reduces:first_upcasted], start=first_reduce+group_for_reduces)]
                                              for i,g in enumerate(full_shape[first_upcasted:], start=first_upcasted):                                                                                   # codegen/lowerer.py            :    73    G: {}     L: {}
                                              self.ridxs = self.idxs[:]                                                                                                                                  # codegen/lowerer.py            :    78    G: {}     L: {}
                                              for a in range(first_reduce, first_reduce+group_for_reduces):                                                                                              # codegen/lowerer.py            :    79    G: {}     L: {}
                                              self.uop_cache: Dict[UOp, UOp] = {}                                                                                                                        # codegen/lowerer.py            :    82    G: {}     L: {}
                                              return self.to_uop(ast)                                                                                                                                    # codegen/lowerer.py            :    83    G: {}     L: {}

                                                class IndependentLowerer:                                                                                                                                # codegen/lowerer.py            :    38    G: {}     L: {}
                                                  def to_uop(self, x:UOp) -> UOp:                                                                                                                        # codegen/lowerer.py            :    85    G: {}     L: {}
                                                    if uop:=self.uop_cache.get(x, None): return uop                                                                                                      # codegen/lowerer.py            :    86    G: {}     L: {'x': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),))}
                                                    ret = self._to_uop(x)                                                                                                                                # codegen/lowerer.py            :    87    G: {}     L: {}

                                                      class IndependentLowerer:                                                                                                                          # codegen/lowerer.py            :    38    G: {}     L: {}
                                                        def _to_uop(self, x:UOp) -> UOp:                                                                                                                 # codegen/lowerer.py            :    91    G: {}     L: {}
                                                          if x.op in BUFFER_UOPS:                                                                                                                        # codegen/lowerer.py            :    92    G: {}     L: {}
                                                          in_uops = tuple(self.to_uop(y) for y in x.src)                                                                                                 # codegen/lowerer.py            :   108    G: {}     L: {}

                                                        class IndependentLowerer:                                                                                                                        # codegen/lowerer.py            :    38    G: {}     L: {}
                                                          def _to_uop(self, x:UOp) -> UOp:                                                                                                               # codegen/lowerer.py            :    91    G: {}     L: {}
                                                            if x.op in BUFFER_UOPS:                                                                                                                # OLD # codegen/lowerer.py            :    92
                                                              idx, valid = x.st_arg.to_indexed_uops(self.ridxs if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL else self.idxs)                 # codegen/lowerer.py            :    93    G: {}     L: {'x': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=(\n      UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),))}

                                                              has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                                            # codegen/lowerer.py            :    95    G: {}     L: {}
                                                              if x.op is UOps.CONST: return valid.where(UOp.const(x.dtype, x.arg), UOp.const(x.dtype, 0))                                                # codegen/lowerer.py            :    96    G: {}     L: {}
                                                              buf = x.src[0]                                                                                                                             # codegen/lowerer.py            :    97    G: {}     L: {}
                                                              if x.op is UOps.LOAD:                                                                                                                      # codegen/lowerer.py            :    98    G: {}     L: {}
                                                              if x.src[0].op is UOps.DEFINE_GLOBAL:                                                                                                      # codegen/lowerer.py            :   102    G: {}     L: {}
                                                                for oidx, ridx in zip(self.idxs, self.ridxs):                                                                                            # codegen/lowerer.py            :   103    G: {}     L: {}
                                                                  if oidx != ridx: valid = valid * oidx.eq(0)                                                                                            # codegen/lowerer.py            :   104    G: {}     L: {}
                                                                has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                                          # codegen/lowerer.py            :   105    G: {}     L: {}
                                                              return UOp(UOps.STORE, None, (buf, idx, self.to_uop(x.src[2])) + ((valid,) if has_valid else ()))                                          # codegen/lowerer.py            :   106    G: {}     L: {}

                                                          class IndependentLowerer:                                                                                                                      # codegen/lowerer.py            :    38    G: {}     L: {}
                                                            def _to_uop(self, x:UOp) -> UOp:                                                                                                             # codegen/lowerer.py            :    91    G: {}     L: {}
                                                              if x.op in BUFFER_UOPS:                                                                                                              # OLD # codegen/lowerer.py            :    92
                                                                  barrier = (UOp(UOps.BARRIER, None, (self.to_uop(x.src[2]),)),) if x.src[0].op is UOps.DEFINE_LOCAL else ()                             # codegen/lowerer.py            :    99    G: {}     L: {'x': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)), 'idx': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n  x0:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n    UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n       x0,\n      UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n    UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)), 'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=())}
                                                                  return UOp(UOps.LOAD, x.dtype, (buf, idx) + ((UOp.const(x.dtype, 0), valid) if has_valid else ()) + barrier)                           # codegen/lowerer.py            :   100    G: {}     L: {}

                                                          class IndependentLowerer:                                                                                                                      # codegen/lowerer.py            :    38    G: {}     L: {}
                                                            def to_uop(self, x:UOp) -> UOp:                                                                                                              # codegen/lowerer.py            :    85    G: {}     L: {}
                                                              if uop:=self.uop_cache.get(x, None): return uop                                                                                      # OLD # codegen/lowerer.py            :    86
                                                              ret = self._to_uop(x)                                                                                                                # OLD # codegen/lowerer.py            :    87
                                                              self.uop_cache[x] = ret                                                                                                                    # codegen/lowerer.py            :    88    G: {}     L: {'uop': None, 'ret': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n    x2:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n      UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n         x2,\n        UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n      UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),))}
                                                              return ret                                                                                                                                 # codegen/lowerer.py            :    89    G: {}     L: {}

                                                          class IndependentLowerer:                                                                                                                      # codegen/lowerer.py            :    38    G: {}     L: {}
                                                            def _to_uop(self, x:UOp) -> UOp:                                                                                                             # codegen/lowerer.py            :    91    G: {}     L: {}
                                                              if x.op in BUFFER_UOPS:                                                                                                              # OLD # codegen/lowerer.py            :    92
                                                              in_uops = tuple(self.to_uop(y) for y in x.src)                                                                                       # OLD # codegen/lowerer.py            :   108
                                                                idx, valid = x.st_arg.to_indexed_uops(self.ridxs if x.op is UOps.LOAD and x.src[0].op is UOps.DEFINE_LOCAL else self.idxs)         # OLD # codegen/lowerer.py            :    93
                                                                has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                                    # OLD # codegen/lowerer.py            :    95
                                                                if x.op is UOps.CONST: return valid.where(UOp.const(x.dtype, x.arg), UOp.const(x.dtype, 0))                                        # OLD # codegen/lowerer.py            :    96
                                                                buf = x.src[0]                                                                                                                     # OLD # codegen/lowerer.py            :    97
                                                                if x.op is UOps.LOAD:                                                                                                              # OLD # codegen/lowerer.py            :    98
                                                                if x.src[0].op is UOps.DEFINE_GLOBAL:                                                                                              # OLD # codegen/lowerer.py            :   102
                                                                  for oidx, ridx in zip(self.idxs, self.ridxs):                                                                                    # OLD # codegen/lowerer.py            :   103
                                                                    if oidx != ridx: valid = valid * oidx.eq(0)                                                                                    # OLD # codegen/lowerer.py            :   104
                                                                  has_valid = valid.op is not UOps.CONST or valid.arg is not True                                                                  # OLD # codegen/lowerer.py            :   105
                                                                return UOp(UOps.STORE, None, (buf, idx, self.to_uop(x.src[2])) + ((valid,) if has_valid else ()))                                  # OLD # codegen/lowerer.py            :   106
                                                                  barrier = (UOp(UOps.BARRIER, None, (self.to_uop(x.src[2]),)),) if x.src[0].op is UOps.DEFINE_LOCAL else ()                       # OLD # codegen/lowerer.py            :    99
                                                                  return UOp(UOps.LOAD, x.dtype, (buf, idx) + ((UOp.const(x.dtype, 0), valid) if has_valid else ()) + barrier)                     # OLD # codegen/lowerer.py            :   100
                                                              if x.op is UOps.REDUCE_AXIS:                                                                                                               # codegen/lowerer.py            :   109    G: {}     L: {'x': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=(\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)), 'in_uops': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n    x2:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n      UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n         x2,\n        UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n      UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),)), UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n  UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),)))}
                                                              return replace(x, src=in_uops)                                                                                                             # codegen/lowerer.py            :   126    G: {}     L: {}

                                        def linearize_uop(sink_in:Union[UOp, List[UOp]], opts:Optional[Renderer]=None, skip_check=False) -> List[UOp]:                                                   # codegen/uopgraph.py           :   525    G: {}     L: {}
                                          sink: UOp = sink_in if isinstance(sink_in, UOp) else UOp(UOps.SINK, None, tuple(sink_in))                                                                      # codegen/uopgraph.py           :   527    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False}
                                          assert sink.op is UOps.SINK, f"sink isn't sink, it's {sink.op}"                                                                                                # codegen/uopgraph.py           :   528    G: {}     L: {}
                                          folder = constant_folder + transcendental_folding(tuple() if TRANSCENDENTAL >= 2 or opts is None else tuple(opts.code_for_op.keys()))                          # codegen/uopgraph.py           :   529    G: {}     L: {}

                                            @functools.lru_cache(None)                                                                                                                                   # codegen/uopgraph.py           :   155    G: {}     L: {}
                                            def transcendental_folding(ops):
                                              return PatternMatcher([(UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=k), cast(Callable, v))                            # codegen/uopgraph.py           :   156    G: {}     L: {'ops': (<UnaryOps.NEG: 7>, <UnaryOps.SQRT: 6>, <UnaryOps.RECIP: 8>, <BinaryOps.ADD: 1>, <BinaryOps.MAX: 4>, <BinaryOps.IDIV: 3>, <BinaryOps.MUL: 2>, <BinaryOps.MOD: 5>, <BinaryOps.CMPLT: 6>, <BinaryOps.CMPNE: 7>, <BinaryOps.XOR: 8>, <BinaryOps.AND: 12>, <BinaryOps.OR: 11>, <TernaryOps.WHERE: 1>)}
                                                                     for k,v in ((UnaryOps.EXP2, xexp2), (UnaryOps.LOG2, xlog2), (UnaryOps.SIN, xsin)) if k not in ops])

                                            class PatternMatcher:                                                                                                                                        # ops.py                        :   280    G: {}     L: {}
                                              @functools.lru_cache(None)  # pylint: disable=method-cache-max-size-none                                                                                   # ops.py                        :   291    G: {}     L: {}
                                              def __add__(self, more:PatternMatcher): return PatternMatcher(self.patterns+more.patterns)

                                          acc_number = 0                                                                                                                                                 # codegen/uopgraph.py           :   532    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False, 'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'folder': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>}
                                          sink = graph_rewrite(sink, folder)                                                                                                                             # codegen/uopgraph.py           :   533    G: {}     L: {}

                                          # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                                    # dtype.py                      :    31    G: {}     L: {}
                                          class PtrDType(DType):
                                            def __hash__(self): return super().__hash__()                                                                                                                # dtype.py                      :    33    G: {}     L: {}

                                          class PatternMatcher:                                                                                                                                          # ops.py                        :   280    G: {}     L: {}
                                            def rewrite(self, uop:UOp) -> Optional[UOp]:                                                                                                                 # ops.py                        :   293    G: {}     L: {}
                                              for p,fxn in itertools.chain(self.pdict[(uop.op, uop.arg)], self.pdict[(uop.op, None)]):                                                             # OLD # ops.py                        :   294
                                                if (matches := _match(uop, p, {})) and (ret:=fxn(**matches[0])) is not None: return ret # NOTE: if it returns None, we keep trying to match              # ops.py                        :   295    G: {}     L: {'self': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>, 'uop': UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n  UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n  UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)), 'p': UPat((UOps.ALU), BinaryOps.MUL, name='x', dtype={dtypes.bool}, allow_any_len=True, src=(None)), 'fxn': <function <lambda> at 0x7c929979dd80>}

                                                  def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:                                                                           # ops.py                        :   266    G: {}     L: {}
                                                    if (pat.name is not None and store.setdefault(pat.name, uop) is not uop) or \                                                                        # ops.py                        :   267    G: {}     L: {'pat': UPat((UOps.ALU), BinaryOps.MUL, name='x', dtype={dtypes.bool}, allow_any_len=True, src=(None)), 'store': {}}
                                                       (pat.dtype is not None and uop.dtype not in pat.dtype) or \
                                                       (pat.arg is not None and pat.arg != uop.arg) or \
                                                       (pat.op is not None and uop.op not in pat.op): return []

                                                  def _match(uop:UOp, pat:UPat, store:Dict[str, UOp]) -> List[Dict[str, UOp]]:                                                                           # ops.py                        :   266    G: {}     L: {}
                                                    if (pat.name is not None and store.setdefault(pat.name, uop) is not uop) or \                                                                  # OLD # ops.py                        :   267
                                                       (pat.dtype is not None and uop.dtype not in pat.dtype) or \
                                                       (pat.arg is not None and pat.arg != uop.arg) or \
                                                       (pat.op is not None and uop.op not in pat.op): return []
                                                    if pat.src is None: return [store]                                                                                                                   # ops.py                        :   271    G: {}     L: {'pat': UPat((UOps.ALU), BinaryOps.MUL, name=None, dtype=None, allow_any_len=False, src=[\n  UPat(None, None, name='x', dtype=None, allow_any_len=True, src=(None)),\n  UPat((UOps.CONST), 1, name=None, dtype=None, allow_any_len=True, src=(None)),]), 'store': {}}
                                                    res: List[Dict[str, UOp]] = []                                                                                                                       # ops.py                        :   272    G: {}     L: {}
                                                    for vp in pat.src:                                                                                                                                   # ops.py                        :   273    G: {}     L: {}
                                                      if pat.allowed_len != 0 and len(uop.src) != pat.allowed_len: return []                                                                             # ops.py                        :   274    G: {}     L: {}
                                                      new_stores = [store.copy()]                                                                                                                        # ops.py                        :   275    G: {}     L: {}
                                                      for uu, vv in zip(uop.src, vp): new_stores = [rstore for nstore in new_stores for rstore in _match(uu, vv, nstore)]                                # ops.py                        :   276    G: {}     L: {}

                                                      res.extend(new_stores)                                                                                                                             # ops.py                        :   277    G: {}     L: {}

                                                    return res                                                                                                                                           # ops.py                        :   278    G: {}     L: {}

                                          @dataclass(frozen=True, eq=False)                                                                                                                              # ops.py                        :   100    G: {}     L: {}
                                          class UOp:
                                            @property                                                                                                                                                    # ops.py                        :   197    G: {}     L: {}
                                            def vmin(self) -> UOp: return x if (x:=self._min_max[0]) is not None and not math.isnan(x.arg) else self.sconst(dtypes.min(cast(DType, self.dtype)))

                                              @dataclass(frozen=True, eq=False)                                                                                                                          # ops.py                        :   100    G: {}     L: {}
                                              class UOp:
                                                @functools.cached_property                                                                                                                               # ops.py                        :   201    G: {}     L: {}
                                                def _min_max(self) -> Tuple[Optional[UOp], Optional[UOp]]:
                                                  if self.op is UOps.DEFINE_VAR and self.src: return self.src[0], self.src[1] if isinstance(self.src[1].arg, int) else None                              # ops.py                        :   203    G: {}     L: {}
                                                  if self.op is UOps.RANGE: return self.src[0].vmin, (self.src[1]-1).vmax                                                                                # ops.py                        :   204    G: {}     L: {}
                                                  if self.op is UOps.SPECIAL: return self.const(0), self.const(self.arg[1]-1) if isinstance(self.arg[1], int) else None                                  # ops.py                        :   206    G: {}     L: {}
                                                  if self.op is UOps.CONST: return self, self                                                                                                            # ops.py                        :   207    G: {}     L: {}
                                                  if self.op is UOps.ALU and cast(DType, self.dtype).count == 1:                                                                                         # ops.py                        :   208    G: {}     L: {}
                                                    s0,s1 = [cast(UOp, self.src[i] if i < len(self.src) else None) for i in range(2)]                                                                    # ops.py                        :   209    G: {}     L: {}
                                                    if self.arg is UnaryOps.NEG and self.dtype != dtypes.bool and not dtypes.is_unsigned(cast(DType, self.dtype)):                                       # ops.py                        :   210    G: {}     L: {}
                                                    if self.arg is BinaryOps.ADD: return self.sconst(s0.vmin.arg+s1.vmin.arg), self.sconst(s0.vmax.arg+s1.vmax.arg)                                      # ops.py                        :   212    G: {}     L: {}

                                                @dataclass(frozen=True, eq=False)                                                                                                                        # ops.py                        :   100    G: {}     L: {}
                                                class UOp:
                                                  @functools.cached_property                                                                                                                             # ops.py                        :   201    G: {}     L: {}
                                                  def _min_max(self) -> Tuple[Optional[UOp], Optional[UOp]]:
                                                    if self.op is UOps.DEFINE_VAR and self.src: return self.src[0], self.src[1] if isinstance(self.src[1].arg, int) else None                      # OLD # ops.py                        :   203
                                                    if self.op is UOps.RANGE: return self.src[0].vmin, (self.src[1]-1).vmax                                                                        # OLD # ops.py                        :   204
                                                    if self.op is UOps.SPECIAL: return self.const(0), self.const(self.arg[1]-1) if isinstance(self.arg[1], int) else None                          # OLD # ops.py                        :   206
                                                    if self.op is UOps.CONST: return self, self                                                                                                    # OLD # ops.py                        :   207
                                                    if self.op is UOps.ALU and cast(DType, self.dtype).count == 1:                                                                                 # OLD # ops.py                        :   208
                                                      s0,s1 = [cast(UOp, self.src[i] if i < len(self.src) else None) for i in range(2)]                                                            # OLD # ops.py                        :   209
                                                      if self.arg is UnaryOps.NEG and self.dtype != dtypes.bool and not dtypes.is_unsigned(cast(DType, self.dtype)):                               # OLD # ops.py                        :   210
                                                      if self.arg is BinaryOps.ADD: return self.sconst(s0.vmin.arg+s1.vmin.arg), self.sconst(s0.vmax.arg+s1.vmax.arg)                              # OLD # ops.py                        :   212
                                                    return None, None                                                                                                                                    # ops.py                        :   226    G: {}     L: {'self': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),))}

                                                class dtypes:                                                                                                                                            # dtype.py                      :    38    G: {}     L: {}
                                                  @staticmethod                                                                                                                                          # dtype.py                      :    59    G: {}     L: {}
                                                  def min(dtype:DType):
                                                    if dtypes.is_int(dtype): return 0 if dtypes.is_unsigned(dtype) else -2**(dtype.itemsize*8-1)                                                         # dtype.py                      :    60    G: {}     L: {'dtype': dtypes.int}

                                                @dataclass(frozen=True, eq=False)                                                                                                                        # ops.py                        :   100    G: {}     L: {}
                                                class UOp:
                                                  def sconst(self:Union[UOp, DType, None], b:ConstType|Variable):                                                                                        # ops.py                        :   151    G: {}     L: {}
                                                    return UOp._const(cast(DType, self.dtype if isinstance(self, UOp) else self).scalar() if self is not None else self, b)                              # ops.py                        :   152    G: {}     L: {'b': -2147483648}

                                                      @dataclass(frozen=True, eq=False)                                                                                                                  # ops.py                        :   100    G: {}     L: {}
                                                      class UOp:
                                                        @property                                                                                                                                        # ops.py                        :   199    G: {}     L: {}
                                                        def vmax(self) -> UOp: return x if (x:=self._min_max[1]) is not None and not math.isnan(x.arg) else self.sconst(dtypes.max(cast(DType, self.dtype)))

                                                          class dtypes:                                                                                                                                  # dtype.py                      :    38    G: {}     L: {}
                                                            @staticmethod                                                                                                                                # dtype.py                      :    63    G: {}     L: {}
                                                            def max(dtype:DType):
                                                              if dtypes.is_int(dtype): return (2**(dtype.itemsize*8-(0 if dtypes.is_unsigned(dtype) else 1)))-1                                          # dtype.py                      :    64    G: {}     L: {}

                                          sink = graph_rewrite(sink, no_pyint)                                                                                                                           # codegen/uopgraph.py           :   536    G: {}     L: {}

                                          linearize_cnt += 1                                                                                                                                             # codegen/uopgraph.py           :   539    G: {}     L: {}
                                          if linearize_cnt != getenv("DEBUG_EXPAND", 0):                                                                                                                 # codegen/uopgraph.py           :   540    G: {}     L: {}

                                            sink = graph_rewrite(sink, folder+expander+float4_folding if opts is not None and opts.supports_float4 else folder+expander)                                 # codegen/uopgraph.py           :   541    G: {}     L: {}

                                          def do_expand(root:UOp):                                                                                                                                       # codegen/uopgraph.py           :   382    G: {}     L: {}
                                            expands = [x for x in root.src if x.op is UOps.EXPAND]                                                                                                       # codegen/uopgraph.py           :   383    G: {}     L: {'root': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}
                                            if len(expands) == 0: return None                                                                                                                            # codegen/uopgraph.py           :   384    G: {}     L: {}

                                          def create_gate(root:UOp) -> Optional[UOp]:                                                                                                                    # codegen/uopgraph.py           :   452    G: {}     L: {}
                                            return None if len(root.src) == 3 or (ret:=_gate_srcs(root, root.src[3])) is root else ret                                                                   # codegen/uopgraph.py           :   457    G: {}     L: {}

                                          def fold_expanded(ex, buf):                                                                                                                                    # codegen/uopgraph.py           :    13    G: {}     L: {}
                                            if buf.dtype != PtrDType(dtypes.float) and buf.dtype != PtrDType(dtypes.half) and not isinstance(buf.dtype, ImageDType): return None                         # codegen/uopgraph.py           :    14    G: {}     L: {'ex': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}

                                              # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                                # dtype.py                      :    31    G: {}     L: {}
                                              class PtrDType(DType):
                                                def __ne__(self, dt): return not (self == dt)                                                                                                            # dtype.py                      :    35    G: {}     L: {}

                                                  # @dataclass(frozen=True, init=False, repr=False, eq=False)                                                                                            # dtype.py                      :    31    G: {}     L: {}
                                                  class PtrDType(DType):
                                                    def __eq__(self, dt): return self.priority==dt.priority and self.itemsize==dt.itemsize and self.name==dt.name and self.count==dt.count               # dtype.py                      :    34    G: {}     L: {}

                                            sink = graph_rewrite(sink, folder+expander+reducer)                                                                                                          # codegen/uopgraph.py           :   542    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False, 'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'folder': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>}

                                          def fix_unfoldable_image_load(load:UOp, buf:UOp):                                                                                                              # codegen/uopgraph.py           :    71    G: {}     L: {}
                                            if not isinstance(buf.dtype, ImageDType) or cast(DType, load.src[1].dtype).count == 2: return None                                                           # codegen/uopgraph.py           :    72    G: {}     L: {'buf': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), 'load': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}

                                          def no_vectorized_alu(alu):                                                                                                                                    # codegen/uopgraph.py           :   446    G: {}     L: {}
                                            if alu.dtype.count == 1: return None                                                                                                                         # codegen/uopgraph.py           :   447    G: {}     L: {'alu': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))}

                                          def delete_redundant_gates(root:UOp) -> Optional[UOp]:                                                                                                         # codegen/uopgraph.py           :   479    G: {}     L: {}
                                            if len(root.src) == 3 or (gate:=find_gate(root)) is None or gate.src[0] is not root.src[3]: return None                                                      # codegen/uopgraph.py           :   484    G: {}     L: {}

                                          if opts is not None and opts.extra_matcher is not None: sink = graph_rewrite(sink, folder+opts.extra_matcher)                                                  # codegen/uopgraph.py           :   545    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False, 'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'folder': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>}
                                          children: Dict[UOp, List[UOp]] = {}                                                                                                                            # codegen/uopgraph.py           :   549    G: {}     L: {}
                                          range_srcs: Dict[UOp, Dict[UOp, None]] = {}                                                                                                                    # codegen/uopgraph.py           :   550    G: {}     L: {}
                                          in_degree: Dict[UOp, int] = {}                                                                                                                                 # codegen/uopgraph.py           :   551    G: {}     L: {}
                                          get_children_dfs(sink, children, range_srcs, in_degree)                                                                                                        # codegen/uopgraph.py           :   552    G: {}     L: {}

                                            def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                                       # codegen/uopgraph.py           :   502    G: {}     L: {}
                                              if u in children: return srcs[u]                                                                                                                           # codegen/uopgraph.py           :   503    G: {}     L: {'u': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'srcs': {}}
                                              srcs[u] = {}                                                                                                                                               # codegen/uopgraph.py           :   504    G: {}     L: {}
                                              children[u] = []                                                                                                                                           # codegen/uopgraph.py           :   505    G: {}     L: {}
                                              for x in u.src:                                                                                                                                            # codegen/uopgraph.py           :   506    G: {}     L: {}
                                                srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                                           # codegen/uopgraph.py           :   507    G: {}     L: {}

                                              def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                                     # codegen/uopgraph.py           :   502    G: {}     L: {}
                                                if u in children: return srcs[u]                                                                                                                   # OLD # codegen/uopgraph.py           :   503
                                                srcs[u] = {}                                                                                                                                       # OLD # codegen/uopgraph.py           :   504
                                                children[u] = []                                                                                                                                   # OLD # codegen/uopgraph.py           :   505
                                                for x in u.src:                                                                                                                                    # OLD # codegen/uopgraph.py           :   506
                                                  srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                                   # OLD # codegen/uopgraph.py           :   507
                                                in_degree[u] = len(u.src)                                                                                                                                # codegen/uopgraph.py           :   510    G: {}     L: {'u': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}
                                                return srcs[u]                                                                                                                                           # codegen/uopgraph.py           :   511    G: {}     L: {}

                                                  def get_children_dfs(u:UOp, children:Dict[UOp, List[UOp]], srcs:Dict[UOp, Dict[UOp, None]], in_degree:Dict[UOp, int]):                                 # codegen/uopgraph.py           :   502    G: {}     L: {}
                                                    if u in children: return srcs[u]                                                                                                               # OLD # codegen/uopgraph.py           :   503
                                                    srcs[u] = {}                                                                                                                                   # OLD # codegen/uopgraph.py           :   504
                                                    children[u] = []                                                                                                                               # OLD # codegen/uopgraph.py           :   505
                                                    for x in u.src:                                                                                                                                # OLD # codegen/uopgraph.py           :   506
                                                      srcs[u].update(get_children_dfs(x, children, srcs, in_degree))                                                                               # OLD # codegen/uopgraph.py           :   507
                                                      if x.op is UOps.RANGE and x.arg[1]: srcs[u][x] = None                                                                                              # codegen/uopgraph.py           :   508    G: {}     L: {'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'x': UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=())}
                                                      children[x].append(u)                                                                                                                              # codegen/uopgraph.py           :   509    G: {}     L: {}

                                          scope_children = {p:get_recursive_children(p, END_FOR_UOP[p.op][0]) for p in reversed(in_degree) if p.op in END_FOR_UOP}                                       # codegen/uopgraph.py           :   560    G: {}     L: {}

                                          def linearize_uop(sink_in:Union[UOp, List[UOp]], opts:Optional[Renderer]=None, skip_check=False) -> List[UOp]:                                                 # codegen/uopgraph.py           :   525    G: {}     L: {}
                                            @functools.lru_cache(None)                                                                                                                                   # codegen/uopgraph.py           :   555    G: {}     L: {}
                                            def get_recursive_children(x:UOp, end:UOps, include_self=False) -> Set[UOp]:
                                              if x.op is UOps.SINK: return set()                                                                                                                         # codegen/uopgraph.py           :   556    G: {}     L: {'include_self': False, 'end': <UOps.PHI: 23>, 'x': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))}
                                              return set.union({x} if include_self else set(), *([get_recursive_children(u, end, True) for u in children[x] if x.op is not end]))                        # codegen/uopgraph.py           :   557    G: {}     L: {}

                                          range_phi = {r:[p for p in scope_children[r] if p.op is UOps.PHI] for r in scope_children if r.op is UOps.RANGE}                                               # codegen/uopgraph.py           :   561    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False, 'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'folder': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>, 'range_srcs': {UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): {}, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): {}, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): {}, UOp(UOps.CONST, dtypes.int, arg=0, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=3, src=()): {}, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): {}, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): {}, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): {}, UOp(UOps.CONST, dtypes.int, arg=2, src=()): {}}, 'in_degree': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 0, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 2, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 0, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 2, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 0, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 2, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): 3, UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): 1}, 'scope_children': {UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): {UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))}}}
                                          queue:List[Tuple[int, UOp]] = []                                                                                                                               # codegen/uopgraph.py           :   563    G: {}     L: {}
                                          for u in children:                                                                                                                                             # codegen/uopgraph.py           :   576    G: {}     L: {}
                                            if in_degree[u] == 0: push(u)                                                                                                                                # codegen/uopgraph.py           :   577    G: {}     L: {}

                                              def linearize_uop(sink_in:Union[UOp, List[UOp]], opts:Optional[Renderer]=None, skip_check=False) -> List[UOp]:                                             # codegen/uopgraph.py           :   525    G: {}     L: {}
                                                def push(u:UOp):                                                                                                                                         # codegen/uopgraph.py           :   564    G: {}     L: {}
                                                  priority = 0                                                                                                                                           # codegen/uopgraph.py           :   565    G: {}     L: {}
                                                  if u.op is UOps.RANGE and u.arg[1]:                                                                                                                    # codegen/uopgraph.py           :   567    G: {}     L: {}
                                                    priority -= sum([(l.arg[0]+1) + 1000*l.arg[1] for l,ss in scope_children.items() if l.op is UOps.RANGE and u in ss])                                 # codegen/uopgraph.py           :   573    G: {}     L: {}
                                                  heapq.heappush(queue, (priority, u))                                                                                                                   # codegen/uopgraph.py           :   574    G: {}     L: {}

                                          @dataclass(frozen=True, eq=False)                                                                                                                              # ops.py                        :   100    G: {}     L: {}
                                          class UOp:
                                            def __lt__(self, x:UOp): return self.cmp_tuple < x.cmp_tuple                                                                                                 # ops.py                        :   113    G: {}     L: {}

                                              @dataclass(frozen=True, eq=False)                                                                                                                          # ops.py                        :   100    G: {}     L: {}
                                              class UOp:
                                                @functools.cached_property                                                                                                                               # ops.py                        :   109    G: {}     L: {}
                                                def cmp_tuple(self):
                                                  return (self.op.value, (self.arg if self.op is not UOps.DEFINE_VAR else self.arg.expr) if self.op is not UOps.ALU else \                               # ops.py                        :   111    G: {}     L: {}
                                                          self.arg.value, self.dtype, self.src)

                                          scope_end: Dict[UOp, UOp] = {}                                                                                                                                 # codegen/uopgraph.py           :   579    G: {}     L: {'sink_in': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n      x3:=UOp(UOps.CONST, dtypes.pyint, arg=0, src=()),\n      UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n        x5:=UOp(UOps.RANGE, dtypes.pyint, arg=(0, False), src=(\n           x3,\n          UOp(UOps.CONST, dtypes.pyint, arg=3, src=()),)),\n        x7:=UOp(UOps.CONST, dtypes.pyint, arg=1, src=()),)),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.ADD, src=(\n           x3,\n          UOp(UOps.ALU, dtypes.pyint, arg=BinaryOps.MUL, src=(\n             x5,\n             x7,)),)),)),\n      UOp(UOps.ALU, dtypes.int, arg=TernaryOps.WHERE, src=(\n        UOp(UOps.CONST, dtypes.bool, arg=True, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=2, src=()),\n        UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),)),)),)), 'opts': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'skip_check': False, 'sink': UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)), 'folder': <tinygrad.ops.PatternMatcher object at 0x7c9287b9a560>, 'children': {UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): [], UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): [UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))], UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))], UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], UOp(UOps.CONST, dtypes.int, arg=0, src=()): [UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))], UOp(UOps.CONST, dtypes.int, arg=3, src=()): [UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))], UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): [UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),))], UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): [UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))], UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): [UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], UOp(UOps.CONST, dtypes.int, arg=2, src=()): [UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))]}, 'in_degree': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 0, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 0, UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 2, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 0, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 2, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 0, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 2, UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)): 3, UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),)): 1}, 'get_recursive_children': <functools._lru_cache_wrapper object at 0x7c92879db5e0>, 'push': <function linearize_uop.<locals>.push at 0x7c9287b9feb0>, 'u': UOp(UOps.CONST, dtypes.int, arg=2, src=())}
                                          _uops: List[UOp] = []                                                                                                                                          # codegen/uopgraph.py           :   580    G: {}     L: {}
                                          while queue:                                                                                                                                                   # codegen/uopgraph.py           :   581    G: {}     L: {}
                                            p,x = heapq.heappop(queue)                                                                                                                                   # codegen/uopgraph.py           :   582    G: {}     L: {}

                                            if DEBUG >= 7: print(f"{p:5d}",x)                                                                                                                            # codegen/uopgraph.py           :   583    G: {}     L: {}

                                            if x in scope_children: scope_end[x] = x                                                                                                                     # codegen/uopgraph.py           :   584    G: {}     L: {}
                                            if x.op is UOps.DEFINE_ACC:                                                                                                                                  # codegen/uopgraph.py           :   585    G: {}     L: {}
                                            else: _uops.append(x)                                                                                                                                        # codegen/uopgraph.py           :   588    G: {}     L: {}
                                            for u, ss in scope_children.items():                                                                                                                         # codegen/uopgraph.py           :   589    G: {}     L: {}
                                              if x in ss:                                                                                                                                                # codegen/uopgraph.py           :   590    G: {}     L: {}
                                            for u in children[x]:                                                                                                                                        # codegen/uopgraph.py           :   593    G: {}     L: {}
                                              in_degree[u] -= 1                                                                                                                                          # codegen/uopgraph.py           :   594    G: {}     L: {}
                                              if in_degree[u] == 0: push(u)                                                                                                                              # codegen/uopgraph.py           :   595    G: {}     L: {}

                                                ss.remove(x)                                                                                                                                             # codegen/uopgraph.py           :   591    G: {}     L: {}
                                                if len(ss) == 0: scope_end[u] = x                                                                                                                        # codegen/uopgraph.py           :   592    G: {}     L: {}

                                            for u in children[x]:                                                                                                                                  # OLD # codegen/uopgraph.py           :   593
                                              in_degree[u] -= 1                                                                                                                                    # OLD # codegen/uopgraph.py           :   594
                                              if in_degree[u] == 0: push(u)                                                                                                                        # OLD # codegen/uopgraph.py           :   595
                                          for u, x in scope_end.items(): _uops.insert(_uops.index(x)+1, UOp(END_FOR_UOP[u.op][1], None, (u,)))                                                           # codegen/uopgraph.py           :   598    G: {}     L: {}
                                          if not skip_check:                                                                                                                                             # codegen/uopgraph.py           :   601    G: {}     L: {}
                                            bad_ops = dedup([x.op for x in _uops if x.op in {UOps.EXPAND, UOps.CONTRACT, UOps.REDUCE}])                                                                  # codegen/uopgraph.py           :   602    G: {}     L: {}

                                            try:                                                                                                                                                         # codegen/uopgraph.py           :   603    G: {}     L: {}
                                              type_verify(_uops)                                                                                                                                         # codegen/uopgraph.py           :   604    G: {}     L: {}

                                                def type_verify(uops):                                                                                                                                   # ops.py                        :   298    G: {}     L: {}
                                                  for u in uops:                                                                                                                                         # ops.py                        :   299    G: {}     L: {'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.SINK, None, arg=KernelInfo(local_dims=0, upcasted=0, dont_use_locals=False), src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    x2:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n         x2,)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)),))]}
                                                    uop, arg, src, dtype = u.op, u.arg, u.src, u.dtype                                                                                                   # ops.py                        :   300    G: {}     L: {}
                                                    if uop is UOps.DEFINE_LOCAL: assert isinstance(dtype, PtrDType), f"invalid dtype for local buffer {dtype}"                                           # ops.py                        :   301    G: {}     L: {}
                                                    if uop is UOps.DEFINE_GLOBAL: assert isinstance(dtype, (PtrDType, ImageDType)), f"invalid dtype for global buffer {dtype}"                           # ops.py                        :   302    G: {}     L: {}
                                                    if isinstance(dtype, ImageDType): assert uop is UOps.DEFINE_GLOBAL, f"{uop} can't be image"                                                          # ops.py                        :   303    G: {}     L: {}
                                                    if uop in {UOps.CONST, UOps.DEFINE_ACC}:                                                                                                             # ops.py                        :   304    G: {}     L: {}
                                                    if uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}: assert arg is None and dtype is not None # type is the output type, not an arg                  # ops.py                        :   309    G: {}     L: {}
                                                    if uop is UOps.CAST: assert dtype.count == 1 and len(src) == 1                                                                                       # ops.py                        :   310    G: {}     L: {}
                                                    if uop is UOps.VECTORIZE:                                                                                                                            # ops.py                        :   311    G: {}     L: {}
                                                    if uop is UOps.LOAD and len(src) > 3 and src[3].op is UOps.ALU: assert src[3].dtype == dtypes.bool and src[2].dtype == dtype                         # ops.py                        :   314    G: {}     L: {}
                                                    if uop is UOps.GEP: assert dtype == src[0].dtype.scalar(), f"GEP of {src[0].dtype=} should be {src[0].dtype.scalar()} != {dtype}"                    # ops.py                        :   315    G: {}     L: {}
                                                    if uop is UOps.STORE:                                                                                                                                # ops.py                        :   316    G: {}     L: {}
                                                    if uop is UOps.ALU:                                                                                                                                  # ops.py                        :   319    G: {}     L: {}
                                                      if uop is UOps.CONST:                                                                                                                              # ops.py                        :   305    G: {}     L: {}
                                                        assert dtype is not None and dtype == dtype.scalar(), f"consts must be scalar, got {dtype}"                                                      # ops.py                        :   306    G: {}     L: {}

                                                        assert type(arg) is type(dtypes.as_const(arg, dtype)), f"type of {arg=} does not match {dtype}"                                                  # ops.py                        :   307    G: {}     L: {}

                                                      if uop is UOps.DEFINE_ACC: assert dtype is not None and src[0].dtype == dtype, f"dtype mismatch {src[0].dtype=} != {dtype=}"                       # ops.py                        :   308    G: {}     L: {}

                                                    if uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}: assert arg is None and dtype is not None # type is the output type, not an arg            # OLD # ops.py                        :   309
                                                    if uop is UOps.CAST: assert dtype.count == 1 and len(src) == 1                                                                                 # OLD # ops.py                        :   310
                                                    if uop is UOps.VECTORIZE:                                                                                                                      # OLD # ops.py                        :   311
                                                    if uop is UOps.LOAD and len(src) > 3 and src[3].op is UOps.ALU: assert src[3].dtype == dtypes.bool and src[2].dtype == dtype                   # OLD # ops.py                        :   314
                                                    if uop is UOps.GEP: assert dtype == src[0].dtype.scalar(), f"GEP of {src[0].dtype=} should be {src[0].dtype.scalar()} != {dtype}"              # OLD # ops.py                        :   315
                                                    if uop is UOps.STORE:                                                                                                                          # OLD # ops.py                        :   316
                                                    if uop is UOps.ALU:                                                                                                                            # OLD # ops.py                        :   319
                                                      if arg in UnaryOps: assert dtype == src[0].dtype, f"{arg} dtype mismatch {dtype=} != {src[0].dtype=}"                                              # ops.py                        :   320    G: {}     L: {}
                                                      elif arg in {BinaryOps.CMPLT, BinaryOps.CMPNE}:                                                                                                    # ops.py                        :   321    G: {}     L: {}
                                                      elif arg is BinaryOps.IDIV:                                                                                                                        # ops.py                        :   324    G: {}     L: {}
                                                      elif arg in {BinaryOps.SHL, BinaryOps.SHR}:                                                                                                        # ops.py                        :   327    G: {}     L: {}
                                                      elif arg in BinaryOps: assert dtype == src[0].dtype == src[1].dtype, f"{arg} dtype mismatch {dtype=} != {src[0].dtype=} != {src[1].dtype=}"        # ops.py                        :   330    G: {}     L: {}
                                                      assert dtype is None, f"{uop} dtype must be None, got {dtype}"                                                                                     # ops.py                        :   317    G: {}     L: {}
                                                      if len(src) == 4: assert src[3].dtype == dtypes.bool, f"gate dtype mismatch {src[3].dtype} != {dtypes.bool}"                                       # ops.py                        :   318    G: {}     L: {}

                                              assert _uops[-1].op is UOps.SINK, f"didn't end with SINK, ended with {_uops[-1]}"                                                                          # codegen/uopgraph.py           :   605    G: {}     L: {}
                                              assert len(bad_ops) == 0, f"bad UOps left in list: {bad_ops}"                                                                                              # codegen/uopgraph.py           :   606    G: {}     L: {}
                                              assert len(all_stores := [x.src[0:2]+x.src[3:] for x in _uops if x.op is UOps.STORE and x.src[0].op is not UOps.DEFINE_LOCAL]) \                           # codegen/uopgraph.py           :   609    G: {}     L: {}
                                                == len(dedup(all_stores)), "repeated stores in uops"

                                          return _uops[:-1]                                                                                                                                              # codegen/uopgraph.py           :   619    G: {}     L: {}

                                      if DEBUG >= 5: print_uops(self.uops)                                                                                                                               # codegen/kernel.py             :   741    G: {}     L: {}

                                      if getenv("GRAPHUOPS"):                                                                                                                                            # codegen/kernel.py             :   742    G: {}     L: {}

                                      return self                                                                                                                                                        # codegen/kernel.py             :   745    G: {}     L: {}

                                src = self.opts.render(name:=to_function_name(ansiname:=(name_override if name_override is not None else self.name)), self.uops)                                         # codegen/kernel.py             :   749    G: {}     L: {'name_override': None}

                                  class Kernel:                                                                                                                                                          # codegen/kernel.py             :    54    G: {}     L: {}
                                    @functools.cached_property                                                                                                                                           # codegen/kernel.py             :   621    G: {}     L: {}
                                    def name(self) -> str:
                                      name = ("r" if self.reduceop else ("C" if all(x.op in BUFFER_UOPS for x in self.ast.parents) else "E")) + \                                                        # codegen/kernel.py             :   623    G: {}     L: {}
                                                   (f"{len(self.ast.src)}_" if len(self.ast.src) > 1 else "_") + \
                                                   colored('_', 'BLACK').join([colored(str(x), c) for x,c in zip(self.full_shape, self.colors())])

                                        class Kernel:                                                                                                                                                    # codegen/kernel.py             :    54    G: {}     L: {}
                                          @property                                                                                                                                                      # codegen/kernel.py             :   150    G: {}     L: {}
                                          def reduceop(self) -> Optional[UOp]: return self.reduceops[0] if len(self.reduceops) > 0 else None

                                        class Kernel:                                                                                                                                                    # codegen/kernel.py             :    54    G: {}     L: {}
                                          # there's eight chunks of the shape                                                                                                                            # codegen/kernel.py             :   181    G: {}     L: {}
                                          # blue   -- global dims
                                          # cyan   -- local dims (warp ones first)
                                          #  *** self.first_reduce
                                          # green  -- reduce-local dims
                                          # white  -- reduce-late upcasted dim (self.upcast_in_mid_reduce_axes)
                                          # red    -- reduce loops
                                          #  *** self.upcasted
                                          # purple -- reduce upcasted
                                          # yellow -- normal upcasted dimensions
                                          def colors(self) -> List[str]:
                                            colors = ["blue"] * self.global_dims if not self.dont_use_locals else ["BLUE"] * self.global_dims                                                            # codegen/kernel.py             :   183    G: {}     L: {}

                                              class Kernel:                                                                                                                                              # codegen/kernel.py             :    54    G: {}     L: {}
                                                @property                                                                                                                                                # codegen/kernel.py             :   169    G: {}     L: {}
                                                def global_dims(self) -> int: return self.first_reduce-self.local_dims

                                            colors += ["cyan"] * self.local_dims                                                                                                                         # codegen/kernel.py             :   185    G: {}     L: {'colors': ['blue']}
                                            colors += ["white" if i in self.upcast_in_mid_reduce_axes else "green" for i in range(self.first_reduce, self.first_reduce + self.group_for_reduces)]  # noqa: E501 # codegen/kernel.py             :   187    G: {}     L: {}

                                            colors += ["red"] * (self.first_upcast - (self.first_reduce + self.group_for_reduces))                                                                       # codegen/kernel.py             :   189    G: {}     L: {}

                                            colors += ["magenta" if self.full_shape[i] != self.sts[0].shape[i] else "yellow" for i in range(self.first_upcast, self.shape_len)]                          # codegen/kernel.py             :   191    G: {}     L: {}

                                            assert len(colors) == self.shape_len, "colors size mismatch"                                                                                                 # codegen/kernel.py             :   192    G: {}     L: {}

                                            return colors                                                                                                                                                # codegen/kernel.py             :   193    G: {}     L: {}

                                      Kernel.kernel_cnt[(function_name := to_function_name(name))] += 1                                                                                                  # codegen/kernel.py             :   628    G: {}     L: {'name': 'E_\x1b[34m3\x1b[0m'}

                                        @functools.lru_cache(maxsize=None)                                                                                                                               # helpers.py                    :    77    G: {}     L: {}
                                        def to_function_name(s:str): return ''.join([c if c in (string.ascii_letters+string.digits+'_') else f'{ord(c):02X}' for c in ansistrip(s)])

                                          def ansistrip(s:str): return re.sub('\x1b\\[(K|.*?m)', '', s)                                                                                                  # helpers.py                    :    31    G: {}     L: {}

                                      suffix = f"{'n'+str(Kernel.kernel_cnt[function_name]-1)}" if Kernel.kernel_cnt[function_name] > 1 else ""                                                          # codegen/kernel.py             :   629    G: {}     L: {}
                                      return name+colored(suffix, 'BLACK')                                                                                                                               # codegen/kernel.py             :   630    G: {}     L: {}

                                  class CStyleLanguage(Renderer):                                                                                                                                        # renderer/cstyle.py            :     9    G: {}     L: {}
                                    def render(self, name:str, uops:List[UOp]) -> str:                                                                                                                   # renderer/cstyle.py            :    96    G: {}     L: {}
                                      kernel = []                                                                                                                                                        # renderer/cstyle.py            :    97    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'self': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>}
                                      bufs: Dict[UOp, Tuple[str, Tuple[DType, bool]]] = {}                                                                                                               # renderer/cstyle.py            :    98    G: {}     L: {}
                                      depth = 1                                                                                                                                                          # renderer/cstyle.py            :    99    G: {}     L: {}
                                      c: DefaultDict[str, int] = defaultdict(int)                                                                                                                        # renderer/cstyle.py            :   102    G: {}     L: {}
                                      r: Dict[UOp, str] = {}                                                                                                                                             # renderer/cstyle.py            :   103    G: {}     L: {}
                                      child_count = Counter(v for ru in uops for v in ru.src)                                                                                                            # renderer/cstyle.py            :   112    G: {}     L: {}
                                      seen_vars = set()                                                                                                                                                  # renderer/cstyle.py            :   114    G: {}     L: {}
                                      for u in uops:                                                                                                                                                     # renderer/cstyle.py            :   115    G: {}     L: {}
                                        uop,dtype,src,args = u.op,u.dtype,u.src,u.arg                                                                                                                    # renderer/cstyle.py            :   116    G: {}     L: {}
                                        if uop is UOps.IF:                                                                                                                                               # renderer/cstyle.py            :   118    G: {}     L: {}
                                        elif uop is UOps.BARRIER: kk(self.barrier)                                                                                                                       # renderer/cstyle.py            :   121    G: {}     L: {}
                                        elif uop in {UOps.ENDRANGE, UOps.ENDIF}:                                                                                                                         # renderer/cstyle.py            :   122    G: {}     L: {}
                                        elif uop is UOps.STORE:                                                                                                                                          # renderer/cstyle.py            :   125    G: {}     L: {}
                                          assert dtype is not None, f"None dtype for uop {uop}"                                                                                                          # renderer/cstyle.py            :   132    G: {}     L: {}
                                          if uop is UOps.RANGE:                                                                                                                                          # renderer/cstyle.py            :   133    G: {}     L: {}
                                          elif uop is UOps.ALU:                                                                                                                                          # renderer/cstyle.py            :   136    G: {}     L: {}
                                          elif uop is UOps.SPECIAL:                                                                                                                                      # renderer/cstyle.py            :   146    G: {}     L: {}
                                          elif uop is UOps.DEFINE_VAR:                                                                                                                                   # renderer/cstyle.py            :   149    G: {}     L: {}
                                          elif uop is UOps.LOAD:                                                                                                                                         # renderer/cstyle.py            :   154    G: {}     L: {}
                                          elif uop is UOps.PHI:                                                                                                                                          # renderer/cstyle.py            :   159    G: {}     L: {}
                                          elif uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}:                                                                                                         # renderer/cstyle.py            :   162    G: {}     L: {}
                                          elif uop is UOps.DEFINE_LOCAL:                                                                                                                                 # renderer/cstyle.py            :   172    G: {}     L: {}
                                          elif uop is UOps.DEFINE_GLOBAL:                                                                                                                                # renderer/cstyle.py            :   175    G: {}     L: {}
                                            bufs[u] = (nm:=f"data{args}", (dtype, False))                                                                                                                # renderer/cstyle.py            :   176    G: {}     L: {}
                                            r[u] = nm                                                                                                                                                    # renderer/cstyle.py            :   177    G: {}     L: {}
                                          elif uop is UOps.WMMA: kk(f"{self.render_dtype(dtype)} {ssa('wmma',u)} = __{args[0]}({r[src[0]]}, {r[src[1]]}, {r[src[2]]});")                                 # renderer/cstyle.py            :   178    G: {}     L: {}
                                          elif uop is UOps.DEFINE_ACC: kk(f"{self.render_dtype(dtype)} {ssa('acc',u)} = {r[src[0]]};")                                                                   # renderer/cstyle.py            :   179    G: {}     L: {}
                                          elif uop is UOps.CONST: r[u] = self.render_const(args, dtype) if args >= 0 else f"({self.render_const(args, dtype)})"                                          # renderer/cstyle.py            :   180    G: {}     L: {}

                                            class CStyleLanguage(Renderer):                                                                                                                              # renderer/cstyle.py            :     9    G: {}     L: {}
                                              # returns a str expression of the const with the given type                                                                                                # renderer/cstyle.py            :    48    G: {}     L: {}
                                              def render_const(self, x:ConstType, dtype:DType) -> str:
                                                assert dtype.count == 1, f"consts should be scalar, got {dtype}"                                                                                         # renderer/cstyle.py            :    49    G: {}     L: {'x': 0}
                                                if math.isnan(x): val = self.nan                                                                                                                         # renderer/cstyle.py            :    50    G: {}     L: {}
                                                elif math.isinf(x): val = ("-" if x < 0 else "") + self.infinity                                                                                         # renderer/cstyle.py            :    51    G: {}     L: {}
                                                elif dtype == dtypes.bool: val = "1" if x else "0"                                                                                                       # renderer/cstyle.py            :    52    G: {}     L: {}
                                                elif dtype == dtypes.float: val = f"{x}f"                                                                                                                # renderer/cstyle.py            :    53    G: {}     L: {}
                                                elif dtype == dtypes.uint64: val = f"{x}ULL"                                                                                                             # renderer/cstyle.py            :    54    G: {}     L: {}
                                                else: val = str(x)                                                                                                                                       # renderer/cstyle.py            :    55    G: {}     L: {}
                                                return (self.render_cast(val, dtype) if dtype not in [dtypes.float, dtypes.int, dtypes.bool] else val)                                                   # renderer/cstyle.py            :    56    G: {}     L: {}

                                            kk(f"for (int {(expr := ssa('ridx',u))} = {r[src[0]]}; {expr} < {r[src[1]]}; {expr}++) {{")                                                                  # renderer/cstyle.py            :   134    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': [], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 1, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x7c92879ebd00>, 'c': defaultdict(<class 'int'>, {}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x7c9287aba5f0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), 'uop': <UOps.RANGE: 26>, 'src': (UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=())), 'args': (0, False), 'nm': 'data1'}

                                              class CStyleLanguage(Renderer):                                                                                                                            # renderer/cstyle.py            :     9    G: {}     L: {}
                                                def render(self, name:str, uops:List[UOp]) -> str:                                                                                                       # renderer/cstyle.py            :    96    G: {}     L: {}
                                                  def ssa(prefix:str, u:Optional[UOp]=None):                                                                                                             # renderer/cstyle.py            :   105    G: {}     L: {}
                                                    ret = f"{prefix}{c[prefix]}"                                                                                                                         # renderer/cstyle.py            :   107    G: {}     L: {'prefix': 'ridx'}
                                                    if u is not None: r[u] = ret                                                                                                                         # renderer/cstyle.py            :   108    G: {}     L: {}
                                                    c[prefix] += 1                                                                                                                                       # renderer/cstyle.py            :   109    G: {}     L: {}
                                                    return ret                                                                                                                                           # renderer/cstyle.py            :   110    G: {}     L: {}

                                              class CStyleLanguage(Renderer):                                                                                                                            # renderer/cstyle.py            :     9    G: {}     L: {}
                                                def render(self, name:str, uops:List[UOp]) -> str:                                                                                                       # renderer/cstyle.py            :    96    G: {}     L: {}
                                                  def kk(s): kernel.append("  "*depth+s)                                                                                                                 # renderer/cstyle.py            :   100    G: {}     L: {}

                                            depth += 1                                                                                                                                                   # renderer/cstyle.py            :   135    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'self': <tinygrad.renderer.cstyle.ClangRenderer object at 0x7c9287b4a1d0>, 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x7c92879ebd00>, 'c': defaultdict(<class 'int'>, {'ridx': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x7c9287aba5f0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), 'uop': <UOps.RANGE: 26>, 'dtype': dtypes.int, 'src': (UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=())), 'args': (0, False), 'nm': 'data1', 'expr': 'ridx0'}
                                          elif uop is UOps.ALU:                                                                                                                                    # OLD # renderer/cstyle.py            :   136
                                          elif uop is UOps.SPECIAL:                                                                                                                                # OLD # renderer/cstyle.py            :   146
                                          elif uop is UOps.DEFINE_VAR:                                                                                                                             # OLD # renderer/cstyle.py            :   149
                                          elif uop is UOps.LOAD:                                                                                                                                   # OLD # renderer/cstyle.py            :   154
                                            val = self.render_load(dtype, r[src[0]], src[0].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)                                              # renderer/cstyle.py            :   155    G: {}     L: {}

                                              def strip_parens(fst:str): return fst[1:-1] if fst[0] == '(' and fst[-1] == ')' and fst[1:-1].find('(') <= fst[1:-1].find(')') else fst                    # helpers.py                    :    37    G: {}     L: {}

                                              class CStyleLanguage(Renderer):                                                                                                                            # renderer/cstyle.py            :     9    G: {}     L: {}
                                                # returns a str expression of the loaded value with the output type                                                                                      # renderer/cstyle.py            :    59    G: {}     L: {}
                                                def render_load(self, output_dtype, buf_name, buf_dtype, idx, local=False) -> str:
                                                  if isinstance(buf_dtype, ImageDType):                                                                                                                  # renderer/cstyle.py            :    60    G: {}     L: {'output_dtype': dtypes.int, 'buf_name': 'data1', 'buf_dtype': PtrDType(dtypes.int), 'idx': 'ridx0', 'local': False}
                                                  if self.uses_vload and buf_dtype.scalar() == dtypes.float16 and output_dtype.scalar() != dtypes.float16:                                               # renderer/cstyle.py            :    63    G: {}     L: {}
                                                  if output_dtype.count > 1:                                                                                                                             # renderer/cstyle.py            :    65    G: {}     L: {}
                                                  return f"*({buf_name}+{idx})" if self.uses_ptr_arithmetic else f"{buf_name}[{idx}]"                                                                    # renderer/cstyle.py            :    67    G: {}     L: {}

                                            if len(src) > 3 and src[3].op is UOps.ALU: val = self.code_for_op[TernaryOps.WHERE](r[src[3]], val, r[src[2]], dtype)                                        # renderer/cstyle.py            :   157    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x7c92879ebd00>, 'c': defaultdict(<class 'int'>, {'ridx': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x7c9287aba5f0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), 'uop': <UOps.LOAD: 21>, 'dtype': dtypes.int, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),))), 'args': None, 'nm': 'data1', 'expr': 'ridx0', 'val': 'data1[ridx0]'}
                                            kk(f"{self.render_dtype(dtype)} {ssa('val',u)} = {val};")                                                                                                    # renderer/cstyle.py            :   158    G: {}     L: {}

                                              class CStyleLanguage(Renderer):                                                                                                                            # renderer/cstyle.py            :     9    G: {}     L: {}
                                                def render_dtype(self, var_dtype:DType) -> str:                                                                                                          # renderer/cstyle.py            :    93    G: {}     L: {}
                                                  return self.type_map.get(scalar:=var_dtype.scalar(), scalar.name) + (str(var_dtype.count) if (var_dtype.count) > 1 else "")                            # renderer/cstyle.py            :    94    G: {}     L: {'var_dtype': dtypes.int}

                                            if args in {BinaryOps.ADD,BinaryOps.MUL,BinaryOps.XOR}: operands = [strip_parens(r[v]) if v.arg == args else r[v]for v in src]                               # renderer/cstyle.py            :   138    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {', '    int val0 = data1[ridx0];'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), False)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x7c92879ebd00>, 'c': defaultdict(<class 'int'>, {'ridx': 1, 'val': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0', UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 'val0'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x7c9287aba5f0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), 'uop': <UOps.ALU: 17>, 'dtype': dtypes.int, 'src': (UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.CONST, dtypes.int, arg=2, src=())), 'args': <BinaryOps.ADD: 1>, 'nm': 'data1', 'expr': 'ridx0', 'val': 'data1[ridx0]'}
                                            val = self.code_for_op[args](*operands, dtype)                                                                                                               # renderer/cstyle.py            :   141    G: {}     L: {}
                                            assert child_count[u] != 0, f"childless ALU op found {u}"                                                                                                    # renderer/cstyle.py            :   142    G: {}     L: {}
                                            if child_count[u] <= 1 and args is not BinaryOps.MAX and not getenv("EXPAND_SSA"): r[u] = val                                                                # renderer/cstyle.py            :   144    G: {}     L: {}

                                          assert src[0].dtype is not None and src[2].dtype is not None                                                                                                   # renderer/cstyle.py            :   126    G: {}     L: {}
                                          if src[0].op is UOps.DEFINE_GLOBAL: bufs[src[0]] = (bufs[src[0]][0], (bufs[src[0]][1][0], True))                                                               # renderer/cstyle.py            :   128    G: {}     L: {}
                                          rendered_store = self.render_store(r[src[0]], src[0].dtype, r[src[2]], src[2].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)                  # renderer/cstyle.py            :   129    G: {}     L: {}

                                            class CStyleLanguage(Renderer):                                                                                                                              # renderer/cstyle.py            :     9    G: {}     L: {}
                                              # returns a str statement that does the store                                                                                                              # renderer/cstyle.py            :    81    G: {}     L: {}
                                              def render_store(self, buf_name:str, buf_dtype:DType, var_name:str, var_dtype:DType, idx:str, local=False) -> str:
                                                if isinstance(buf_dtype, ImageDType):                                                                                                                    # renderer/cstyle.py            :    82    G: {}     L: {'buf_name': 'data0', 'buf_dtype': PtrDType(dtypes.int), 'var_name': '(val0+2)', 'var_dtype': dtypes.int, 'idx': 'ridx0', 'local': False}
                                                if self.uses_vload and buf_dtype.scalar() == dtypes.float16 and var_dtype.scalar() != dtypes.float16:                                                    # renderer/cstyle.py            :    85    G: {}     L: {}
                                                if var_dtype.count > 1:                                                                                                                                  # renderer/cstyle.py            :    87    G: {}     L: {}
                                                return f"*({buf_name}+{idx}) = {var_name};" if self.uses_ptr_arithmetic else f"{buf_name}[{idx}] = {var_name};"                                          # renderer/cstyle.py            :    90    G: {}     L: {}

                                          kk(f"if ({r[src[3]]}) {{ {rendered_store} }}" if len(src) > 3 else rendered_store)                                                                             # renderer/cstyle.py            :   130    G: {}     L: {'name': 'E_3', 'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {', '    int val0 = data1[ridx0];'], 'bufs': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): ('data0', (PtrDType(dtypes.int), True)), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): ('data1', (PtrDType(dtypes.int), False))}, 'depth': 2, 'kk': <function CStyleLanguage.render.<locals>.kk at 0x7c92879ebd00>, 'c': defaultdict(<class 'int'>, {'ridx': 1, 'val': 1}), 'r': {UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 'data0', UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 'data1', UOp(UOps.CONST, dtypes.int, arg=0, src=()): '0', UOp(UOps.CONST, dtypes.int, arg=2, src=()): '2', UOp(UOps.CONST, dtypes.int, arg=3, src=()): '3', UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 'ridx0', UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 'val0', UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): '(val0+2)'}, 'ssa': <function CStyleLanguage.render.<locals>.ssa at 0x7c9287aba5f0>, 'child_count': Counter({UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)): 3, UOp(UOps.CONST, dtypes.int, arg=0, src=()): 1, UOp(UOps.CONST, dtypes.int, arg=3, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()): 1, UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)): 1, UOp(UOps.CONST, dtypes.int, arg=2, src=()): 1, UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()): 1, UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)): 1}), 'seen_vars': set(), 'u': UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), 'uop': <UOps.STORE: 22>, 'dtype': None, 'src': (UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),))), 'args': None, 'nm': 'data1', 'expr': 'ridx0', 'val': '(val0+2)', 'operands': ['val0', '2'], 'rendered_store': 'data0[ridx0] = (val0+2);'}

                                          depth -= 1                                                                                                                                                     # renderer/cstyle.py            :   123    G: {}     L: {}
                                          kk("}")                                                                                                                                                        # renderer/cstyle.py            :   124    G: {}     L: {}

                                        elif uop is UOps.STORE:                                                                                                                                    # OLD # renderer/cstyle.py            :   125
                                          assert dtype is not None, f"None dtype for uop {uop}"                                                                                                    # OLD # renderer/cstyle.py            :   132
                                          if uop is UOps.RANGE:                                                                                                                                    # OLD # renderer/cstyle.py            :   133
                                          elif uop is UOps.ALU:                                                                                                                                    # OLD # renderer/cstyle.py            :   136
                                          elif uop is UOps.SPECIAL:                                                                                                                                # OLD # renderer/cstyle.py            :   146
                                          elif uop is UOps.DEFINE_VAR:                                                                                                                             # OLD # renderer/cstyle.py            :   149
                                          elif uop is UOps.LOAD:                                                                                                                                   # OLD # renderer/cstyle.py            :   154
                                          elif uop is UOps.PHI:                                                                                                                                    # OLD # renderer/cstyle.py            :   159
                                          elif uop in {UOps.CAST, UOps.BITCAST, UOps.VECTORIZE}:                                                                                                   # OLD # renderer/cstyle.py            :   162
                                          elif uop is UOps.DEFINE_LOCAL:                                                                                                                           # OLD # renderer/cstyle.py            :   172
                                          elif uop is UOps.DEFINE_GLOBAL:                                                                                                                          # OLD # renderer/cstyle.py            :   175
                                            bufs[u] = (nm:=f"data{args}", (dtype, False))                                                                                                          # OLD # renderer/cstyle.py            :   176
                                            r[u] = nm                                                                                                                                              # OLD # renderer/cstyle.py            :   177
                                          elif uop is UOps.WMMA: kk(f"{self.render_dtype(dtype)} {ssa('wmma',u)} = __{args[0]}({r[src[0]]}, {r[src[1]]}, {r[src[2]]});")                           # OLD # renderer/cstyle.py            :   178
                                          elif uop is UOps.DEFINE_ACC: kk(f"{self.render_dtype(dtype)} {ssa('acc',u)} = {r[src[0]]};")                                                             # OLD # renderer/cstyle.py            :   179
                                          elif uop is UOps.CONST: r[u] = self.render_const(args, dtype) if args >= 0 else f"({self.render_const(args, dtype)})"                                    # OLD # renderer/cstyle.py            :   180
                                            kk(f"for (int {(expr := ssa('ridx',u))} = {r[src[0]]}; {expr} < {r[src[1]]}; {expr}++) {{")                                                            # OLD # renderer/cstyle.py            :   134
                                            depth += 1                                                                                                                                             # OLD # renderer/cstyle.py            :   135
                                            val = self.render_load(dtype, r[src[0]], src[0].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)                                        # OLD # renderer/cstyle.py            :   155
                                            if len(src) > 3 and src[3].op is UOps.ALU: val = self.code_for_op[TernaryOps.WHERE](r[src[3]], val, r[src[2]], dtype)                                  # OLD # renderer/cstyle.py            :   157
                                            kk(f"{self.render_dtype(dtype)} {ssa('val',u)} = {val};")                                                                                              # OLD # renderer/cstyle.py            :   158
                                            if args in {BinaryOps.ADD,BinaryOps.MUL,BinaryOps.XOR}: operands = [strip_parens(r[v]) if v.arg == args else r[v]for v in src]                         # OLD # renderer/cstyle.py            :   138
                                            val = self.code_for_op[args](*operands, dtype)                                                                                                         # OLD # renderer/cstyle.py            :   141
                                            assert child_count[u] != 0, f"childless ALU op found {u}"                                                                                              # OLD # renderer/cstyle.py            :   142
                                            if child_count[u] <= 1 and args is not BinaryOps.MAX and not getenv("EXPAND_SSA"): r[u] = val                                                          # OLD # renderer/cstyle.py            :   144
                                          assert src[0].dtype is not None and src[2].dtype is not None                                                                                             # OLD # renderer/cstyle.py            :   126
                                          if src[0].op is UOps.DEFINE_GLOBAL: bufs[src[0]] = (bufs[src[0]][0], (bufs[src[0]][1][0], True))                                                         # OLD # renderer/cstyle.py            :   128
                                          rendered_store = self.render_store(r[src[0]], src[0].dtype, r[src[2]], src[2].dtype, strip_parens(r[src[1]]), src[0].op is UOps.DEFINE_LOCAL)            # OLD # renderer/cstyle.py            :   129
                                          kk(f"if ({r[src[3]]}) {{ {rendered_store} }}" if len(src) > 3 else rendered_store)                                                                       # OLD # renderer/cstyle.py            :   130
                                      return self.render_kernel(name, kernel, list(bufs.values()), uops)                                                                                                 # renderer/cstyle.py            :   189    G: {}     L: {}

                                        class ClangRenderer(CStyleLanguage):                                                                                                                             # renderer/cstyle.py            :   194    G: {}     L: {}
                                          def render_kernel(self, function_name, kernel, bufs, uops, prefix=None) -> str:                                                                                # renderer/cstyle.py            :   209    G: {}     L: {}
                                            prefix = [_make_clang_dtype(self, dtype) for dtype in dedup(uop.dtype for uop in uops if uop.dtype is not None and uop.dtype.count>1)]                       # renderer/cstyle.py            :   210    G: {}     L: {'function_name': 'E_3', 'bufs': [('data0', (PtrDType(dtypes.int), True)), ('data1', (PtrDType(dtypes.int), False))], 'prefix': None, '__class__': <class 'tinygrad.renderer.cstyle.ClangRenderer'>}

                                            return super().render_kernel(function_name, kernel, bufs, uops, prefix)                                                                                      # renderer/cstyle.py            :   211    G: {}     L: {}

                                              class CStyleLanguage(Renderer):                                                                                                                            # renderer/cstyle.py            :     9    G: {}     L: {}
                                                def render_kernel(self, function_name:str, kernel:List[str], bufs:List[Tuple[str,Tuple[DType,bool]]], uops:List[UOp], prefix=None) -> str:               # renderer/cstyle.py            :    70    G: {}     L: {}
                                                  tmp = "const sampler_t smp = CLK_NORMALIZED_COORDS_FALSE | CLK_ADDRESS_CLAMP | CLK_FILTER_NEAREST;\n" if any(isinstance(dtype, ImageDType) for _,(dtype,_) in bufs) else ""  # noqa: E501 # renderer/cstyle.py            :    71    G: {}     L: {}
                                                  buftypes = [(name,f"{'write_only' if mutable else 'read_only'} image2d_t" if dtype.name.startswith('image') else                                       # renderer/cstyle.py            :    72    G: {}     L: {}
                                                              ("" if mutable else "const ")+self.buffer_prefix+self.render_dtype(dtype)+"*"+self.buffer_suffix if isinstance(dtype, PtrDType) else
                                                              self.arg_int_prefix if dtype == dtypes.int else None) for name,(dtype,mutable) in bufs]

                                                  prg = ''.join([f"{self.kernel_prefix}void {self.get_kernel_modifier(uops)}{function_name}(",] +                                                        # renderer/cstyle.py            :    75    G: {}     L: {}
                                                  [', '.join([f'{t} {name}' for name,t in buftypes] + self.extra_args)] +
                                                  [") {\n" + tmp] + ['\n'.join(kernel), "\n}"])

                                                    class CStyleLanguage(Renderer):                                                                                                                      # renderer/cstyle.py            :     9    G: {}     L: {}
                                                      def get_kernel_modifier(self, uops:List[UOp]) -> str: return ""                                                                                    # renderer/cstyle.py            :    69    G: {}     L: {}

                                                  return prg if prefix is None else "\n".join(prefix)+f"\n{prg}"                                                                                         # renderer/cstyle.py            :    78    G: {}     L: {'function_name': 'E_3', 'kernel': ['  for (int ridx0 = 0; ridx0 < 3; ridx0++) {', '    int val0 = data1[ridx0];', '    data0[ridx0] = (val0+2);', '  }'], 'bufs': [('data0', (PtrDType(dtypes.int), True)), ('data1', (PtrDType(dtypes.int), False))], 'prefix': [], 'tmp': '', 'buftypes': [('data0', 'int* restrict'), ('data1', 'const int* restrict')], 'prg': 'void E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}

                                if getenv("RUN_PROCESS_REPLAY"):                                                                                                                                         # codegen/kernel.py             :   751    G: {}     L: {'name_override': None, 'ansiname': 'E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', 'name': 'E_3', 'src': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}
                                mem_bytes = sum(max(cast(DType, x.src[0].dtype).itemsize * x.st_arg.real_size() for x in group)                                                                          # codegen/kernel.py             :   757    G: {}     L: {}
                                  for _, group in itertools.groupby([x for x in self.ast.parents if x.op in BUFFER_UOPS and x.src[0].op is UOps.DEFINE_GLOBAL],
                                                    key=lambda x: (x.op, x.src[0].arg)))

                              @dataclass(frozen=True)                                                                                                                                                    # shape/shapetracker.py         :    36    G: {}     L: {}
                              class ShapeTracker:
                                def real_size(self) -> int:                                                                                                                                              # shape/shapetracker.py         :    84    G: {}     L: {}
                                  if 0 in self.shape: return 0                                                                                                                                           # shape/shapetracker.py         :    85    G: {}     L: {'self': ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),))}

                                  idx, valid = self.expr_idxs()                                                                                                                                          # shape/shapetracker.py         :    86    G: {}     L: {}

                                    @dataclass(frozen=True)                                                                                                                                              # shape/shapetracker.py         :    36    G: {}     L: {}
                                    class ShapeTracker:
                                      def expr_idxs(self, idxs:Optional[Iterable[Node]]=None) -> Tuple[Node, Node]:                                                                                      # shape/shapetracker.py         :   122    G: {}     L: {}
                                        idxs = [Variable(f"idx{i}", 0, s-1) for i,s in enumerate(self.shape)] if idxs is None else list(idxs)                                                            # shape/shapetracker.py         :   123    G: {}     L: {'idxs': None}

                                      class Variable(Node):                                                                                                                                              # shape/symbolic.py             :   110    G: {}     L: {}
                                        def __new__(cls, *args):                                                                                                                                         # shape/symbolic.py             :   111    G: {}     L: {}
                                          expr, nmin, nmax = args                                                                                                                                        # shape/symbolic.py             :   112    G: {}     L: {'cls': <class 'tinygrad.shape.symbolic.Variable'>, 'args': ('idx0', 0, 2), '__class__': <class 'tinygrad.shape.symbolic.Variable'>}
                                          assert nmin >= 0 and nmin <= nmax, f"invalid Variable {expr=} {nmin=} {nmax=}"                                                                                 # shape/symbolic.py             :   113    G: {}     L: {}
                                          if nmin == nmax: return NumNode(nmin)                                                                                                                          # shape/symbolic.py             :   114    G: {}     L: {}
                                          return super().__new__(cls)                                                                                                                                    # shape/symbolic.py             :   115    G: {}     L: {}

                                      class Variable(Node):                                                                                                                                              # shape/symbolic.py             :   110    G: {}     L: {}
                                        def __init__(self, expr:str, nmin:int, nmax:sint):                                                                                                               # shape/symbolic.py             :   119    G: {}     L: {}
                                          self.expr, self.min, self.max = expr, nmin, nmax                                                                                                               # shape/symbolic.py             :   120    G: {}     L: repr failed
                                          self._val: Optional[int] = None                                                                                                                                # shape/symbolic.py             :   121    G: {}     L: {}

                                        idx, valid = self.views[-1].expr(idxs)                                                                                                                           # shape/shapetracker.py         :   124    G: {}     L: {}

                                          @dataclass(frozen=True)                                                                                                                                        # shape/view.py                 :    85    G: {}     L: {}
                                          class View:
                                            def expr(self, idxs:List[Node], valid:Optional[Node]=None) -> Tuple[Node, Node]:                                                                             # shape/view.py                 :   316    G: {}     L: {}
                                              assert len(idxs) == len(self.shape), f"need an idx for all dimensions {idxs} vs {self.shape}"                                                              # shape/view.py                 :   317    G: {}     L: {'idxs': [Variable('idx0', 0, 2)], 'valid': None}
                                              iexpr: List[Node] = [NumNode(self.offset) if isinstance(self.offset, int) else self.offset]                                                                # shape/view.py                 :   318    G: {}     L: {}

                                                class NumNode(Node):                                                                                                                                     # shape/symbolic.py             :   136    G: {}     L: {}
                                                  def __init__(self, num:int):                                                                                                                           # shape/symbolic.py             :   137    G: {}     L: {}
                                                    assert isinstance(num, int), f"{num} is not an int"                                                                                                  # shape/symbolic.py             :   138    G: {}     L: repr failed
                                                    self.b:int = num                                                                                                                                     # shape/symbolic.py             :   139    G: {}     L: {}
                                                    self.min, self.max = num, num                                                                                                                        # shape/symbolic.py             :   140    G: {}     L: {}

                                              vexpr: List[Node] = [valid] if valid is not None else []                                                                                                   # shape/view.py                 :   319    G: {}     L: {}
                                              for idx,sh,st,m in zip(idxs, self.shape, self.strides, self.mask if self.mask is not None else [None]*len(self.shape)):                                    # shape/view.py                 :   320    G: {}     L: {}
                                                if sh != 1 and st != 0: iexpr.append(idx*st)                                                                                                             # shape/view.py                 :   321    G: {}     L: {}

                                                  class Node:                                                                                                                                            # shape/symbolic.py             :    10    G: {}     L: {}
                                                    def __mul__(self, b:Union[Node, int]):                                                                                                               # shape/symbolic.py             :    41    G: {}     L: {}
                                                      if b == 0: return NumNode(0)                                                                                                                       # shape/symbolic.py             :    42    G: {}     L: {'self': Variable('idx0', 0, 2), 'b': 1}
                                                      if b == 1: return self                                                                                                                             # shape/symbolic.py             :    43    G: {}     L: {}

                                                if m is not None: vexpr += [create_ge_node(idx, m[0]), create_lt_node(idx, m[1])]  # idx >= m[0], idx < m[1]                                             # shape/view.py                 :   322    G: {}     L: {}
                                              return Node.sum(iexpr), Node.ands(vexpr)                                                                                                                   # shape/view.py                 :   323    G: {}     L: {}

                                                class Node:                                                                                                                                              # shape/symbolic.py             :    10    G: {}     L: {}
                                                  @staticmethod                                                                                                                                          # shape/symbolic.py             :    83    G: {}     L: {}
                                                  def sum(nodes:List[Node]) -> Node:
                                                    nodes = [x for x in nodes if x.max or x.min]                                                                                                         # shape/symbolic.py             :    84    G: {}     L: {'nodes': [NumNode(0), Variable('idx0', 0, 2)]}
                                                    if not nodes: return NumNode(0)                                                                                                                      # shape/symbolic.py             :    85    G: {}     L: {}
                                                    if len(nodes) == 1: return nodes[0]                                                                                                                  # shape/symbolic.py             :    86    G: {}     L: {}

                                                class Node:                                                                                                                                              # shape/symbolic.py             :    10    G: {}     L: {}
                                                  @staticmethod                                                                                                                                          # shape/symbolic.py             :    99    G: {}     L: {}
                                                  def ands(nodes:List[Node]) -> Node:
                                                    if not nodes: return NumNode(1)                                                                                                                      # shape/symbolic.py             :   100    G: {}     L: {'nodes': []}

                                        for view in reversed(self.views[0:-1]):                                                                                                                          # shape/shapetracker.py         :   125    G: {}     L: {}
                                        assert not isinstance(idx.min, int) or idx.min >= -2**31, f"idx.min too small. {idx=}, {idx.min=}"                                                               # shape/shapetracker.py         :   133    G: {}     L: {}
                                        assert not isinstance(idx.max, int) or idx.max < 2**31, f"idx.max too big. {idx=}, {idx.max=}"                                                                   # shape/shapetracker.py         :   134    G: {}     L: {}
                                        return idx, valid                                                                                                                                                # shape/shapetracker.py         :   135    G: {}     L: {}

                                  if not valid: return 0                                                                                                                                                 # shape/shapetracker.py         :    87    G: {}     L: {}

                                    class Node:                                                                                                                                                          # shape/symbolic.py             :    10    G: {}     L: {}
                                      def __bool__(self): return not (self.max == self.min == 0)                                                                                                         # shape/symbolic.py             :    28    G: {}     L: {}

                                  ret = idx.max                                                                                                                                                          # shape/shapetracker.py         :    89    G: {}     L: {}
                                  if not isinstance(ret, int): ret = ret.max  # might be represent by symbolic shape, take one more max for int max                                                      # shape/shapetracker.py         :    90    G: {}     L: {}
                                  assert isinstance(ret, int), f"ret must be integer, {ret=} isn't"                                                                                                      # shape/shapetracker.py         :    91    G: {}     L: {}
                                  return ret+1                                                                                                                                                           # shape/shapetracker.py         :    92    G: {}     L: {}

                                return Program(ansiname, src, self.opts.device, self.uops, mem_estimate=mem_bytes,                                                                                       # codegen/kernel.py             :   760    G: {}     L: {}
                                               global_size=[1,1,1] if self.opts.has_local else None, local_size=[1,1,1] if self.opts.has_local else None)

                                  @dataclass                                                                                                                                                             # renderer/__init__.py          :    18    G: {}     L: {}
                                  class Program:
                                    def __post_init__(self):                                                                                                                                             # renderer/__init__.py          :    33    G: {}     L: {}
                                      if not self._ran_post_init and self.uops is not None:                                                                                                              # renderer/__init__.py          :    34    G: {}     L: {'self': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[], outs=[], _ran_post_init=False)}
                                        for u in self.uops:                                                                                                                                              # renderer/__init__.py          :    36    G: {}     L: {}
                                          if u.op is UOps.DEFINE_VAR: self.vars.append(u.arg)                                                                                                            # renderer/__init__.py          :    37    G: {}     L: {}
                                          if u.op is UOps.DEFINE_GLOBAL: self.globals.append(u.arg)                                                                                                      # renderer/__init__.py          :    38    G: {}     L: {}
                                          if u.op is UOps.STORE: self.outs.extend([x.arg for x in u.src[0].sparents if x.op is UOps.DEFINE_GLOBAL])                                                      # renderer/__init__.py          :    39    G: {}     L: {}
                                          if u.op is UOps.SPECIAL:                                                                                                                                       # renderer/__init__.py          :    40    G: {}     L: {}

                                        self.vars = sorted(self.vars, key=lambda v: v.expr)                                                                                                              # renderer/__init__.py          :    49    G: {}     L: {}
                                        self.outs = sorted(dedup(self.outs))                                                                                                                             # renderer/__init__.py          :    50    G: {}     L: {}

                                        self._ran_post_init = True                                                                                                                                       # renderer/__init__.py          :    51    G: {}     L: {}

                          if getenv("FUZZ_UOPS"):                                                                                                                                                        # engine/realize.py             :   158    G: {}     L: {'dname': 'CLANG', 'ckey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, False), 'cret': None, 'bkey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, True), 'bret': None, 'prg': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True)}

                          method_cache[ckey] = method_cache[bkey] = ret = CompiledRunner(replace(prg, dname=dname))                                                                                      # engine/realize.py             :   161    G: {}     L: {}

                            class CompiledRunner(Runner):                                                                                                                                                # engine/realize.py             :    79    G: {}     L: {}
                              def __init__(self, p:Program, precompiled:Optional[bytes]=None):                                                                                                           # engine/realize.py             :    80    G: {}     L: {}
                                if DEBUG >= 4: print(p.src)                                                                                                                                              # engine/realize.py             :    81    G: {}     L: {'self': <tinygrad.engine.realize.CompiledRunner object at 0x7c9287ab37c0>, 'p': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True), 'precompiled': None, '__class__': <class 'tinygrad.engine.realize.CompiledRunner'>}

                                self.p:Program = p                                                                                                                                                       # engine/realize.py             :    82    G: {}     L: {}
                                self.lib:bytes = precompiled if precompiled is not None else Device[p.dname].compiler.compile_cached(p.src)                                                              # engine/realize.py             :    83    G: {}     L: {}

                                  class Compiler:                                                                                                                                                        # device.py                     :   176    G: {}     L: {}
                                    def compile_cached(self, src:str) -> bytes:                                                                                                                          # device.py                     :   179    G: {}     L: {}
                                      if self.cachekey is None or (lib := diskcache_get(self.cachekey, src)) is None:                                                                                    # device.py                     :   180    G: {}     L: {'self': <tinygrad.runtime.ops_clang.ClangCompiler object at 0x7c9287b4ac20>, 'src': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}

                                        def diskcache_get(table:str, key:Union[Dict, str, int]) -> Any:                                                                                                  # helpers.py                    :   227    G: {}     L: {}
                                          if CACHELEVEL == 0: return None                                                                                                                                # helpers.py                    :   228    G: {}     L: {'table': 'compile_clang', 'key': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}
                                          if isinstance(key, (str,int)): key = {"key": key}                                                                                                              # helpers.py                    :   229    G: {}     L: {}
                                          conn = db_connection()                                                                                                                                         # helpers.py                    :   230    G: {}     L: {}

                                            def db_connection():                                                                                                                                         # helpers.py                    :   211    G: {}     L: {}
                                              if _db_connection is None:                                                                                                                                 # helpers.py                    :   213    G: {}     L: {}
                                                os.makedirs(CACHEDB.rsplit(os.sep, 1)[0], exist_ok=True)                                                                                                 # helpers.py                    :   214    G: {}     L: {}
                                                _db_connection = sqlite3.connect(CACHEDB, timeout=60, isolation_level="IMMEDIATE")                                                                       # helpers.py                    :   215    G: {}     L: {}
                                                with contextlib.suppress(sqlite3.OperationalError): _db_connection.execute("PRAGMA journal_mode=WAL").fetchone()                                         # helpers.py                    :   218    G: {}     L: {}
                                                if DEBUG >= 7: _db_connection.set_trace_callback(print)                                                                                                  # helpers.py                    :   219    G: {}     L: {}

                                              return _db_connection                                                                                                                                      # helpers.py                    :   220    G: {}     L: {}

                                          cur = conn.cursor()                                                                                                                                            # helpers.py                    :   231    G: {}     L: {'table': 'compile_clang', 'key': {'key': '\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}'}, 'conn': <sqlite3.Connection object at 0x7c9299735b40>}
                                          try:                                                                                                                                                           # helpers.py                    :   232    G: {}     L: {}
                                            res = cur.execute(f"SELECT val FROM '{table}_{VERSION}' WHERE {' AND '.join([f'{x}=?' for x in key.keys()])}", tuple(key.values()))                          # helpers.py                    :   233    G: {}     L: {}
                                          if (val:=res.fetchone()) is not None: return pickle.loads(val[0])                                                                                              # helpers.py                    :   236    G: {}     L: {}

                                      return lib                                                                                                                                                         # device.py                     :   184    G: {}     L: {}

                                self.clprg = Device[p.dname].runtime(p.function_name, self.lib)                                                                                                          # engine/realize.py             :    84    G: {}     L: {}

                                  @dataclass                                                                                                                                                             # renderer/__init__.py          :    18    G: {}     L: {}
                                  class Program:
                                    @functools.cached_property                                                                                                                                           # renderer/__init__.py          :    64    G: {}     L: {}
                                    def function_name(self) -> str: return to_function_name(self.name)

                                  class ClangProgram:                                                                                                                                                    # runtime/ops_clang.py          :    14    G: {}     L: {}
                                    def __init__(self, name:str, lib:bytes):                                                                                                                             # runtime/ops_clang.py          :    15    G: {}     L: {}
                                      if DEBUG >= 6: cpu_objdump(lib)                                                                                                                                    # runtime/ops_clang.py          :    16    G: {}     L: {'self': <tinygrad.runtime.ops_clang.ClangProgram object at 0x7c9287aa3d60>, 'name': 'E_3', 'lib': b"\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00>\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x00 1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00@\x008\x00\x08\x00@\x00\x0c\x00\x0b\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x85\x02\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x05\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x06\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x04\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00Q\xe5td\x06\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00R\xe5td\x04\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00GNU\x00\xa7K\x95\xab\x90\x99\x8dZUPD\x8f\xce\x1ak\xe0\xf0\xf1\xe7\x99\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x08\x10\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\xdd\xe4\x87\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00E_3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x8b\x06\x83\xc0\x02\x89\x07\x8bF\x04\x83\xc0\x02\x89G\x04\x8bF\x08\x83\xc0\x02\x89G\x08\xc3\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xf5\xfe\xffo\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x0b\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Ubuntu clang version 14.0.0-1ubuntu1.1\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\xf1\xff\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x01\x00\x07\x00P/\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x0c\x00\x00\x00\x12\x00\x05\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00-\x00_DYNAMIC\x00E_3\x00\x00.symtab\x00.strtab\x00.shstrtab\x00.note.gnu.build-id\x00.gnu.hash\x00.dynsym\x00.dynstr\x00.text\x00.eh_frame\x00.dynamic\x00.comment\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x1b\x00\x00\x00\x07\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00.\x00\x00\x00\xf6\xff\xffo\x02\x00\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00(\x02\x00\x00\x00\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x008\x00\x00\x00\x0b\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x00P\x02\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x03\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x80\x02\x00\x00\x00\x00\x00\x00\x05\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00H\x00\x00\x00\x01\x00\x00\x00\x06\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00N\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00 \x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00X\x00\x00\x00\x06\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00P/\x00\x00\x00\x00\x00\x00\xb0\x00\x00\x00\x00\x00\x00\x00\x04\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00a\x00\x00\x00\x01\x00\x00\x000\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x000\x00\x00\x00\x00\x00\x00'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00(0\x00\x00\x00\x00\x00\x00x\x00\x00\x00\x00\x00\x00\x00\n\x00\x00\x00\x04\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00\x18\x00\x00\x00\x00\x00\x00\x00\t\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xa00\x00\x00\x00\x00\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x11\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\xb00\x00\x00\x00\x00\x00\x00j\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"}

                                      self.name, self.lib = name, lib                                                                                                                                    # runtime/ops_clang.py          :    17    G: {}     L: {}
                                      with tempfile.NamedTemporaryFile(delete=True) as cached_file_path:                                                                                                 # runtime/ops_clang.py          :    19    G: {}     L: {}
                                        pathlib.Path(cached_file_path.name).write_bytes(lib)                                                                                                             # runtime/ops_clang.py          :    20    G: {}     L: {}
                                        self.fxn = ctypes.CDLL(str(cached_file_path.name))[name]                                                                                                         # runtime/ops_clang.py          :    21    G: {}     L: {}

                                super().__init__(p.name, p.dname, p.op_estimate, p.mem_estimate, p.lds_estimate)                                                                                         # engine/realize.py             :    85    G: {}     L: {}

                                  @dataclass                                                                                                                                                             # renderer/__init__.py          :    18    G: {}     L: {}
                                  class Program:
                                    @property                                                                                                                                                            # renderer/__init__.py          :    54    G: {}     L: {}
                                    def op_estimate(self) -> sint: return self._ops_lds[0]

                                      @dataclass                                                                                                                                                         # renderer/__init__.py          :    18    G: {}     L: {}
                                      class Program:
                                        @functools.cached_property                                                                                                                                       # renderer/__init__.py          :    58    G: {}     L: {}
                                        def _ops_lds(self) -> Tuple[sint, sint]: return (0,0) if self.uops is None else flops_mem(self.uops, ignore_indexing=True)

                                          def flops_mem(uops:List[UOp], ignore_indexing=False) -> Tuple[sint, sint]:                                                                                     # ops.py                        :   346    G: {}     L: {}
                                            flops: sint = 0                                                                                                                                              # ops.py                        :   347    G: {}     L: {'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'ignore_indexing': True}
                                            mem: sint = 0                                                                                                                                                # ops.py                        :   348    G: {}     L: {}
                                            mults: sint = 1                                                                                                                                              # ops.py                        :   349    G: {}     L: {}
                                            mult_stack: List[sint] = []                                                                                                                                  # ops.py                        :   350    G: {}     L: {}
                                            dont_count: Set[UOp] = set()                                                                                                                                 # ops.py                        :   351    G: {}     L: {}
                                            if ignore_indexing:                                                                                                                                          # ops.py                        :   352    G: {}     L: {}
                                              for u in uops:                                                                                                                                             # ops.py                        :   353    G: {}     L: {}
                                                if u.op is UOps.LOAD:                                                                                                                                    # ops.py                        :   354    G: {}     L: {}
                                                elif u.op is UOps.STORE:                                                                                                                                 # ops.py                        :   357    G: {}     L: {}
                                                elif u.op is UOps.IF:                                                                                                                                    # ops.py                        :   360    G: {}     L: {}
                                                  dont_count = dont_count.union(u.src[1].sparents)                                                                                                       # ops.py                        :   355    G: {}     L: {}

                                                  if len(u.src) > 3: dont_count = dont_count.union(u.src[2].sparents)                                                                                    # ops.py                        :   356    G: {}     L: {}
                                                elif u.op is UOps.STORE:                                                                                                                           # OLD # ops.py                        :   357
                                                  dont_count = dont_count.union(u.src[1].sparents)                                                                                                       # ops.py                        :   358    G: {}     L: {}

                                                  if len(u.src) > 3: dont_count = dont_count.union(u.src[3].sparents)                                                                                    # ops.py                        :   359    G: {}     L: {}
                                                elif u.op is UOps.IF:                                                                                                                              # OLD # ops.py                        :   360
                                            for u in uops:                                                                                                                                               # ops.py                        :   362    G: {}     L: {}
                                              if u.op is UOps.RANGE:                                                                                                                                     # ops.py                        :   363    G: {}     L: {}
                                              elif u.op is UOps.ENDRANGE:                                                                                                                                # ops.py                        :   366    G: {}     L: {}
                                              elif u.op is UOps.SPECIAL:                                                                                                                                 # ops.py                        :   368    G: {}     L: {}
                                              elif u.op is UOps.LOAD:                                                                                                                                    # ops.py                        :   370    G: {}     L: {}
                                              elif u.op is UOps.STORE:                                                                                                                                   # ops.py                        :   373    G: {}     L: {}
                                              elif u.op is UOps.ALU and u not in dont_count:                                                                                                             # ops.py                        :   376    G: {}     L: {}
                                              elif u.op is UOps.WMMA and u not in dont_count:                                                                                                            # ops.py                        :   379    G: {}     L: {}
                                                mult_stack.append(mults)                                                                                                                                 # ops.py                        :   364    G: {}     L: {}
                                                mults *= uop_alu_resolve(u.src[1] - u.src[0])                                                                                                            # ops.py                        :   365    G: {}     L: {}

                                                  def uop_alu_resolve(u:UOp) -> sint:                                                                                                                    # ops.py                        :   336    G: {}     L: {}
                                                    if u.op in {UOps.CONST, UOps.DEFINE_VAR}: return u.arg                                                                                               # ops.py                        :   337    G: {}     L: {'u': UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),\n  UOp(UOps.ALU, dtypes.int, arg=UnaryOps.NEG, src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),)),))}
                                                    if u.op is UOps.ALU: return exec_alu(u.arg, cast(DType,u.dtype), tuple(map(uop_alu_resolve, u.src)))                                                 # ops.py                        :   338    G: {}     L: {}

                                                    def exec_alu(op:Op, dtype:DType, operands): return truncate.get(dtype, lambda x: x)(python_alu[op](*operands))                                       # ops.py                        :    76    G: {}     L: {}

                                              elif u.op is UOps.ENDRANGE:                                                                                                                          # OLD # ops.py                        :   366
                                              elif u.op is UOps.SPECIAL:                                                                                                                           # OLD # ops.py                        :   368
                                              elif u.op is UOps.LOAD:                                                                                                                              # OLD # ops.py                        :   370
                                                assert u.dtype is not None                                                                                                                               # ops.py                        :   371    G: {}     L: {'uops': [UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], 'ignore_indexing': True, 'flops': 0, 'mem': 0, 'mults': 3, 'mult_stack': [1], 'dont_count': {UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.CONST, dtypes.int, arg=0, src=())}, 'u': UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))}
                                                mem += u.dtype.itemsize * mults                                                                                                                          # ops.py                        :   372    G: {}     L: {}
                                              elif u.op is UOps.STORE:                                                                                                                             # OLD # ops.py                        :   373
                                              elif u.op is UOps.ALU and u not in dont_count:                                                                                                       # OLD # ops.py                        :   376
                                                assert u.dtype is not None                                                                                                                               # ops.py                        :   377    G: {}     L: {}
                                                flops += (mults * (2 if u.arg == TernaryOps.MULACC else 1)) * u.dtype.count                                                                              # ops.py                        :   378    G: {}     L: {}
                                                assert u.src[2].dtype is not None                                                                                                                        # ops.py                        :   374    G: {}     L: {}
                                                mem += u.src[2].dtype.itemsize * mults                                                                                                                   # ops.py                        :   375    G: {}     L: {}
                                                mults = mult_stack.pop(-1)                                                                                                                               # ops.py                        :   367    G: {}     L: {}
                                              elif u.op is UOps.SPECIAL:                                                                                                                           # OLD # ops.py                        :   368
                                              elif u.op is UOps.LOAD:                                                                                                                              # OLD # ops.py                        :   370
                                              elif u.op is UOps.STORE:                                                                                                                             # OLD # ops.py                        :   373
                                              elif u.op is UOps.ALU and u not in dont_count:                                                                                                       # OLD # ops.py                        :   376
                                              elif u.op is UOps.WMMA and u not in dont_count:                                                                                                      # OLD # ops.py                        :   379
                                                assert u.dtype is not None                                                                                                                         # OLD # ops.py                        :   371
                                                mem += u.dtype.itemsize * mults                                                                                                                    # OLD # ops.py                        :   372
                                                assert u.dtype is not None                                                                                                                         # OLD # ops.py                        :   377
                                                flops += (mults * (2 if u.arg == TernaryOps.MULACC else 1)) * u.dtype.count                                                                        # OLD # ops.py                        :   378
                                                assert u.src[2].dtype is not None                                                                                                                  # OLD # ops.py                        :   374
                                                mem += u.src[2].dtype.itemsize * mults                                                                                                             # OLD # ops.py                        :   375
                                            return flops, mem                                                                                                                                            # ops.py                        :   382    G: {}     L: {}

                                  @dataclass                                                                                                                                                             # renderer/__init__.py          :    18    G: {}     L: {}
                                  class Program:
                                    @property                                                                                                                                                            # renderer/__init__.py          :    56    G: {}     L: {}
                                    def lds_estimate(self) -> sint: return self._ops_lds[1]

                        return ret                                                                                                                                                                       # engine/realize.py             :   162    G: {}     L: {'dname': 'CLANG', 'ast': UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), 'ckey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, False), 'cret': None, 'bkey': ('CLANG', b'\x9e\xfaL\xce5hP#\xe3\xf00\xa3i<xW\xf9g\x13RUqt\n-QM\xe3\x90\xaa\xf7m', 0, 0, True), 'bret': None, 'prg': Program(name='E_\x1b[34m3\x1b[0m\x1b[90m\x1b[0m', src='\nvoid E_3(int* restrict data0, const int* restrict data1) {\n  for (int ridx0 = 0; ridx0 < 3; ridx0++) {\n    int val0 = data1[ridx0];\n    data0[ridx0] = (val0+2);\n  }\n}', dname='CLANG', uops=[UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()), UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()), UOp(UOps.CONST, dtypes.int, arg=0, src=()), UOp(UOps.CONST, dtypes.int, arg=2, src=()), UOp(UOps.CONST, dtypes.int, arg=3, src=()), UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n  UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n  UOp(UOps.CONST, dtypes.int, arg=3, src=()),)), UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)), UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n  UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n    UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n      UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n      UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),)),\n  UOp(UOps.CONST, dtypes.int, arg=2, src=()),)), UOp(UOps.STORE, None, arg=None, src=(\n  UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n  x1:=UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),\n  UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n    UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n      UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n       x1,)),\n    UOp(UOps.CONST, dtypes.int, arg=2, src=()),)),)), UOp(UOps.ENDRANGE, None, arg=None, src=(\n  UOp(UOps.RANGE, dtypes.int, arg=(0, False), src=(\n    UOp(UOps.CONST, dtypes.int, arg=0, src=()),\n    UOp(UOps.CONST, dtypes.int, arg=3, src=()),)),))], mem_estimate=24, global_size=None, local_size=None, vars=[], globals=[0, 1], outs=[0], _ran_post_init=True), 'ret': <tinygrad.engine.realize.CompiledRunner object at 0x7c9287ab37c0>}

                    return ExecItem(runner, [si.bufs[x] for x in runner.p.globals], si.metadata)                                                                                                         # engine/realize.py             :   193    G: {}     L: {'si': ScheduleItem(ast=UOp(UOps.SINK, None, arg=None, src=(\n  UOp(UOps.STORE, None, arg=None, src=(\n    UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=0, src=()),\n    UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),\n    UOp(UOps.ALU, dtypes.int, arg=BinaryOps.ADD, src=(\n      UOp(UOps.LOAD, dtypes.int, arg=None, src=(\n        UOp(UOps.DEFINE_GLOBAL, PtrDType(dtypes.int), arg=1, src=()),\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(1,), offset=0, mask=None, contiguous=True),)), src=()),)),\n      UOp(UOps.CONST, dtypes.int, arg=2, src=(\n        UOp(UOps.SHAPETRACKER, None, arg=ShapeTracker(views=(View(shape=(3,), strides=(0,), offset=0, mask=None, contiguous=False),)), src=()),)),)),)),)), bufs=(<buf real:False device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>), metadata=[__add__]), 'runner': <tinygrad.engine.realize.CompiledRunner object at 0x7c9287ab37c0>}

                class CompiledRunner(Runner):                                                                                                                                                            # engine/realize.py             :    79    G: {}     L: {}
                  def __call__(self, rawbufs:List[Buffer], var_vals:Dict[Variable, int], wait=False) -> Optional[float]:                                                                                 # engine/realize.py             :    89    G: {}     L: {}
                    global_size, local_size = self.p.launch_dims(var_vals)                                                                                                                               # engine/realize.py             :    90    G: {}     L: {'self': <tinygrad.engine.realize.CompiledRunner object at 0x7c9287ab37c0>, 'rawbufs': [<buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>], 'wait': False, 'var_vals': {}}

                      @dataclass                                                                                                                                                                         # renderer/__init__.py          :    18    G: {}     L: {}
                      class Program:
                        def launch_dims(self, var_vals:Dict[Variable, int]):                                                                                                                             # renderer/__init__.py          :    66    G: {}     L: {}
                          global_size = [sym_infer(sz, var_vals) for sz in self.global_size] if self.global_size is not None else None                                                                   # renderer/__init__.py          :    67    G: {}     L: {'var_vals': {}}
                          local_size = [sym_infer(sz, var_vals) for sz in self.local_size] if self.local_size is not None else None                                                                      # renderer/__init__.py          :    68    G: {}     L: {}
                          return global_size, local_size                                                                                                                                                 # renderer/__init__.py          :    69    G: {}     L: {}

                    if global_size is not None and local_size is None and all_int(self.p.global_size): # type: ignore[arg-type]                                                                          # engine/realize.py             :    91    G: {}     L: {}
                    lra = {}                                                                                                                                                                             # engine/realize.py             :    97    G: {}     L: {}
                    if global_size:                                                                                                                                                                      # engine/realize.py             :    98    G: {}     L: {}
                    if local_size:                                                                                                                                                                       # engine/realize.py             :   101    G: {}     L: {}
                    return self.clprg(*[x._buf for x in rawbufs], **lra, vals=tuple(var_vals[k] for k in self.p.vars), wait=wait)                                                                        # engine/realize.py             :   104    G: {}     L: {}

                      class ClangProgram:                                                                                                                                                                # runtime/ops_clang.py          :    14    G: {}     L: {}
                        def __call__(self, *bufs, vals=(), wait=False): return cpu_time_execution(lambda: self.fxn(*bufs, *vals), enable=wait)                                                           # runtime/ops_clang.py          :    23    G: {}     L: {}

                          def cpu_time_execution(cb, enable):                                                                                                                                            # helpers.py                    :   285    G: {}     L: {}
                            if enable: st = time.perf_counter()                                                                                                                                          # helpers.py                    :   286    G: {}     L: {'cb': <function ClangProgram.__call__.<locals>.<lambda> at 0x7c92879e9240>, 'enable': False}
                            cb()                                                                                                                                                                         # helpers.py                    :   287    G: {}     L: {}

                            if enable: return time.perf_counter()-st                                                                                                                                     # helpers.py                    :   288    G: {}     L: {}

            return self                                                                                                                                                                                  # tensor.py                     :   205    G: {}     L: {'do_update_stats': True}

          buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)                                                                                                                               # tensor.py                     :   246    G: {}     L: {'self': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>)> on CLANG with grad None>, 'cpu': <Tensor <LB CLANG (3,) int (<BinaryOps.ADD: 1>, <buf real:True device:CLANG size:3 dtype:dtypes.int offset:0>)> on CLANG with grad None>}

          if self.device != "CLANG": buf.options = BufferOptions(nolru=True)                                                                                                                             # tensor.py                     :   247    G: {}     L: {}

          return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)                                                                                                                # tensor.py                     :   248    G: {}     L: {}

        class _MallocAllocator(LRUAllocator):                                                                                                                                                            # device.py                     :   163    G: {}     L: {}
          def copyout(self, dest:memoryview, src): ctypes.memmove(from_mv(dest), src, len(dest))                                                                                                         # device.py                     :   167    G: {}     L: {}

