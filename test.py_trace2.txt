# test.py
# __init__.py
# tensor.py
# dtype.py
# helpers.py
if TYPE_CHECKING:  # TODO: remove this and import TypeGuard from typing once minimum python supported version is 3.10
T = TypeVar("T")
U = TypeVar("U")
OSX = platform.system() == "Darwin"
CI = os.getenv("CI", "") != ""

  class Context(contextlib.ContextDecorator):
    stack: ClassVar[List[dict[str, int]]] = [{}]

  class ContextVar:
    _cache: ClassVar[Dict[str, ContextVar]] = {}
    value: int
    key: str

DEBUG, IMAGE, BEAM, NOOPT, JIT = ContextVar("DEBUG", 0), ContextVar("IMAGE", 0), ContextVar("BEAM", 0), ContextVar("NOOPT", 0), ContextVar("JIT", 1)

  class ContextVar:
    def __new__(cls, key, default_value):
      if key in ContextVar._cache: return ContextVar._cache[key]
      instance = ContextVar._cache[key] = super().__new__(cls)
      instance.value, instance.key = getenv(key, default_value), key

  class ContextVar:
    def __new__(cls, key, default_value):
      return instance

WINO, THREEFRY, CAPTURING, TRACEMETA = ContextVar("WINO", 0), ContextVar("THREEFRY", 0), ContextVar("CAPTURING", 1), ContextVar("TRACEMETA", 1)
GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING = ContextVar("GRAPH", 0), getenv("GRAPHPATH", "/tmp/net"), ContextVar("SAVE_SCHEDULE", 0), ContextVar("RING", 1)
MULTIOUTPUT, PROFILE, TRANSCENDENTAL = ContextVar("MULTIOUTPUT", 1), ContextVar("PROFILE", 0), ContextVar("TRANSCENDENTAL", 1)
USE_TC, TC_OPT = ContextVar("TC", 1), ContextVar("TC_OPT", 0)
FUSE_AS_ONE_KERNEL = ContextVar("FUSE_AS_ONE_KERNEL", 0)

  @dataclass(frozen=True)
  class Metadata:
    name: str
    caller: str
    backward: bool = False

_METADATA: contextvars.ContextVar[Optional[Metadata]] = contextvars.ContextVar("_METADATA", default=None)

  class GlobalCounters:
    global_ops: ClassVar[int] = 0
    global_mem: ClassVar[int] = 0
    time_sum_s: ClassVar[float] = 0.0
    kernel_count: ClassVar[int] = 0
    mem_used: ClassVar[int] = 0   # NOTE: this is not reset

  class ProfileLogger:
    writers: int = 0
    mjson: List[Dict] = []
    actors: Dict[str, int] = {}
    subactors: Dict[Tuple[str, str], int] = {}
    path = getenv("PROFILE_OUTPUT_FILE", temp("tinygrad_profile.json"))

_cache_dir: str = getenv("XDG_CACHE_HOME", os.path.expanduser("~/Library/Caches" if OSX else "~/.cache"))
CACHEDB: str = getenv("CACHEDB", os.path.abspath(os.path.join(_cache_dir, "tinygrad", "cache.db")))
CACHELEVEL = getenv("CACHELEVEL", 2)
VERSION = 16
_db_connection = None
_db_tables = set()
# dtype.py
ConstType = Union[float, int, bool]

  @dataclass(frozen=True, order=True)
  class DType:
    priority: int  # this determines when things get upcasted
    itemsize: int
    name: str
    fmt: Optional[str]
    count: int

  # dependent typing?
  @dataclass(frozen=True, repr=False)
  class ImageDType(DType):
    shape: Tuple[int, ...]   # arbitrary arg for the dtype, used in image for the shape
    base: DType

  class dtypes:
    bigint: Final[DType] = DType(-1, 0, "bigint", None, 1)   # arbitrary precision integer
    bool: Final[DType] = DType(0, 1, "bool", '?', 1)
    int8: Final[DType] = DType(1, 1, "char", 'b', 1)
    uint8: Final[DType] = DType(2, 1, "unsigned char", 'B', 1)
    int16: Final[DType] = DType(3, 2, "short", 'h', 1)
    uint16: Final[DType] = DType(4, 2, "unsigned short", 'H', 1)
    int32: Final[DType] = DType(5, 4, "int", 'i', 1)
    uint32: Final[DType] = DType(6, 4, "unsigned int", 'I', 1)
    int64: Final[DType] = DType(7, 8, "long", 'l', 1)
    uint64: Final[DType] = DType(8, 8, "unsigned long", 'L', 1)
    float16: Final[DType] = DType(9, 2, "half", 'e', 1)
    bfloat16: Final[DType] = DType(10, 2, "__bf16", None, 1)
    float32: Final[DType] = DType(11, 4, "float", 'f', 1)
    float64: Final[DType] = DType(12, 8, "double", 'd', 1)
    half = float16; float = float32; double = float64 # noqa: E702
    uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 # noqa: E702
    char = int8; short = int16; int = int32; long = int64 # noqa: E702
    default_float: ClassVar[DType] = float32
    default_int: ClassVar[DType] = int32

if (env_default_float := getenv("DEFAULT_FLOAT", "")):
# helpers.py
# dtype.py
promo_lattice = { dtypes.bool: [dtypes.int8, dtypes.uint8], dtypes.int8: [dtypes.int16], dtypes.int16: [dtypes.int32], dtypes.int32: [dtypes.int64],
  dtypes.int64: [dtypes.float16, dtypes.bfloat16], dtypes.uint8: [dtypes.int16, dtypes.uint16], dtypes.uint16: [dtypes.int32, dtypes.uint32],
  dtypes.uint32: [dtypes.int64, dtypes.uint64], dtypes.uint64: [dtypes.float16, dtypes.bfloat16],
  dtypes.float16: [dtypes.float32], dtypes.bfloat16: [dtypes.float32], dtypes.float32: [dtypes.float64], }
DTYPES_DICT = {k: v for k, v in dtypes.__dict__.items() if not (k.startswith(('__', 'default', 'bigint')) or v.__class__ is staticmethod)}
INVERSE_DTYPES_DICT = {v.name:k for k,v in DTYPES_DICT.items()}
INVERSE_DTYPES_DICT['bigint'] = 'bigint'
# tensor.py
# lazy.py
# ops.py
# shape/symbolic.py

  class Node:
    b: Union[Node, int]
    min: int
    max: sint

sint = Union[int, Variable, MulNode, SumNode]
  Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
    else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
    else f"{self.expr}"),
  NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
  MulNode: render_mulnode,
  DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
  ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
  LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
  SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
  AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
render_python: Dict[Type, Callable[..., str]] = {
  Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
    else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
    else f"{self.expr}"),
  NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
  MulNode: render_mulnode,
  DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
  ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
  LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
  SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
  AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
}
# ops.py
# shape/shapetracker.py
# shape/view.py

  @dataclass(frozen=True)
  class View:
    shape:Tuple[sint, ...]
    strides:Tuple[sint, ...]
    offset:sint
    mask:Optional[Tuple[Tuple[sint, sint], ...]]
    contiguous:bool

# shape/shapetracker.py

  @dataclass(frozen=True)
  class ShapeTracker:
    views: Tuple[View, ...]

# ops.py

  # these are the llops your accelerator must implement, along with toCpu
  # the Enum class doesn't work with mypy, this is static. sorry it's ugly
  # NOTE: MOD, CMPLT don't have to be implemented on vectors, just scalars
  # NOTE: many GPUs don't have DIV, but UnaryOps.RECIP doesn't work for integer division
  class UnaryOps(Enum):
    """A -> A (elementwise)"""
    EXP2 = auto(); LOG2 = auto(); CAST = auto(); BITCAST = auto(); SIN = auto(); SQRT = auto(); NEG = auto(); RECIP = auto() # noqa: E702

  class BinaryOps(Enum):
    """A + A -> A (elementwise)"""
    ADD = auto(); MUL = auto(); IDIV = auto(); MAX = auto(); MOD = auto(); CMPLT = auto(); CMPNE = auto(); XOR = auto() # noqa: E702
    SHL = auto(); SHR = auto(); OR = auto(); AND = auto(); THREEFRY = auto() # noqa: E702

  class TernaryOps(Enum):
    """A + A + A -> A (elementwise)"""
    WHERE = auto(); MULACC = auto() # noqa: E702

  class ReduceOps(Enum):
    """A -> B (reduce)"""
    SUM = auto(); MAX = auto(); WMMA = auto() # noqa: E702

  class MetaOps(Enum):
    EMPTY = auto(); CONST = auto(); COPY = auto(); CONTIGUOUS = auto(); CUSTOM = auto(); ASSIGN = auto(); VIEW = auto(); KERNEL = auto() # noqa: E702

Op = Union[UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps, BufferOps]
UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}

  @dataclass(frozen=True)
  class MemBuffer:
    idx: int
    dtype: DType
    st: ShapeTracker

  @dataclass(frozen=True)
  class ConstBuffer:
    val: ConstType | Variable
    dtype: DType
    st: ShapeTracker

  @dataclass(frozen=True)
  class KernelInfo:
    local_dims: int = 0           # number of local dimensions  (this is remapping RANGE to SPECIAL)
    upcasted: int = 0             # count that are upcasted     (this is remapping RANGE to EXPAND)
    dont_use_locals: bool = False # don't use local indexing

  @dataclass(frozen=True, eq=False)
  class LazyOp:
    op: Op
    src: Tuple[LazyOp, ...] = ()
    arg: Any = None

python_alu: Dict[Op, Callable]  = {
  UnaryOps.LOG2: lambda x: math.log2(x) if x > 0 else -math.inf if x == 0 else math.nan,
  UnaryOps.EXP2: hook_overflow(math.inf, lambda x: 2**x),
  UnaryOps.SQRT: lambda x: math.sqrt(x) if x >= 0 else math.nan,
  UnaryOps.SIN: lambda x: math.sin(x) if not math.isinf(x) else math.nan,
  UnaryOps.RECIP: lambda x: 1/x if x != 0 else math.copysign(math.inf, x),
  UnaryOps.NEG: lambda x: (not x) if isinstance(x, bool) else -x,
  BinaryOps.SHR: operator.rshift, BinaryOps.SHL: operator.lshift,
  BinaryOps.MUL: operator.mul, BinaryOps.ADD: operator.add,
  BinaryOps.XOR: operator.xor, BinaryOps.MAX: max, BinaryOps.CMPNE: operator.ne, BinaryOps.CMPLT: operator.lt,
  BinaryOps.OR: operator.or_, BinaryOps.AND: operator.and_,
  BinaryOps.MOD: lambda x,y: abs(int(x))%abs(int(y))*(1,-1)[x<0], BinaryOps.IDIV: lambda x, y: int(x/y) if y != 0 else x*math.inf,
  TernaryOps.MULACC: lambda x,y,z: (x*y)+z,
  TernaryOps.WHERE: lambda x,y,z: y if x else z}

  def hook_overflow(dv, fxn):
    return wfxn

truncate: Dict[DType, Callable] = {dtypes.bool: bool,
  # TODO: bfloat16
  dtypes.float16: truncate_fp16, dtypes.float32: lambda x: ctypes.c_float(x).value, dtypes.float64: lambda x: ctypes.c_double(x).value,
  dtypes.uint8: lambda x: ctypes.c_uint8(x).value, dtypes.uint16: lambda x: ctypes.c_uint16(x).value,
  dtypes.uint32: lambda x: ctypes.c_uint32(x).value, dtypes.uint64: lambda x: ctypes.c_uint64(x).value,
  dtypes.int8: lambda x: ctypes.c_int8(x).value, dtypes.int16: lambda x: ctypes.c_int16(x).value, dtypes.int32: lambda x: ctypes.c_int32(x).value \
      if isinstance(x,int) else x, dtypes.int64: lambda x: ctypes.c_int64(x).value, dtypes.bigint: lambda x: x }
# lazy.py
# device.py
# renderer/__init__.py
# codegen/uopgraph.py
# codegen/uops.py

  # the order of these UOps controls the order of the toposort
  class UOps(Enum):
    SINK = auto(); VAR = auto(); EXPAND = auto(); CONTRACT = auto() # noqa: E702
    DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702
    CONST = auto(); SPECIAL = auto() # noqa: E702
    NOOP = auto(); UNMUL = auto(); GEP = auto() # noqa: E702
    CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702
    ALU = auto(); REDUCE = auto(); WMMA = auto() # noqa: E702
    LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702
    BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702
    ENDRANGE = auto(); ENDIF = auto() # noqa: E702

END_FOR_UOP = {UOps.IF:(UOps.STORE, UOps.ENDIF), UOps.RANGE:(UOps.PHI, UOps.ENDRANGE)}

  @dataclass(frozen=True, eq=False)
  class UOp:
    op: UOps
    dtype: Optional[DType] = None
    src: Tuple[UOp, ...] = tuple()
    arg: Any = None

# codegen/uopgraph.py
# codegen/transcendental.py
TRANSCENDENTAL_SUPPORTED_DTYPES = {dtypes.float16, dtypes.float32, dtypes.float64}
# codegen/uopgraph.py
if TYPE_CHECKING:
# codegen/uops.py

    @dataclass(frozen=True, eq=False)
    class UOp:
      @staticmethod
      @functools.lru_cache(maxsize=None)
      def _const(dtype:Optional[DType], b:ConstType|Variable):
        if isinstance(b, Variable): return UOp(UOps.DEFINE_VAR, dtype, (), b)
        return UOp(UOps.CONST, dtype, arg=dtypes.as_const(b, dtype) if dtype is not None else b)
# dtype.py

# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
float4_folding = PatternMatcher([
  # reorder index to bring const closer to store
  (UOp(UOps.STORE, src=(UOp.var("buf"), UOp.var("idx")+
    (UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex")+UOp.var("idx2")), UOp.var("var"))).name("store"),
    lambda buf, store, idx, idx2, ex, var: UOp(UOps.STORE, store.dtype, (buf, idx+idx2+ex, var), store.arg)),
  # float(2,4) load
  (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"))).name("load"), float4_expand_load),
  (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx"))).name("load"), float4_expand_load),
  (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex"))).name("load"), float4_expand_load),
  # float(2,4) store
  # TODO: fold ADDs into one UOp and remove add chains
  (UOp(UOps.STORE, src=(UOp.var("buf"),
    UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2")+UOp.var("idx3"), UOp.var("var"))).name("store_allow_any_len"),
    float4_contract_store),
  (UOp(UOps.STORE, src=(UOp.var("buf"),
    UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"), UOp.var("var"))).name("store_allow_any_len"),
    float4_contract_store),
  (UOp(UOps.STORE, src=(UOp.var("buf"),
    UOp(UOps.EXPAND).name("ex")+UOp.var("idx"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
  (UOp(UOps.STORE, src=(UOp.var("buf"),
    UOp(UOps.EXPAND).name("ex"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
  # image handling
  (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
     UOp.var('id4'))))).name("ls_allow_any_len"), image_contract_load),
  (UOp(UOps.STORE, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
     UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex"))), UOp.var("var"))).name("ls_allow_any_len"),
     image_contract_store),
])
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# dtype.py

  @dataclass(frozen=True, order=True)
  class DType:
    def vec(self, sz:int):
      assert sz > 1 and self.count == 1, f"can't vectorize {self} with size {sz}"
      return DType(self.priority, self.itemsize*sz, f"{INVERSE_DTYPES_DICT[self.name]}{sz}", None, sz)

# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py

  class PatternMatcher:
    def __init__(self, patterns:List[Tuple[Union[UPat, UOp], Callable]]):
      self.patterns = patterns
      self.pdict: DefaultDict[Tuple[UOps, Any], List[Tuple[UPat, Callable]]] = defaultdict(list)
      for p,fxn in self.patterns:
        if isinstance(p, UOp): p = UPat.compile(p)

    class UPat:
      @staticmethod
      def compile(u: UOp, name:Optional[str]=None) -> UPat:
        if u.op is UOps.VAR: return UPat(name=name or u.arg, dtype=u.dtype) if len(u.src) == 0 else UPat.compile(u.src[0], name or u.arg)

      class UPat:
        @staticmethod
        def compile(u: UOp, name:Optional[str]=None) -> UPat:
          return UPat(u.op, u.arg, (list if u.commutative() else tuple)([UPat.compile(src) for src in u.src]) if u.src != () else None,
                      name, u.dtype, allow_any_len=(isinstance(name, str) and 'allow_any_len' in name))

        @dataclass(frozen=True, eq=False)
        class UOp:
          def commutative(self) -> bool:
            return self.op is UOps.UNMUL or (self.op is UOps.ALU and \
              self.arg in {BinaryOps.ADD, BinaryOps.MUL, BinaryOps.MAX, BinaryOps.CMPNE, BinaryOps.XOR, BinaryOps.AND, BinaryOps.OR})

          class UPat:
            def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
                         name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
              self.op: Optional[Tuple[UOps, ...]] = None if op is None else (tuple(op) if isinstance(op, set) else (op,))
              self.dtype: Optional[Tuple[DType, ...]] = None if dtype is None else (tuple(dtype) if isinstance(dtype, set) else (dtype,))
              self.arg = arg
              self.src: Any = None
              if isinstance(src, list):
              elif isinstance(src, tuple):
              elif isinstance(src, UPat):
              self.name: Optional[str] = name
              self.allowed_len: int = 0 if allow_any_len or isinstance(src, UPat) or src is None else len(src)

              class UPat:
                def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
                             name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
                    self.src = [src]

        class UPat:
          def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
                       name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
              self.src = list(itertools.permutations(src))

      assert p.op is not None
      for uop in p.op: self.pdict[(uop, p.arg)].append((p, fxn))
# codegen/uopgraph.py
# dtype.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# dtype.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# dtype.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(2))).name("expand"),))
   .name("reduce_allow_any_len"), reduce_before_expand),
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(8))).name("expand"),))
   .name("reduce_allow_any_len"), reduce_before_expand),
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("add") + UOp(UOps.WMMA).name("wmma"),
    lambda add, wmma: UOp(wmma.op, wmma.dtype, (wmma.src[0], wmma.src[1], wmma.src[2]+add), wmma.arg)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.ALU, dtype=dtypes.uint64, src=(UOp.var("x"), UOp.var("seed")), arg=BinaryOps.THREEFRY), threefry2x32),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(BinaryOps.MUL,
    UOp.cvar("mval"), UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
    UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None,0)), loop_collapse),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(UnaryOps.NEG,
    UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
    UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None, 0)),
    lambda **kwargs: loop_collapse(mval=UOp.const(dtypes.int, -1), **kwargs)),
# codegen/uops.py
# codegen/uopgraph.py
  (UPat(UOps.PHI, src=(UPat(UOps.DEFINE_ACC, name="phi_input", src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),
                       UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
# codegen/uops.py
# codegen/uopgraph.py
  (UPat(UOps.PHI, src=(UPat(UOps.GEP, name="phi_input", src=(UPat(UOps.DEFINE_ACC, src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),)),
                       UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.cvar('c1') * UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v'))), lambda c1,c2,v: v if c1.arg == c2.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.cvar('c1') * (UOp.var('add') + UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v')))),
    lambda c1, add, c2, v: (add*c1+v) if c1.arg == c2.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.UNMUL, src=(UOp.const(None, 0).name('zero'), UOp.var())), lambda zero: zero),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.UNMUL).name('unmul').cast().name('root'), lambda root,unmul: UOp(UOps.UNMUL, root.dtype, (unmul.src[0].cast(root.dtype), unmul.src[1]))),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).cast()*
    UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"),
    lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).where(
    UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"), UOp.const(None, 0.0)),
    lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.cvar("c1") - (UOp.var("x") + UOp.cvar("c2")), lambda c1, c2, x: (c1-c2)-x),  # c1 - (x + c2) -> (c1-c2) - x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')), lambda c,s: c if (s.arg[2]-1) <= c.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if 0 >= c.arg else None),  # TODO: generic
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), -(UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.arg[2]-1+c2.arg) >= c.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')), lambda c,s: s if s.src[0].arg >= c.arg else None),  # TODO: generic
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if s.src[0].arg >= c.arg else None),  # TODO: generic
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s'))), lambda c,s: -s if -(s.src[1].arg-1) >= c.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.src[1].arg-1+c2.arg) >= c.arg else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.GEP, src=(UOp.cvar("c"),)).name("root"), lambda root, c: UOp.const(root.dtype, c.arg)),
# codegen/uops.py
# codegen/uopgraph.py
  (UPat(UOps.CAST, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
# codegen/uops.py

  class UPat:
    def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
                 name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
        self.src = [itertools.repeat(src)]

# codegen/uopgraph.py
  (UPat(UOps.VECTORIZE, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC).name("acc"), UOp.var("acc"))), lambda acc: UOp.cast(acc.src[0], acc.dtype)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)), UOp.var("x"))), lambda x: x),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.PHI, src=(UOp.cvar(), UOp.var("x"))), lambda x: x),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)).name("root"), lambda root: UOp.cast(root.src[0], root.dtype)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.GEP, src=(UOp.cvar("x"),)).name("root"), lambda root,x: UOp.const(root.dtype, x.arg)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.max(UOp.var('x'), UOp.const(dtypes.int, -2147483648)), lambda x: x),
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var().lt(UOp.const(dtypes.bool, False)), lambda: UOp.const(dtypes.bool, False)),
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.const(dtypes.bool, True).lt(UOp.var()), lambda: UOp.const(dtypes.bool, False)),
# codegen/uops.py
# dtype.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var().where(UOp.var("val"), UOp.var("val")), lambda val: val),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.cvar('gate').where(UOp.var('c0'), UOp.var('c1')), lambda gate, c0, c1: c0 if gate.arg else c1),
# codegen/uops.py
# codegen/uopgraph.py
  (UPat(UOps.ALU, name="root", src=UPat(UOps.CONST)), lambda root: UOp.const(root.dtype, exec_alu(root.arg, root.dtype, [x.arg for x in root.src]))),
# codegen/uops.py
# codegen/uopgraph.py
  (-(-UOp.var('x')), lambda x: x),    # -(-x) -> x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') + 0, lambda x: x),    # x+0 -> x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') * 1, lambda x: x),    # x*1 -> x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') * -1, lambda x: -x),  # x*-1 -> -x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') // UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x//x -> 1
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') // 1, lambda x: x),   # x//1 -> x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') // -1, lambda x: -x), # x//-1 -> -x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') / UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x/x -> 1
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') / UOp.cvar('c'), lambda x,c: x*exec_alu(UnaryOps.RECIP, c.dtype, [c.arg])),    # x/c -> x*(1/c)
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x', dtype=dtypes.bool).max(UOp.const(dtypes.bool, False)), lambda x: x),  # max(x, False) -> x
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') * 0, lambda x: UOp.const(x.dtype, float('nan') if isinstance(x.arg, float) and (math.isnan(x.arg) or math.isinf(x.arg)) else 0)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var('x') - UOp.var('x'), lambda x: UOp.const(x.dtype, 0)),   # x-x -> 0
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.load(UOp.var("buf"), UOp.var("idx"))), lambda buf,idx:UOp(UOps.NOOP)),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var('x') + UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, c2.arg]))),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var('x') - UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c2.arg, -c1.arg]))),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.cvar('c')*UOp.var('x')) % UOp.cvar('c'), lambda x,c: x.const(0)),
# codegen/uops.py
# codegen/uopgraph.py
  (((UOp.cvar('c')*UOp.var('x'))+UOp.var('x2')) % UOp.cvar('c'), lambda x,c,x2: x2%c),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var("x") * UOp.cvar("c1")) * UOp.cvar("c2"), lambda x,c1,c2: x*UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c1.arg, c2.arg]))),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("x") % UOp.const(None, 1), lambda x: UOp.const(x.dtype, 0)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("x") * UOp.cvar("c0") + UOp.var("x") * UOp.cvar("c1"), lambda x,c0,c1: x*exec_alu(BinaryOps.ADD, x.dtype, [c0.arg, c1.arg])),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var("x") * UOp.cvar("c0")) // UOp.cvar("c0"), lambda x,c0: x if c0.arg != 0 else None),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var("x") * UOp.var("x2")) / UOp.var("x2"), lambda x,x2: x),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var("x") // UOp.cvar("c0")) // UOp.cvar("c1"), lambda x,c0,c1: x//UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c0.arg, c1.arg]))),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.var("x") / UOp.var("x2")) / UOp.var("x3"), lambda x,x2,x3: x/(x2*x3)),
# codegen/uops.py
# codegen/uopgraph.py
  ((UOp.cvar("c0") + UOp.var("x")).lt(UOp.cvar("c1")),
    lambda x,c0,c1: UOp.lt(x, UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, -c0.arg])))),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("x") + UOp.var("x") * UOp.cvar("c0"), lambda x,c0: x*UOp.const(x.dtype, c0.arg+1)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("x").ne(0), lambda x: x.cast(dtypes.bool)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.var("x", dtype=dtypes.bool).ne(1), lambda x: -x),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.alu(TernaryOps.WHERE, UOp.var("gate"), UOp.var("alt"), UOp.load(UOp.var("buf"), UOp.var("idx")))),
   lambda buf, idx, gate, alt: UOp.store(buf, idx, alt, gate)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(4))).name("root"),
   lambda root, val, v0, v1, v2, v3: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1, v2, v3))))),
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(2))).name("root"),
   lambda root, val, v0, v1: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1))))),
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.lt(-UOp.var('x'), UOp.cvar('c', dtypes.int)), lambda c,x: UOp.lt(UOp.const(c.dtype, -c.arg), x)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.CAST).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.VECTORIZE).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var")), lambda buf,idx,var: UOp.load(buf, idx, dtype=var.dtype)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var"), UOp.var("barrier")),
   lambda buf,idx,var,barrier: UOp.load(buf, idx, barrier, dtype=var.dtype)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var")), lambda var: var),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var"), UOp.var()), lambda var: var),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.var("val"), UOp.const(dtypes.bool, True)), UOp.store),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp.store(UOp.var(), UOp.var(), UOp.var(), UOp.const(dtypes.bool, False)), lambda: UOp(UOps.NOOP)),
# codegen/uops.py
# codegen/uopgraph.py
  (UOp(UOps.SINK).name("root"),
    lambda root: UOp(UOps.SINK, root.dtype, a, root.arg) if len(a:=tuple(x for x in root.src if x.op is not UOps.NOOP)) != len(root.src) else None),
# codegen/uops.py
# codegen/uopgraph.py
acc_number = 0
expander = PatternMatcher([
  (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST, UOps.GEP, UOps.WMMA, UOps.LOAD, UOps.STORE,
         UOps.VECTORIZE, UOps.REDUCE, UOps.EXPAND, UOps.IF}, name="root"), do_expand),
  (UOp(UOps.REDUCE).name("root"), do_reduce_with_expand),
  (UOp(UOps.CONTRACT).name("con"), do_contract),
  # remove EXPANDs from SINK
  (UOp(UOps.SINK).name("root"),
   lambda root: UOp(UOps.SINK, root.dtype, a, root.arg)
    if len(a:=tuple(flatten(x.src if x.op is UOps.EXPAND else (x,) for x in root.src))) != len(root.src) else None),
  # BARRIERs aren't actually expanded
  (UOp(UOps.BARRIER, src=(UOp(UOps.EXPAND).name("ex"),)), lambda ex: UOp(UOps.EXPAND, None, (UOp(UOps.BARRIER, None, ex.src),)*len(ex.src), ex.arg)),
  # empty EXPAND is NOOP
  (UOp(UOps.EXPAND, src=(UOp.var('x'),), arg=()), lambda x: x),
  # no ALU on vectorized dtypes
  (UPat({UOps.ALU, UOps.CAST}, name="alu"), no_vectorized_alu),
])
# codegen/uops.py
# codegen/uopgraph.py

  class UOpGraph:
    cnt = 0

# renderer/__init__.py

  @dataclass(frozen=True)
  class TensorCore: # D = A * B + C, A is (M x K), B is (K x N), C and D are (M x N)
    dims: Tuple[int,int,int] # N, M, K
    dtype_in: DType # dtype for A and B
    dtype_out: DType # dtype for C and D
    threads: List[Tuple[int,int]] # list of (TC dim,amt) that construct the warp thread structure
    thread_local_sizes: List[List[int]] # in each thread, the number of elements stored in registers for each TC dim

  @dataclass(frozen=True)
  class Program:
    name:str
    src:str
    dname:str
    global_size:Optional[List[int]]=None
    local_size:Optional[List[int]]=None
    uops:Optional[UOpGraph]=None
    op_estimate:sint=0
    mem_estimate:sint=0

  class Renderer:
    device: str = ""
    suffix: str = ""
    supports_float4: bool = True
    has_local: bool = True
    has_shared: bool = True
    global_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
    local_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
    shared_max: int = 32768
    tensor_cores: List[TensorCore] = []

# device.py
Device = _Device()

  @dataclass(frozen=True, eq=True)
  class BufferOptions:
    image: Optional[ImageDType] = None
    uncached: bool = False
    cpu_access: bool = False
    host: bool = False
    nolru: bool = False

  class LRUAllocator(Allocator):  # pylint: disable=abstract-method
    """

MallocAllocator = _MallocAllocator()

  class HWCommandQueue:
    """

    def hcq_command(func):
      return __wrapper

  class HCQCompiled(Compiled):
    """

  class HCQAllocator(LRUAllocator): # pylint: disable=abstract-method
    """

# lazy.py
lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()
# helpers.py
# lazy.py
view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}

  class LazyBuffer:
                 op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),
                 base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):
      self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, dtype, st.shape, st.size, metadata
      self._base: Optional[LazyBuffer] = None
      if base is None:
        # properties on base
        self.op, self.arg, self.srcs = op, arg, srcs  # this is a LazyOp, except the src is LazyBuffers and not LazyOps
        assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"
        if self.op is MetaOps.VIEW:
          # some LazyBuffers can be processed with only a view, no AST required
          self.buffer: Buffer = srcs[0].base.buffer.view(st.size, dtype, srcs[0].st.views[0].offset * srcs[0].dtype.itemsize)
        else:
          self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, dtype)
        self.buffer.ref(1)
        self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None
        self.forced_realize = False
      else:
        # properties on view
        assert base.base == base, "base must be a base itself"
        self._base = base
    def __del__(self):
      if hasattr(self, 'buffer'): self.buffer.ref(-1)
    def __repr__(self) -> str:
      return f"<LB {self.device} {self.shape} {str(self.dtype)[7:]} {self.st if self.base != self else (self.op, self.realized)}>"
    @property
    def realized(self) -> Optional[Buffer]:
      # NOTE: we check for a lack of srcs instead of an allocated buffer to make unrealized assigns return None here
      return self.buffer if self._base is None and not hasattr(self, 'srcs') else None
    # NOTE: this has to be a function to prevent self reference
    @property
    def base(self) -> LazyBuffer: return self._base if self._base is not None else self
    # same API as multi
    @property
    def lbs(self) -> List[LazyBuffer]: return [self]
    @staticmethod
    def metaop(op, shape:Tuple[sint,...], dtype:DType, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:
      assert isinstance(src, tuple)
      return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)
    def const(self, val:ConstType, shape:Optional[Tuple[sint,...]]=None) -> LazyBuffer:
      assert isinstance(val, (int,float,bool)), f"{val=} has {type(val)=}, not a ConstType"
      shape = self.shape if shape is None else shape
      return LazyBuffer.metaop(MetaOps.CONST, tuple(), self.dtype, self.device, arg=val).reshape((1,)*len(shape)).expand(shape)
    def is_realized(self) -> bool: return self.base.realized is not None
    def assign(self, x:LazyBuffer) -> LazyBuffer:
      assert x.size == self.size, f"assign target must have same size {self.size=} != {x.size=}"
      return LazyBuffer.metaop(MetaOps.ASSIGN, self.shape, self.dtype, self.device, arg=() if self.st.contiguous else (self.st,), src=(x, self.base))
    def can_view(self): return self.st.consecutive and not self.is_unrealized_const() and self.device.split(":")[0] in view_supported_devices
    def contiguous(self, allow_buffer_view=True):
      if not self.st.contiguous or self.size != self.base.size or self.is_unrealized_const():
        ret = self.e(MetaOps.VIEW) if allow_buffer_view and self.can_view() else self.e(MetaOps.CONTIGUOUS)
        if (sti := self.st.invert(self.base.shape)) is not None: self.base.contiguous_child = ref(ret), sti
        return ret
      self.base.forced_realize = True
      return self
    def cast(self, dtype:DType, bitcast:bool=False, allow_buffer_view=True):
      if self.dtype == dtype: return self
      if self.device.startswith("DISK") and not bitcast: raise RuntimeError("attempted to cast disk buffer (bitcast only)")
      if self.is_unrealized_unmasked_const() and not bitcast:
        return create_lazybuffer(self.device, self.st, dtype, MetaOps.CONST, dtypes.as_const(self.base.arg, dtype))
      new_shape = self.shape
      if bitcast and self.dtype.itemsize != dtype.itemsize:
        if not self.device.startswith("DISK"): raise RuntimeError("shape changing bitcast only supported on DISK right now")
        if not all_int(new_shape): raise RuntimeError("shape changing bitcast with symbolic shape isn't supported yet")
        # https://pytorch.org/docs/stable/generated/torch.Tensor.view.html
        if not (new_shape[-1]*self.dtype.itemsize) % dtype.itemsize == 0: raise RuntimeError("unsupported size in bitcast")
        new_shape = new_shape[:-1] + ((new_shape[-1]*self.dtype.itemsize) // dtype.itemsize,)
      elif getenv("CAST_BEFORE_VIEW", 1) and dtype.itemsize <= self.dtype.itemsize and self != self.base:
        # TODO: applying this makes gpt2 slower
        return self.base.cast(dtype, bitcast)._view(self.st)
      cast_op: Union[MetaOps, UnaryOps] = (MetaOps.VIEW if self.can_view() and allow_buffer_view else UnaryOps.BITCAST) if bitcast else UnaryOps.CAST
      return create_lazybuffer(self.device, ShapeTracker.from_shape(new_shape), dtype, cast_op, dtype, (self,))
    def is_unrealized_const(self): return self.base.realized is None and self.base.op is MetaOps.CONST and not isinstance(self.base.arg, Variable)
    def is_unrealized_unmasked_const(self): return self.is_unrealized_const() and all(v.mask is None for v in self.st.views)
    def _copy(self, device:str) -> LazyBuffer:
      return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, MetaOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)
    def copy_to_device(self, device:str, force: bool = False) -> LazyBuffer:
      # no COPY
      if self.device == device: return self
      # double COPY = one COPY
      if not force and self.st.contiguous and self.size == self.base.size and not self.base.realized and self.base.op is MetaOps.COPY:
        return self.base.srcs[0].copy_to_device(device).reshape(self.st.shape)
      # const doesn't have to be copied (issues with disk tensor)
      if self.is_unrealized_const():
        return LazyBuffer.metaop(MetaOps.CONST, tuple(), self.dtype, device, arg=self.base.arg)._view(self.st)
      # if it's a shrink, do the shrink before the copy with CONTIGUOUS
      if prod(self.st.shape) < prod(self.base.st.shape): return self.contiguous()._copy(device)
      # copy the base and apply the shapetracker on the new device
      return self.base._copy(device)._view(self.st)
    def e(self, op:Union[MetaOps, UnaryOps, BinaryOps, TernaryOps], *in_srcs:LazyBuffer, arg:Optional[Any]=None) -> LazyBuffer:
      srcs: List[LazyBuffer] = []
      for s in (self,)+in_srcs:
        if s == s.base and s.base.contiguous_child and (root:=s.base.contiguous_child[0]()) is not None:
          srcs.append(root._view(s.base.contiguous_child[1]))
        else:
          srcs.append(s)
      assert all_same(dts:=[x.dtype.scalar() for x in (srcs[1:] if op is TernaryOps.WHERE else srcs)]), f"all dtypes must match {dts} on {op}"
      assert all_same([x.shape for x in srcs]), f"all shapes must be the same {[x.shape for x in srcs]}"
      if op is TernaryOps.WHERE: assert srcs[0].dtype == dtypes.bool, "TernaryOps.WHERE must have the first arg be bool"
      if op is UnaryOps.NEG: assert srcs[0].dtype != dtypes.bool, "UnaryOps.NEG does not accept dtype bool"
      out_dtype = dtypes.bool if op in (BinaryOps.CMPLT, BinaryOps.CMPNE) else srcs[-1].dtype
      # const folding
      if op in python_alu and all(s.is_unrealized_unmasked_const() for s in srcs):
        return self.cast(out_dtype).const(exec_alu(op, out_dtype, [s.base.arg for s in srcs]))
      if op is UnaryOps.NEG and self.base.op is UnaryOps.NEG and self.base.realized is None: return self.base.srcs[0]
      if op in BinaryOps:
        x, y = self, in_srcs[0]
        if op is BinaryOps.ADD:
          if y.is_unrealized_unmasked_const() and y.base.arg == 0: return x
          if x.is_unrealized_unmasked_const() and x.base.arg == 0: return y
        if op is BinaryOps.MUL:
          if x.is_unrealized_unmasked_const() and (val := x.base.arg) in (1, 0, -1):
            return y if val == 1 else y.const(0) if val == 0 else y.e(UnaryOps.NEG)
          if y.is_unrealized_unmasked_const() and (val := y.base.arg) in (1, 0, -1):
            return x if val == 1 else x.const(0) if val == 0 else x.e(UnaryOps.NEG)
      return create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))
    # *** reduce ops ***
    def _reduce_op(self, op:ReduceOps, axis:Tuple[int, ...]) -> LazyBuffer:
      assert all(0 <= x < len(self.shape) for x in axis), f"axis args {axis} out of range for shape {self.shape}"
      axis = tuple(sorted([x for x in axis if self.shape[x] != 1]))
      if len(axis) == 0: return self
      return create_lazybuffer(self.device, ShapeTracker.from_shape(reduce_st(self.st, axis)), self.dtype, op, axis, (self,))
    def r(self, op:ReduceOps, axis:Tuple[int, ...]) -> LazyBuffer:
      new_shape = reduce_st(self.st, axis)
      # TODO: this logic should move to the scheduler
      if 0 in self.shape and 0 not in new_shape: return self.const({ReduceOps.SUM: 0.0, ReduceOps.MAX: dtypes.min(self.dtype)}[op], new_shape)
      # const folding
      # TODO: fold this for symbolic?
      if self.is_unrealized_unmasked_const() and all_int(self.shape):
        return self.const(self.base.arg * {ReduceOps.SUM: prod(self.shape[i] for i in axis), ReduceOps.MAX: 1}[op], new_shape)
      # TODO: can we split symbolic shape if the reduce axis is not symbolic?
      if not getenv("SPLIT_REDUCEOP", 1) or not all_int(self.shape) or (0 in self.shape) or \
        prod(self.shape) // prod(new_shape) < getenv("REDUCEOP_SPLIT_THRESHOLD", 32768):
        return self._reduce_op(op, axis)
      # if there are few globals, make some reduces into globals by splitting into two kernels
      # cap output buffer to 2**22: heuristic number of global outputs to achieve max occupancy with enough locals+upcasts for gemm
      #   ~2**10 should be enough if GROUP is used
      # 256 split maximum should be "negligible reduce" for low prod(new_shape), 8 split minimum.
      # split is moved to the end to provide maximum locality for the second phase reduce.
      self_real_strides = self.st.real_strides(ignore_valid=True)
      split_candidates = [(i, x) for i in axis for x in range(min(256,2**getenv("REDUCEOP_SPLIT_SIZE",22)//prod(new_shape)),8-1,-1)

# tensor.py
# multi.py
# tensor.py
# engine/realize.py
# codegen/kernel.py
# codegen/lowerer.py
render_ops: Any = { NumNode: lambda self, ops, ctx: UOp.const(dtypes.bigint, self.b),
                    MulNode: lambda self, ops, ctx: self.a.render(ops, ctx)*variable_to_uop(self.b, ctx),
                    DivNode: lambda self, ops, ctx: self.a.render(ops, ctx)//variable_to_uop(self.b, ctx),
                    ModNode: lambda self, ops, ctx: self.a.render(ops, ctx)%variable_to_uop(self.b, ctx),
                    LtNode: lambda self, ops, ctx: self.a.render(ops, ctx).lt(variable_to_uop(self.b, ctx)),
  Variable: lambda self,ops,ctx: ctx[self] if ctx is not None and self in ctx else UOp(UOps.DEFINE_VAR, dtypes.int32, (), self),
  SumNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a+b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)),
  AndNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a*b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)) }
if getenv("UOP_IS_SYMBOLIC"):
# helpers.py
# codegen/lowerer.py
# codegen/kernel.py

  class OptOps(Enum):
    TC = auto(); UPCAST = auto(); UPCASTMID = auto(); UNROLL = auto(); LOCAL = auto() # noqa: E702
    GROUP = auto(); GROUPTOP = auto(); NOLOCALS = auto(); PADTO = auto(); MERGE = auto(); SWAP = auto() # noqa: E702

  @dataclass(frozen=True, order=True)
  class Opt:
    op: OptOps
    axis: Optional[int] = None
    amt: Optional[int] = None

  @dataclass
  class TensorCoreOptions:
    axes: Tuple[int, ...] # the location of the original N and M axes if still in the shape
    axes_exist: Tuple[bool, ...] # true if the original N and M axes are still in the shape
    axis_pads: Tuple[Tuple[int, int], ...]

  class Kernel:
    kernel_cnt: Final[DefaultDict[str, int]] = defaultdict(int)

# engine/realize.py
# engine/schedule.py
# engine/graph.py
with contextlib.suppress(ImportError): import networkx as nx
if DEBUG >= 2:
# helpers.py
# engine/graph.py
G:Any = None
counts: DefaultDict[type, int] = defaultdict(int)
top_colors = {MetaOps: '#FFFFa0', UnaryOps: "#c0c0c0", ReduceOps: "#FFA0A0", BinaryOps: "#c0c0c0",
              TernaryOps: "#c0c0c0", BufferOps: '#a0a0ff'}
graph_uops_cnt = 0
# engine/schedule.py
sys.setrecursionlimit(10000)
logops = open(getenv("LOGOPS", ""), "a") if getenv("LOGOPS", "") else None
# helpers.py
# engine/schedule.py

  @dataclass(frozen=True)
  class ScheduleItem:
    ast: LazyOp
    bufs: Tuple[Buffer, ...]
    metadata: Optional[List[Metadata]] = None

    children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer], scheduled=False):
  """recursively search the entire graph for all LazyBuffers, insert realizes after expands"""
  if buf in allbufs or buf.base.realized is not None: return
  if GRAPH: log_lazybuffer(buf, scheduled)
  # check if we need to realize views
  if buf is not buf.base:
    # fuse some pads
    if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \
        prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):
      simple_pads[buf.base] = None
    # realize all expands
    elif prod(buf.base.st.shape) < prod(buf.st.shape):
      # this was causing "test_lil_model" to fail
      if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):
        simple_pads[buf.base] = None # don't realize image to image casts. this is part of a larger problem
      elif not FUSE_AS_ONE_KERNEL: realizes[buf.base] = None
    # check all other pads for safe fusion
    elif any(v.mask is not None for v in buf.st.views): simple_pads[buf.base] = None
    return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children, assign_targets)
  allbufs[buf] = None
  if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None
  if buf.op is MetaOps.ASSIGN:
    assert buf.srcs[1].base is buf.srcs[1], f"assign must be to base {buf.srcs[1]}"
    assert buf.srcs[1].realized is not None, f"assign must be already realized to schedule {buf.srcs[1]}"
    assign_targets[buf.srcs[1]] = buf
  if buf.op is MetaOps.COPY:
    assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"
    realizes[buf.srcs[0].base] = None
  if buf.op is MetaOps.VIEW: realizes[buf.srcs[0].base] = None
  for x in buf.srcs:
    if x.base.realized is None: children[x.base][buf] = None
    _recurse_lb(x, realizes, allbufs, simple_pads, children, assign_targets)
def _is_padding_okay(buf:LazyBuffer, realizes:Dict[LazyBuffer, None]) -> bool:
  if buf in realizes or buf.realized is not None: return True
  # NOTE: this broke to_image_idx and coder with JIT
  if buf.op in UNSAFE_PAD_OPS: return False
  return all(_is_padding_okay(x.base, realizes) for x in buf.srcs)
def _recursive_group(tr:LazyBuffer, st:ShapeTracker, r:LazyBuffer, children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]],
SCHEDULES: List = []
# engine/realize.py
logkerns, logkerns_level = open(getenv("LOGKERNS", ""), "a") if getenv("LOGKERNS", "") else None, getenv("LOGKERNS_LEVEL", 1)
# helpers.py
# engine/realize.py
method_cache: Dict[Tuple[str, LazyOp, int, bool], CompiledRunner] = {}

  @dataclass(frozen=True)
  class ExecItem:
    prg: Runner
    bufs: List[Optional[Buffer]]
    metadata: Optional[List[Metadata]] = None

capturing: List = []  # put classes with an add method in here
# tensor.py
# function.py
# tensor.py

  class Tensor:
    """
    __slots__ = "lazydata", "requires_grad", "grad", "_ctx"
    __deletable__ = ('_ctx',)
    training: ClassVar[bool] = False
    no_grad: ClassVar[bool] = False
                 device:Optional[Union[str, tuple, list]]=None, dtype:Optional[DType]=None, requires_grad:Optional[bool]=None):
      assert dtype is None or isinstance(dtype, DType), f"invalid dtype {dtype}"
      device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)
      # tensors can have gradients if you have called .backward
      self.grad: Optional[Tensor] = None
      # NOTE: this can be in three states. False and None: no gradient, True: gradient
      # None (the default) will be updated to True if it's put in an optimizer
      self.requires_grad: Optional[bool] = requires_grad
      # internal variable used for autograd graph construction
      self._ctx: Optional[Function] = None
      # create a LazyBuffer from the different types of inputs
      if isinstance(data, LazyBuffer): assert dtype is None or dtype == data.dtype, "dtype doesn't match, and casting isn't supported"
      elif isinstance(data, get_args(ConstType)): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data), device, data)
      elif isinstance(data, Variable): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data.unbind()[1]), device, data)
      elif isinstance(data, bytes): data = _frompy(data, dtypes.uint8 if dtype is None else dtype)
      elif isinstance(data, (list, tuple)):
        if dtype is None:
          if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool
          else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float
        if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata
        else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))
      elif data is None: data = _metaop(MetaOps.EMPTY, (0,), dtype or dtypes.default_float, device)
      elif isinstance(data, np.ndarray):
        if data.shape == (): data = _metaop(MetaOps.CONST, tuple(), dtype or _from_np_dtype(data.dtype), device, data.item())
        else: data = _fromnp(data.astype(npdtype) if dtype is not None and (npdtype:=_to_np_dtype(dtype)) is not None else data)
      # by this point, it has to be a LazyBuffer
      if not isinstance(data, (LazyBuffer, MultiLazyBuffer)):
        raise RuntimeError(f"can't create Tensor from {data!r} with type {type(data)}")
      # data is a LazyBuffer, but it might be on the wrong device
      if isinstance(device, tuple):
        # if device is a tuple, we should have/construct a MultiLazyBuffer
        if isinstance(data, MultiLazyBuffer):
          assert data.device == device, f"MultiLazyBuffer device mismatch, {data.device} != {device}"
          self.lazydata: Union[LazyBuffer, MultiLazyBuffer] = data
        else:
          self.lazydata = MultiLazyBuffer.from_sharded(data, device, None)
      else:
        self.lazydata = data if data.device == device else data.copy_to_device(device)
    class train(ContextDecorator):
      def __init__(self, mode:bool = True): self.mode = mode
      def __enter__(self): self.prev, Tensor.training = Tensor.training, self.mode
      def __exit__(self, exc_type, exc_value, traceback): Tensor.training = self.prev
    class inference_mode(ContextDecorator):
      def __init__(self, mode:bool = True): self.mode = mode
      def __enter__(self): self.prev, Tensor.no_grad = Tensor.no_grad, self.mode
      def __exit__(self, exc_type, exc_value, traceback): Tensor.no_grad = self.prev
    def __repr__(self):
      return f"<Tensor {self.lazydata!r} on {self.device} with grad {(self.grad.lazydata if self.grad is not None else None)!r}>"
    # Python has a non moving GC, so this should be okay
    def __hash__(self): return id(self)
    def __bool__(self): raise TypeError("__bool__ on Tensor is not defined")
    def __len__(self):
      if not self.shape: raise TypeError("len() of a 0-d tensor")
      return self.shape[0]
    @property
    def device(self) -> Union[str, Tuple[str, ...]]: return self.lazydata.device
    @property
    def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape
    @property
    def dtype(self) -> DType: return self.lazydata.dtype
    # ***** data handlers ****
    def schedule_with_vars(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:
      """Creates the schedule needed to realize these Tensor(s), with Variables."""
      if getenv("FUZZ_SCHEDULE"):
        from test.external.fuzz_schedule import fuzz_schedule
        fuzz_schedule(flatten([x.lazydata.lbs for x in (self,)+lst]))
      schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)
      return memory_planner(schedule), var_vals
    def schedule(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> List[ScheduleItem]:
      """Creates the schedule needed to realize these Tensor(s)."""
      schedule, var_vals = self.schedule_with_vars(*lst, seen=seen)
      assert len(var_vals) == 0
      return schedule
    def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:
      """Triggers the computation needed to create these Tensor(s)."""
      run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)
      return self
    def replace(self, x:Tensor) -> Tensor:
      """

  class Tensor:
    _seed: int = int(time.time())
    _rng_counter: Optional[Tensor] = None

for device in Device._devices: setattr(Tensor, f"{device.lower()}", functools.partialmethod(Tensor.to, device))
if IMAGE:
# helpers.py
# tensor.py
if TRACEMETA >= 1:
# helpers.py
# tensor.py
  for name, fn in inspect.getmembers(Tensor, inspect.isfunction):
    if name in ["__class__", "__init__", "__repr__", "backward", "sequential"]: continue
    setattr(Tensor, name, functools.wraps(fn)(_metadata_wrapper(fn)))

  def _metadata_wrapper(fn):
    return _wrapper

# __init__.py
# engine/jit.py
# nn/__init__.py
# nn/optim.py

  class Optimizer:
    """

  class OptimizerGroup(Optimizer):
    """

  class LARS(Optimizer):
    """

  class LAMB(Optimizer):
    """

# nn/state.py
safe_dtypes = {"BOOL":dtypes.bool, "I8":dtypes.int8, "U8":dtypes.uint8, "I16":dtypes.int16, "U16":dtypes.uint16, "I32":dtypes.int, "U32":dtypes.uint,
               "I64":dtypes.int64, "U64":dtypes.uint64, "F16":dtypes.float16, "BF16":dtypes.bfloat16, "F32":dtypes.float32, "F64":dtypes.float64}
inverse_safe_dtypes = {v:k for k,v in safe_dtypes.items()}
# nn/datasets.py
# nn/__init__.py

  class BatchNorm:
    """

BatchNorm2d = BatchNorm3d = BatchNorm

  class Conv2d:
    """

  class ConvTranspose2d(Conv2d):
    """

  class Linear:
    """

  class GroupNorm:
    """

  class InstanceNorm:
    """

  class LayerNorm:
    """

  class LayerNorm2d(LayerNorm):
    """

  class RMSNorm:
    """

  class Embedding:
    """

# engine/jit.py
ReturnType = TypeVar('ReturnType')
# __init__.py
# test.py
