test.py             :     1:  from tinygrad.tensor import Tensor
helpers.py          :     6:  if TYPE_CHECKING:  # TODO: remove this and import TypeGuard from typing once minimum python supported version is 3.10
helpers.py          :    10:  T = TypeVar("T")
helpers.py          :    11:  U = TypeVar("U")
helpers.py          :    16:  OSX = platform.system() == "Darwin"
helpers.py          :    17:  CI = os.getenv("CI", "") != ""

helpers.py          :    79:    class GraphException(Exception): pass

helpers.py          :    81:    class Context(contextlib.ContextDecorator):
helpers.py          :    82:      stack: ClassVar[List[dict[str, int]]] = [{}]

helpers.py          :    91:    class ContextVar:
helpers.py          :    92:      _cache: ClassVar[Dict[str, ContextVar]] = {}
helpers.py          :    93:      value: int
helpers.py          :    94:      key: str

helpers.py          :   105:  DEBUG, IMAGE, BEAM, NOOPT, JIT = ContextVar("DEBUG", 0), ContextVar("IMAGE", 0), ContextVar("BEAM", 0), ContextVar("NOOPT", 0), ContextVar("JIT", 1)

helpers.py          :    91:    class ContextVar:
helpers.py          :    95:      def __new__(cls, key, default_value):
helpers.py          :    96:        if key in ContextVar._cache: return ContextVar._cache[key]
helpers.py          :    97:        instance = ContextVar._cache[key] = super().__new__(cls)
helpers.py          :    98:        instance.value, instance.key = getenv(key, default_value), key

helpers.py          :    76:      @functools.lru_cache(maxsize=None)
helpers.py          :    76:      def getenv(key:str, default=0): return type(default)(os.getenv(key, default))

helpers.py          :    99:        return instance

helpers.py          :   106:  WINO, THREEFRY, CAPTURING, TRACEMETA = ContextVar("WINO", 0), ContextVar("THREEFRY", 0), ContextVar("CAPTURING", 1), ContextVar("TRACEMETA", 1)
helpers.py          :   107:  GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING = ContextVar("GRAPH", 0), getenv("GRAPHPATH", "/tmp/net"), ContextVar("SAVE_SCHEDULE", 0), ContextVar("RING", 1)
helpers.py          :   108:  MULTIOUTPUT, PROFILE, TRANSCENDENTAL = ContextVar("MULTIOUTPUT", 1), ContextVar("PROFILE", 0), ContextVar("TRANSCENDENTAL", 1)
helpers.py          :   109:  USE_TC, TC_OPT = ContextVar("TC", 1), ContextVar("TC_OPT", 0)
helpers.py          :   110:  FUSE_AS_ONE_KERNEL = ContextVar("FUSE_AS_ONE_KERNEL", 0)
helpers.py          :   113:    @dataclass(frozen=True)
helpers.py          :   113:    class Metadata:
helpers.py          :   114:      name: str
helpers.py          :   115:      caller: str
helpers.py          :   116:      backward: bool = False

helpers.py          :   120:  _METADATA: contextvars.ContextVar[Optional[Metadata]] = contextvars.ContextVar("_METADATA", default=None)

helpers.py          :   124:    class GlobalCounters:
helpers.py          :   125:      global_ops: ClassVar[int] = 0
helpers.py          :   126:      global_mem: ClassVar[int] = 0
helpers.py          :   127:      time_sum_s: ClassVar[float] = 0.0
helpers.py          :   128:      kernel_count: ClassVar[int] = 0
helpers.py          :   129:      mem_used: ClassVar[int] = 0   # NOTE: this is not reset

helpers.py          :   135:    class Timing(contextlib.ContextDecorator):

helpers.py          :   143:    class Profiling(contextlib.ContextDecorator):

helpers.py          :   161:    class ProfileLogger:
helpers.py          :   162:      writers: int = 0
helpers.py          :   163:      mjson: List[Dict] = []
helpers.py          :   164:      actors: Dict[str, int] = {}
helpers.py          :   165:      subactors: Dict[Tuple[str, str], int] = {}
helpers.py          :   166:      path = getenv("PROFILE_OUTPUT_FILE", temp("tinygrad_profile.json"))

helpers.py          :    77:      def temp(x:str) -> str: return (pathlib.Path(tempfile.gettempdir()) / x).as_posix()

helpers.py          :   191:  _cache_dir: str = getenv("XDG_CACHE_HOME", os.path.expanduser("~/Library/Caches" if OSX else "~/.cache"))
helpers.py          :   192:  CACHEDB: str = getenv("CACHEDB", os.path.abspath(os.path.join(_cache_dir, "tinygrad", "cache.db")))
helpers.py          :   193:  CACHELEVEL = getenv("CACHELEVEL", 2)
helpers.py          :   195:  VERSION = 16
helpers.py          :   196:  _db_connection = None
helpers.py          :   223:  _db_tables = set()
helpers.py          :   297:    class tqdm:

helpers.py          :   323:    class trange(tqdm):

dtype.py            :     6:  ConstType = Union[float, int, bool]

dtype.py            :     9:    @dataclass(frozen=True, order=True)
dtype.py            :     9:    class DType:
dtype.py            :    10:      priority: int  # this determines when things get upcasted
dtype.py            :    11:      itemsize: int
dtype.py            :    12:      name: str
dtype.py            :    13:      fmt: Optional[str]
dtype.py            :    14:      count: int

dtype.py            :    23:    # dependent typing?
dtype.py            :    23:    @dataclass(frozen=True, repr=False)
dtype.py            :    23:    class ImageDType(DType):
dtype.py            :    24:      shape: Tuple[int, ...]   # arbitrary arg for the dtype, used in image for the shape
dtype.py            :    25:      base: DType

dtype.py            :    31:    # @dataclass(frozen=True, init=False, repr=False, eq=False)
dtype.py            :    31:    class PtrDType(DType):

dtype.py            :    38:    class dtypes:
dtype.py            :    65:      bigint: Final[DType] = DType(-1, 0, "bigint", None, 1)   # arbitrary precision integer
dtype.py            :    66:      bool: Final[DType] = DType(0, 1, "bool", '?', 1)
dtype.py            :    67:      int8: Final[DType] = DType(1, 1, "char", 'b', 1)
dtype.py            :    68:      uint8: Final[DType] = DType(2, 1, "unsigned char", 'B', 1)
dtype.py            :    69:      int16: Final[DType] = DType(3, 2, "short", 'h', 1)
dtype.py            :    70:      uint16: Final[DType] = DType(4, 2, "unsigned short", 'H', 1)
dtype.py            :    71:      int32: Final[DType] = DType(5, 4, "int", 'i', 1)
dtype.py            :    72:      uint32: Final[DType] = DType(6, 4, "unsigned int", 'I', 1)
dtype.py            :    73:      int64: Final[DType] = DType(7, 8, "long", 'l', 1)
dtype.py            :    74:      uint64: Final[DType] = DType(8, 8, "unsigned long", 'L', 1)
dtype.py            :    75:      float16: Final[DType] = DType(9, 2, "half", 'e', 1)
dtype.py            :    77:      bfloat16: Final[DType] = DType(10, 2, "__bf16", None, 1)
dtype.py            :    78:      float32: Final[DType] = DType(11, 4, "float", 'f', 1)
dtype.py            :    79:      float64: Final[DType] = DType(12, 8, "double", 'd', 1)
dtype.py            :    82:      half = float16; float = float32; double = float64 # noqa: E702
dtype.py            :    83:      uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 # noqa: E702
dtype.py            :    84:      char = int8; short = int16; int = int32; long = int64 # noqa: E702
dtype.py            :    92:      default_float: ClassVar[DType] = float32
dtype.py            :    93:      default_int: ClassVar[DType] = int32

dtype.py            :    95:  if (env_default_float := getenv("DEFAULT_FLOAT", "")):
dtype.py            :   101:  promo_lattice = { dtypes.bool: [dtypes.int8, dtypes.uint8], dtypes.int8: [dtypes.int16], dtypes.int16: [dtypes.int32], dtypes.int32: [dtypes.int64],
dtype.py            :   101:    dtypes.int64: [dtypes.float16, dtypes.bfloat16], dtypes.uint8: [dtypes.int16, dtypes.uint16], dtypes.uint16: [dtypes.int32, dtypes.uint32],
dtype.py            :   101:    dtypes.uint32: [dtypes.int64, dtypes.uint64], dtypes.uint64: [dtypes.float16, dtypes.bfloat16],
dtype.py            :   101:    dtypes.float16: [dtypes.float32], dtypes.bfloat16: [dtypes.float32], dtypes.float32: [dtypes.float64], }
dtype.py            :   115:  DTYPES_DICT = {k: v for k, v in dtypes.__dict__.items() if not (k.startswith(('__', 'default', 'bigint')) or v.__class__ is staticmethod)}
dtype.py            :   116:  INVERSE_DTYPES_DICT = {v.name:k for k,v in DTYPES_DICT.items()}
dtype.py            :   117:  INVERSE_DTYPES_DICT['bigint'] = 'bigint'
shape/symbolic.py   :    10:    class Node:
shape/symbolic.py   :    11:      b: Union[Node, int]
shape/symbolic.py   :    12:      min: int
shape/symbolic.py   :    13:      max: sint

shape/symbolic.py   :   111:    class Variable(Node):

shape/symbolic.py   :   137:    class NumNode(Node):

shape/symbolic.py   :   183:    class OpNode(Node):

shape/symbolic.py   :   190:    class LtNode(OpNode):

shape/symbolic.py   :   198:    class MulNode(OpNode):

shape/symbolic.py   :   212:    class DivNode(OpNode):

shape/symbolic.py   :   219:    class ModNode(OpNode):

shape/symbolic.py   :   231:    class RedNode(Node):

shape/symbolic.py   :   238:    class SumNode(RedNode):

shape/symbolic.py   :   292:    class AndNode(RedNode):

shape/symbolic.py   :   309:  sint = Union[int, Variable, MulNode, SumNode]
shape/symbolic.py   :   317:  render_python: Dict[Type, Callable[..., str]] = {
shape/symbolic.py   :   317:    Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
shape/symbolic.py   :   317:      else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
shape/symbolic.py   :   317:      else f"{self.expr}"),
shape/symbolic.py   :   317:    NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
shape/symbolic.py   :   317:    MulNode: render_mulnode,
shape/symbolic.py   :   317:    DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
shape/symbolic.py   :   317:    ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
shape/symbolic.py   :   317:    LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
shape/symbolic.py   :   317:    SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   317:    AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   317:  }
shape/view.py       :    85:    @dataclass(frozen=True)
shape/view.py       :    85:    class View:
shape/view.py       :    86:      shape:Tuple[sint, ...]
shape/view.py       :    87:      strides:Tuple[sint, ...]
shape/view.py       :    88:      offset:sint
shape/view.py       :    89:      mask:Optional[Tuple[Tuple[sint, sint], ...]]
shape/view.py       :    90:      contiguous:bool

...e/shapetracker.py:    10:    @dataclass(frozen=True)
...e/shapetracker.py:    10:    class ShapeTracker:
...e/shapetracker.py:    11:      views: Tuple[View, ...]

ops.py              :    15:    # these are the llops your accelerator must implement, along with toCpu
ops.py              :    15:    # the Enum class doesn't work with mypy, this is static. sorry it's ugly
ops.py              :    15:    # NOTE: MOD, CMPLT don't have to be implemented on vectors, just scalars
ops.py              :    15:    # NOTE: many GPUs don't have DIV, but UnaryOps.RECIP doesn't work for integer division
ops.py              :    15:    class UnaryOps(Enum):
ops.py              :    16:      """A -> A (elementwise)"""
ops.py              :    17:      EXP2 = auto(); LOG2 = auto(); CAST = auto(); BITCAST = auto(); SIN = auto(); SQRT = auto(); NEG = auto(); RECIP = auto() # noqa: E702

ops.py              :    18:    class BinaryOps(Enum):
ops.py              :    19:      """A + A -> A (elementwise)"""
ops.py              :    20:      ADD = auto(); MUL = auto(); IDIV = auto(); MAX = auto(); MOD = auto(); CMPLT = auto(); CMPNE = auto(); XOR = auto() # noqa: E702
ops.py              :    21:      SHL = auto(); SHR = auto(); OR = auto(); AND = auto(); THREEFRY = auto() # noqa: E702

ops.py              :    22:    class TernaryOps(Enum):
ops.py              :    23:      """A + A + A -> A (elementwise)"""
ops.py              :    24:      WHERE = auto(); MULACC = auto() # noqa: E702

ops.py              :    25:    class ReduceOps(Enum):
ops.py              :    26:      """A -> B (reduce)"""
ops.py              :    27:      SUM = auto(); MAX = auto(); WMMA = auto() # noqa: E702

ops.py              :    28:    class BufferOps(Enum): LOAD = auto(); CONST = auto(); STORE = auto() # noqa: E702

ops.py              :    29:    class MetaOps(Enum):
ops.py              :    30:      EMPTY = auto(); CONST = auto(); COPY = auto(); CONTIGUOUS = auto(); CUSTOM = auto(); ASSIGN = auto(); VIEW = auto(); KERNEL = auto() # noqa: E702

ops.py              :    31:  Op = Union[UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps, BufferOps]
ops.py              :    34:  UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}

ops.py              :    37:    @dataclass(frozen=True)
ops.py              :    37:    class MemBuffer:
ops.py              :    38:      idx: int
ops.py              :    39:      dtype: DType
ops.py              :    40:      st: ShapeTracker

ops.py              :    43:    @dataclass(frozen=True)
ops.py              :    43:    class ConstBuffer:
ops.py              :    44:      val: ConstType | Variable
ops.py              :    45:      dtype: DType
ops.py              :    46:      st: ShapeTracker

ops.py              :    49:    @dataclass(frozen=True)
ops.py              :    49:    class KernelInfo:
ops.py              :    50:      local_dims: int = 0           # number of local dimensions  (this is remapping RANGE to SPECIAL)
ops.py              :    51:      upcasted: int = 0             # count that are upcasted     (this is remapping RANGE to EXPAND)
ops.py              :    52:      dont_use_locals: bool = False # don't use local indexing

ops.py              :    55:    @dataclass(frozen=True, eq=False)
ops.py              :    55:    class LazyOp:
ops.py              :    56:      op: Op
ops.py              :    57:      src: Tuple[LazyOp, ...] = ()
ops.py              :    58:      arg: Any = None

ops.py              :   109:  python_alu: Dict[Op, Callable]  = {
ops.py              :   109:    UnaryOps.LOG2: lambda x: math.log2(x) if x > 0 else -math.inf if x == 0 else math.nan,
ops.py              :   109:    UnaryOps.EXP2: hook_overflow(math.inf, lambda x: 2**x),
ops.py              :   109:    UnaryOps.SQRT: lambda x: math.sqrt(x) if x >= 0 else math.nan,
ops.py              :   109:    UnaryOps.SIN: lambda x: math.sin(x) if not math.isinf(x) else math.nan,
ops.py              :   109:    UnaryOps.RECIP: lambda x: 1/x if x != 0 else math.copysign(math.inf, x),
ops.py              :   109:    UnaryOps.NEG: lambda x: (not x) if isinstance(x, bool) else -x,
ops.py              :   109:    BinaryOps.SHR: operator.rshift, BinaryOps.SHL: operator.lshift,
ops.py              :   109:    BinaryOps.MUL: operator.mul, BinaryOps.ADD: operator.add,
ops.py              :   109:    BinaryOps.XOR: operator.xor, BinaryOps.MAX: max, BinaryOps.CMPNE: operator.ne, BinaryOps.CMPLT: operator.lt,
ops.py              :   109:    BinaryOps.OR: operator.or_, BinaryOps.AND: operator.and_,
ops.py              :   109:    BinaryOps.MOD: lambda x,y: abs(int(x))%abs(int(y))*(1,-1)[x<0], BinaryOps.IDIV: lambda x, y: int(x/y) if y != 0 else x*math.inf,
ops.py              :   109:    TernaryOps.MULACC: lambda x,y,z: (x*y)+z,
ops.py              :   109:    TernaryOps.WHERE: lambda x,y,z: y if x else z}

ops.py              :   103:    def hook_overflow(dv, fxn):
ops.py              :   107:      return wfxn

ops.py              :   131:  truncate: Dict[DType, Callable] = {dtypes.bool: bool,
ops.py              :   131:    # TODO: bfloat16
ops.py              :   131:    dtypes.float16: truncate_fp16, dtypes.float32: lambda x: ctypes.c_float(x).value, dtypes.float64: lambda x: ctypes.c_double(x).value,
ops.py              :   131:    dtypes.uint8: lambda x: ctypes.c_uint8(x).value, dtypes.uint16: lambda x: ctypes.c_uint16(x).value,
ops.py              :   131:    dtypes.uint32: lambda x: ctypes.c_uint32(x).value, dtypes.uint64: lambda x: ctypes.c_uint64(x).value,
ops.py              :   131:    dtypes.int8: lambda x: ctypes.c_int8(x).value, dtypes.int16: lambda x: ctypes.c_int16(x).value, dtypes.int32: lambda x: ctypes.c_int32(x).value \
ops.py              :   131:        if isinstance(x,int) else x, dtypes.int64: lambda x: ctypes.c_int64(x).value, dtypes.bigint: lambda x: x }
codegen/uops.py     :    13:    # the order of these UOps controls the order of the toposort
codegen/uops.py     :    13:    class UOps(Enum):
codegen/uops.py     :    15:      SINK = auto(); VAR = auto(); EXPAND = auto(); CONTRACT = auto() # noqa: E702
codegen/uops.py     :    16:      DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702
codegen/uops.py     :    17:      CONST = auto(); SPECIAL = auto() # noqa: E702
codegen/uops.py     :    18:      NOOP = auto(); UNMUL = auto(); GEP = auto() # noqa: E702
codegen/uops.py     :    20:      CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702
codegen/uops.py     :    21:      ALU = auto(); REDUCE = auto(); WMMA = auto() # noqa: E702
codegen/uops.py     :    23:      LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702
codegen/uops.py     :    25:      BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702
codegen/uops.py     :    27:      ENDRANGE = auto(); ENDIF = auto() # noqa: E702

codegen/uops.py     :    29:  END_FOR_UOP = {UOps.IF:(UOps.STORE, UOps.ENDIF), UOps.RANGE:(UOps.PHI, UOps.ENDRANGE)}

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    34:      op: UOps
codegen/uops.py     :    35:      dtype: Optional[DType] = None
codegen/uops.py     :    36:      src: Tuple[UOp, ...] = tuple()
codegen/uops.py     :    37:      arg: Any = None

codegen/uops.py     :   100:    class UPat:

codegen/uops.py     :   145:    class PatternMatcher:

...transcendental.py:     6:  TRANSCENDENTAL_SUPPORTED_DTYPES = {dtypes.float16, dtypes.float32, dtypes.float64}
codegen/uopgraph.py :    11:  if TYPE_CHECKING:

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    71:      def const(self:Union[UOp, DType, None], b:ConstType|Variable): return UOp._const(self.dtype if isinstance(self, UOp) else self, b)

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    74:        @staticmethod
codegen/uops.py     :    74:        @functools.lru_cache(maxsize=None)
codegen/uops.py     :    74:        def _const(dtype:Optional[DType], b:ConstType|Variable):
codegen/uops.py     :    75:          if isinstance(b, Variable): return UOp(UOps.DEFINE_VAR, dtype, (), b)
codegen/uops.py     :    76:          return UOp(UOps.CONST, dtype, arg=dtypes.as_const(b, dtype) if dtype is not None else b)

dtype.py            :    38:        class dtypes:
dtype.py            :    54:          @staticmethod
dtype.py            :    54:          def as_const(val: ConstType, dtype:DType): return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)

dtype.py            :    38:          class dtypes:
dtype.py            :    42:            @staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool
dtype.py            :    42:            def is_int(x: DType) -> bool: return x.scalar() in (dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64, dtypes.bigint) or dtypes.is_unsigned(x)

dtype.py            :     9:            @dataclass(frozen=True, order=True)
dtype.py            :     9:            class DType:
dtype.py            :    19:              def scalar(self): return DTYPES_DICT[self.name[:-len(str(self.count))]] if self.count > 1 else self

codegen/uopgraph.py :    53:  float4_folding = PatternMatcher([
codegen/uopgraph.py :    53:    # reorder index to bring const closer to store
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"), UOp.var("idx")+
codegen/uopgraph.py :    53:      (UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex")+UOp.var("idx2")), UOp.var("var"))).name("store"),
codegen/uopgraph.py :    53:      lambda buf, store, idx, idx2, ex, var: UOp(UOps.STORE, store.dtype, (buf, idx+idx2+ex, var), store.arg)),
codegen/uopgraph.py :    53:    # float(2,4) load
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    # float(2,4) store
codegen/uopgraph.py :    53:    # TODO: fold ADDs into one UOp and remove add chains
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2")+UOp.var("idx3"), UOp.var("var"))).name("store_allow_any_len"),
codegen/uopgraph.py :    53:      float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"), UOp.var("var"))).name("store_allow_any_len"),
codegen/uopgraph.py :    53:      float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
codegen/uopgraph.py :    53:    # image handling
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
codegen/uopgraph.py :    53:       UOp.var('id4'))))).name("ls_allow_any_len"), image_contract_load),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
codegen/uopgraph.py :    53:       UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex"))), UOp.var("var"))).name("ls_allow_any_len"),
codegen/uopgraph.py :    53:       image_contract_store),
codegen/uopgraph.py :    53:  ])

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    84:      @staticmethod
codegen/uops.py     :    84:      def var(name:Optional[str]=None, dtype:Optional[DType]=None): return UOp(UOps.VAR, dtype=dtype, arg=name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    50:      def name(self, name:Optional[str]): return UOp(UOps.VAR, src=(self,), arg=name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    52:      def __add__(self, x): return UOp.alu(BinaryOps.ADD, self, ufix(self.dtype, x))

codegen/uops.py     :    31:      def ufix(dtype: Optional[DType], x): return UOp.const(dtype, x) if not isinstance(x, UOp) else x

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    78:        @staticmethod
codegen/uops.py     :    78:        def alu(arg, *src:UOp): return UOp(UOps.ALU, dtypes.bool if arg in {BinaryOps.CMPLT, BinaryOps.CMPNE} else src[-1].dtype, src, arg)

dtype.py            :     9:    @dataclass(frozen=True, order=True)
dtype.py            :     9:    class DType:
dtype.py            :    16:      def vec(self, sz:int):
dtype.py            :    17:        assert sz > 1 and self.count == 1, f"can't vectorize {self} with size {sz}"
dtype.py            :    18:        return DType(self.priority, self.itemsize*sz, f"{INVERSE_DTYPES_DICT[self.name]}{sz}", None, sz)

codegen/uops.py     :   145:    class PatternMatcher:
codegen/uops.py     :   146:      def __init__(self, patterns:List[Tuple[Union[UPat, UOp], Callable]]):
codegen/uops.py     :   147:        self.patterns = patterns
codegen/uops.py     :   148:        self.pdict: DefaultDict[Tuple[UOps, Any], List[Tuple[UPat, Callable]]] = defaultdict(list)
codegen/uops.py     :   150:        for p,fxn in self.patterns:
codegen/uops.py     :   151:          if isinstance(p, UOp): p = UPat.compile(p)

codegen/uops.py     :   100:      class UPat:
codegen/uops.py     :   120:        @staticmethod
codegen/uops.py     :   120:        def compile(u: UOp, name:Optional[str]=None) -> UPat:
codegen/uops.py     :   121:          if u.op is UOps.VAR: return UPat(name=name or u.arg, dtype=u.dtype) if len(u.src) == 0 else UPat.compile(u.src[0], name or u.arg)

codegen/uops.py     :   100:        class UPat:
codegen/uops.py     :   120:          @staticmethod
codegen/uops.py     :   120:          def compile(u: UOp, name:Optional[str]=None) -> UPat:
codegen/uops.py     :   122:            return UPat(u.op, u.arg, (list if u.commutative() else tuple)([UPat.compile(src) for src in u.src]) if u.src != () else None,
codegen/uops.py     :   122:                        name, u.dtype, allow_any_len=(isinstance(name, str) and 'allow_any_len' in name))

codegen/uops.py     :    33:          @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:          class UOp:
codegen/uops.py     :    38:            def commutative(self) -> bool:
codegen/uops.py     :    39:              return self.op is UOps.UNMUL or (self.op is UOps.ALU and \
codegen/uops.py     :    39:                self.arg in {BinaryOps.ADD, BinaryOps.MUL, BinaryOps.MAX, BinaryOps.CMPNE, BinaryOps.XOR, BinaryOps.AND, BinaryOps.OR})

codegen/uops.py     :   100:            class UPat:
codegen/uops.py     :   101:              def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                           name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   103:                self.op: Optional[Tuple[UOps, ...]] = None if op is None else (tuple(op) if isinstance(op, set) else (op,))
codegen/uops.py     :   104:                self.dtype: Optional[Tuple[DType, ...]] = None if dtype is None else (tuple(dtype) if isinstance(dtype, set) else (dtype,))
codegen/uops.py     :   105:                self.arg = arg
codegen/uops.py     :   106:                self.src: Any = None
codegen/uops.py     :   107:                if isinstance(src, list):
codegen/uops.py     :   110:                elif isinstance(src, tuple):
codegen/uops.py     :   113:                elif isinstance(src, UPat):
codegen/uops.py     :   116:                self.name: Optional[str] = name
codegen/uops.py     :   117:                self.allowed_len: int = 0 if allow_any_len or isinstance(src, UPat) or src is None else len(src)

codegen/uops.py     :   100:                class UPat:
codegen/uops.py     :   101:                  def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                               name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   112:                      self.src = [src]

codegen/uops.py     :   100:          class UPat:
codegen/uops.py     :   101:            def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                         name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   109:                self.src = list(itertools.permutations(src))

codegen/uops.py     :   152:        assert p.op is not None
codegen/uops.py     :   153:        for uop in p.op: self.pdict[(uop, p.arg)].append((p, fxn))
codegen/uopgraph.py :    84:  transcendental_folding = PatternMatcher([
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="x"),), arg=UnaryOps.EXP2), xexp2),
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=UnaryOps.LOG2), xlog2),
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=UnaryOps.SIN), xsin),
codegen/uopgraph.py :    84:  ])
codegen/uopgraph.py :   130:  constant_folder = PatternMatcher([
codegen/uopgraph.py :   130:    # CONTRACT before ALU/REDUCE/CAST
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.ALU, name="alu"),)),
codegen/uopgraph.py :   130:     lambda con, alu: UOp(alu.op, con.dtype, tuple(UOp(UOps.CONTRACT, x.dtype.vec(con.dtype.count), (x,), con.arg) for x in alu.src), alu.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.REDUCE, dtype={dtypes.half, dtypes.bfloat16, dtypes.float}, name="red"),)),
codegen/uopgraph.py :   130:     lambda con, red: UOp(UOps.REDUCE, con.dtype, (UOp(UOps.CONTRACT, con.dtype, red.src[0:1], con.arg),)+red.src[1:], red.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.CAST, dtype={dtypes.half, dtypes.bfloat16, dtypes.float}, src=(UPat(name="casted"),)),)),
codegen/uopgraph.py :   130:     lambda con, casted: UOp(UOps.CAST, con.dtype, (UOp(UOps.CONTRACT, casted.dtype.vec(con.dtype.count), (casted,), con.arg),))),
codegen/uopgraph.py :   130:    # bigint is rewritten to int32
codegen/uopgraph.py :   130:    (UPat({UOps.CONST, UOps.ALU, UOps.SPECIAL, UOps.RANGE, UOps.EXPAND}, dtype=dtypes.bigint, name="x"),
codegen/uopgraph.py :   130:     lambda x: UOp(x.op, dtypes.int32, x.src, x.arg)),
codegen/uopgraph.py :   130:    # VECTORIZE/GEP
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp(UOps.VECTORIZE).name("cast"),)).name("gep"), lambda gep, cast: cast.src[gep.arg]),
codegen/uopgraph.py :   130:    *[(UOp(UOps.VECTORIZE, dtypes.float.vec(i), tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=j)
codegen/uopgraph.py :   130:        for j in range(i))), lambda x: x) for i in [2, 4, 8]],
codegen/uopgraph.py :   130:    # tensor core with a 0 input is acc
codegen/uopgraph.py :   130:    (UOp(UOps.WMMA, src=(UOp.const(None, 0.0), UOp.var(), UOp.var('acc'))), lambda acc: acc),
codegen/uopgraph.py :   130:    (UOp(UOps.WMMA, src=(UOp.var(), UOp.const(None, 0.0), UOp.var('acc'))), lambda acc: acc),
codegen/uopgraph.py :   130:    # tensor core cleanups
codegen/uopgraph.py :   130:    (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(2))).name("expand"),))
codegen/uopgraph.py :   130:     .name("reduce_allow_any_len"), reduce_before_expand),
codegen/uopgraph.py :   130:    (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(8))).name("expand"),))
codegen/uopgraph.py :   130:     .name("reduce_allow_any_len"), reduce_before_expand),
codegen/uopgraph.py :   130:    (UOp.var("add") + UOp(UOps.WMMA).name("wmma"),
codegen/uopgraph.py :   130:      lambda add, wmma: UOp(wmma.op, wmma.dtype, (wmma.src[0], wmma.src[1], wmma.src[2]+add), wmma.arg)),
codegen/uopgraph.py :   130:    # threefry
codegen/uopgraph.py :   130:    (UOp(UOps.ALU, dtype=dtypes.uint64, src=(UOp.var("x"), UOp.var("seed")), arg=BinaryOps.THREEFRY), threefry2x32),
codegen/uopgraph.py :   130:    # arange loop folding (early)
codegen/uopgraph.py :   130:    (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(BinaryOps.MUL,
codegen/uopgraph.py :   130:      UOp.cvar("mval"), UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
codegen/uopgraph.py :   130:      UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None,0)), loop_collapse),
codegen/uopgraph.py :   130:    (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(UnaryOps.NEG,
codegen/uopgraph.py :   130:      UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
codegen/uopgraph.py :   130:      UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None, 0)),
codegen/uopgraph.py :   130:      lambda **kwargs: loop_collapse(mval=UOp.const(dtypes.int, -1), **kwargs)),
codegen/uopgraph.py :   130:    # sum collapse to mul (with possible GEP)
codegen/uopgraph.py :   130:    (UPat(UOps.PHI, src=(UPat(UOps.DEFINE_ACC, name="phi_input", src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),
codegen/uopgraph.py :   130:                         UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
codegen/uopgraph.py :   130:    (UPat(UOps.PHI, src=(UPat(UOps.GEP, name="phi_input", src=(UPat(UOps.DEFINE_ACC, src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),)),
codegen/uopgraph.py :   130:                         UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
codegen/uopgraph.py :   130:    # deal with UNMUL
codegen/uopgraph.py :   130:    (UOp.cvar('c1') * UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v'))), lambda c1,c2,v: v if c1.arg == c2.arg else None),
codegen/uopgraph.py :   130:    (UOp.cvar('c1') * (UOp.var('add') + UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v')))),
codegen/uopgraph.py :   130:      lambda c1, add, c2, v: (add*c1+v) if c1.arg == c2.arg else None),
codegen/uopgraph.py :   130:    (UOp(UOps.UNMUL, src=(UOp.const(None, 0).name('zero'), UOp.var())), lambda zero: zero),
codegen/uopgraph.py :   130:    (UOp(UOps.UNMUL).name('unmul').cast().name('root'), lambda root,unmul: UOp(UOps.UNMUL, root.dtype, (unmul.src[0].cast(root.dtype), unmul.src[1]))),
codegen/uopgraph.py :   130:    # indexing (with a multiply offset)!
codegen/uopgraph.py :   130:    (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).cast()*
codegen/uopgraph.py :   130:      UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"),
codegen/uopgraph.py :   130:      lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
codegen/uopgraph.py :   130:    (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).where(
codegen/uopgraph.py :   130:      UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"), UOp.const(None, 0.0)),
codegen/uopgraph.py :   130:      lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
codegen/uopgraph.py :   130:    # other arange folders
codegen/uopgraph.py :   130:    (UOp.cvar("c1") - (UOp.var("x") + UOp.cvar("c2")), lambda c1, c2, x: (c1-c2)-x),  # c1 - (x + c2) -> (c1-c2) - x
codegen/uopgraph.py :   130:    # max on special can go away (TODO: special should be variable, same thing applies)
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')), lambda c,s: c if (s.arg[2]-1) <= c.arg else None),
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if 0 >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.arg[2]-1+c2.arg) >= c.arg else None),
codegen/uopgraph.py :   130:    # max on range can go away (ugh: copy of SPECIAL, and with/without const)
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')), lambda c,s: s if s.src[0].arg >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if s.src[0].arg >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s'))), lambda c,s: -s if -(s.src[1].arg-1) >= c.arg else None),
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.src[1].arg-1+c2.arg) >= c.arg else None),
codegen/uopgraph.py :   130:    # const rules
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp.cvar("c"),)).name("root"), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CAST, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.VECTORIZE, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    # a phi on a DEFINE_ACC without loops or a CONST is a noop. this is for correctness, not just speed
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC).name("acc"), UOp.var("acc"))), lambda acc: UOp.cast(acc.src[0], acc.dtype)),
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)), UOp.var("x"))), lambda x: x),
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp.cvar(), UOp.var("x"))), lambda x: x),
codegen/uopgraph.py :   130:    # a DEFINE_ACC without inputs is a const + GEP on a const is the const
codegen/uopgraph.py :   130:    (UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)).name("root"), lambda root: UOp.cast(root.src[0], root.dtype)),
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp.cvar("x"),)).name("root"), lambda root,x: UOp.const(root.dtype, x.arg)),
codegen/uopgraph.py :   130:    # max -2147483648
codegen/uopgraph.py :   130:    (UOp.max(UOp.var('x'), UOp.const(dtypes.int, -2147483648)), lambda x: x),
codegen/uopgraph.py :   130:    # bool < False is always false, True < bool is always false
codegen/uopgraph.py :   130:    (UOp.var().lt(UOp.const(dtypes.bool, False)), lambda: UOp.const(dtypes.bool, False)),
codegen/uopgraph.py :   130:    (UOp.const(dtypes.bool, True).lt(UOp.var()), lambda: UOp.const(dtypes.bool, False)),
codegen/uopgraph.py :   130:    # a conditional with the same results either way is a noop, also fold const conditionals
codegen/uopgraph.py :   130:    (UOp.var().where(UOp.var("val"), UOp.var("val")), lambda val: val),
codegen/uopgraph.py :   130:    (UOp.cvar('gate').where(UOp.var('c0'), UOp.var('c1')), lambda gate, c0, c1: c0 if gate.arg else c1),
codegen/uopgraph.py :   130:    # ** constant folding **
codegen/uopgraph.py :   130:    (UPat(UOps.ALU, name="root", src=UPat(UOps.CONST)), lambda root: UOp.const(root.dtype, exec_alu(root.arg, root.dtype, [x.arg for x in root.src]))),
codegen/uopgraph.py :   130:    # ** self folding **
codegen/uopgraph.py :   130:    (-(-UOp.var('x')), lambda x: x),    # -(-x) -> x
codegen/uopgraph.py :   130:    (UOp.var('x') + 0, lambda x: x),    # x+0 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') * 1, lambda x: x),    # x*1 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') * -1, lambda x: -x),  # x*-1 -> -x
codegen/uopgraph.py :   130:    (UOp.var('x') // UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x//x -> 1
codegen/uopgraph.py :   130:    (UOp.var('x') // 1, lambda x: x),   # x//1 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') // -1, lambda x: -x), # x//-1 -> -x
codegen/uopgraph.py :   130:    (UOp.var('x') / UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x/x -> 1
codegen/uopgraph.py :   130:    (UOp.var('x') / UOp.cvar('c'), lambda x,c: x*exec_alu(UnaryOps.RECIP, c.dtype, [c.arg])),    # x/c -> x*(1/c)
codegen/uopgraph.py :   130:    (UOp.var('x', dtype=dtypes.bool).max(UOp.const(dtypes.bool, False)), lambda x: x),  # max(x, False) -> x
codegen/uopgraph.py :   130:    # ** zero folding **
codegen/uopgraph.py :   130:    #x*0 -> 0 or 0*x -> 0
codegen/uopgraph.py :   130:    #if x is nan or inf it should render the nan value.
codegen/uopgraph.py :   130:    # NOTE: this can be wrong for loaded NaN
codegen/uopgraph.py :   130:    (UOp.var('x') * 0, lambda x: UOp.const(x.dtype, float('nan') if isinstance(x.arg, float) and (math.isnan(x.arg) or math.isinf(x.arg)) else 0)),
codegen/uopgraph.py :   130:    (UOp.var('x') - UOp.var('x'), lambda x: UOp.const(x.dtype, 0)),   # x-x -> 0
codegen/uopgraph.py :   130:    # ** load/store folding **
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.load(UOp.var("buf"), UOp.var("idx"))), lambda buf,idx:UOp(UOps.NOOP)),
codegen/uopgraph.py :   130:    # ** two stage add/sub folding **
codegen/uopgraph.py :   130:    ((UOp.var('x') + UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, c2.arg]))),
codegen/uopgraph.py :   130:    ((UOp.var('x') - UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c2.arg, -c1.arg]))),
codegen/uopgraph.py :   130:    # *** rules from symbolic ***
codegen/uopgraph.py :   130:    # mod divides
codegen/uopgraph.py :   130:    ((UOp.cvar('c')*UOp.var('x')) % UOp.cvar('c'), lambda x,c: x.const(0)),
codegen/uopgraph.py :   130:    (((UOp.cvar('c')*UOp.var('x'))+UOp.var('x2')) % UOp.cvar('c'), lambda x,c,x2: x2%c),
codegen/uopgraph.py :   130:    # two stage mul, (x*c1)*c2 = x*(c1*c2)
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.cvar("c1")) * UOp.cvar("c2"), lambda x,c1,c2: x*UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c1.arg, c2.arg]))),
codegen/uopgraph.py :   130:    # -(x+y) -> -x + -y
codegen/uopgraph.py :   130:    #(-(UOp.var("x") + UOp.var("y")), lambda x,y: (-x)+(-y)),
codegen/uopgraph.py :   130:    # x%1 -> 0
codegen/uopgraph.py :   130:    (UOp.var("x") % UOp.const(None, 1), lambda x: UOp.const(x.dtype, 0)),
codegen/uopgraph.py :   130:    # (x*c0)+(x*c1) -> x*(c0+c1)
codegen/uopgraph.py :   130:    (UOp.var("x") * UOp.cvar("c0") + UOp.var("x") * UOp.cvar("c1"), lambda x,c0,c1: x*exec_alu(BinaryOps.ADD, x.dtype, [c0.arg, c1.arg])),
codegen/uopgraph.py :   130:    # (x*c0)+(y*c0) -> (x+y)*c0
codegen/uopgraph.py :   130:    #((UOp.var("x") * UOp.cvar("c0")) + (UOp.var("y") * UOp.cvar("c0")), lambda x,y,c0: c0*(x+y)),
codegen/uopgraph.py :   130:    # (x*c0)//c0 -> x
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.cvar("c0")) // UOp.cvar("c0"), lambda x,c0: x if c0.arg != 0 else None),
codegen/uopgraph.py :   130:    # (x*x2)/x2 -> x
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.var("x2")) / UOp.var("x2"), lambda x,x2: x),
codegen/uopgraph.py :   130:    # (x//c0)//c1 -> x//(c0*c1)
codegen/uopgraph.py :   130:    ((UOp.var("x") // UOp.cvar("c0")) // UOp.cvar("c1"), lambda x,c0,c1: x//UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c0.arg, c1.arg]))),
codegen/uopgraph.py :   130:    # (x/x1)/x2 -> x/(x1*x2)
codegen/uopgraph.py :   130:    ((UOp.var("x") / UOp.var("x2")) / UOp.var("x3"), lambda x,x2,x3: x/(x2*x3)),
codegen/uopgraph.py :   130:    # c0 + x < c1 -> x < c1 - c0
codegen/uopgraph.py :   130:    ((UOp.cvar("c0") + UOp.var("x")).lt(UOp.cvar("c1")),
codegen/uopgraph.py :   130:      lambda x,c0,c1: UOp.lt(x, UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, -c0.arg])))),
codegen/uopgraph.py :   130:    # (x+x*c0)-> x*(c0+1)
codegen/uopgraph.py :   130:    (UOp.var("x") + UOp.var("x") * UOp.cvar("c0"), lambda x,c0: x*UOp.const(x.dtype, c0.arg+1)),
codegen/uopgraph.py :   130:    # x!=0 -> (bool)x
codegen/uopgraph.py :   130:    (UOp.var("x").ne(0), lambda x: x.cast(dtypes.bool)),
codegen/uopgraph.py :   130:    # bool != 1 -> not bool
codegen/uopgraph.py :   130:    (UOp.var("x", dtype=dtypes.bool).ne(1), lambda x: -x),
codegen/uopgraph.py :   130:    # TODO: can do the invert of this (flip alt/load) when we fix double ops
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.alu(TernaryOps.WHERE, UOp.var("gate"), UOp.var("alt"), UOp.load(UOp.var("buf"), UOp.var("idx")))),
codegen/uopgraph.py :   130:     lambda buf, idx, gate, alt: UOp.store(buf, idx, alt, gate)),
codegen/uopgraph.py :   130:    # VECTORIZE-PHI-GEP -> PHI-VECTORIZE
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(4))).name("root"),
codegen/uopgraph.py :   130:     lambda root, val, v0, v1, v2, v3: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1, v2, v3))))),
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(2))).name("root"),
codegen/uopgraph.py :   130:     lambda root, val, v0, v1: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1))))),
codegen/uopgraph.py :   130:    # NEG/CMPLT -> CMPLT
codegen/uopgraph.py :   130:    (UOp.lt(-UOp.var('x'), UOp.cvar('c', dtypes.int)), lambda c,x: UOp.lt(UOp.const(c.dtype, -c.arg), x)),
codegen/uopgraph.py :   130:    # cast NOOP (NOTE: it's str to deal with PtrDType)
codegen/uopgraph.py :   130:    (UOp(UOps.CAST).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
codegen/uopgraph.py :   130:    # fold gated LOAD/STORE
codegen/uopgraph.py :   130:    (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var")), lambda buf,idx,var: UOp.load(buf, idx, dtype=var.dtype)),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var"), UOp.var("barrier")),
codegen/uopgraph.py :   130:     lambda buf,idx,var,barrier: UOp.load(buf, idx, barrier, dtype=var.dtype)),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var")), lambda var: var),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var"), UOp.var()), lambda var: var),
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.var("val"), UOp.const(dtypes.bool, True)), UOp.store),
codegen/uopgraph.py :   130:    (UOp.store(UOp.var(), UOp.var(), UOp.var(), UOp.const(dtypes.bool, False)), lambda: UOp(UOps.NOOP)),
codegen/uopgraph.py :   130:    # remove NOOPs from SINK
codegen/uopgraph.py :   130:    (UOp(UOps.SINK).name("root"),
codegen/uopgraph.py :   130:      lambda root: UOp(UOps.SINK, root.dtype, a, root.arg) if len(a:=tuple(x for x in root.src if x.op is not UOps.NOOP)) != len(root.src) else None),
codegen/uopgraph.py :   130:  ])
codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    86:      @staticmethod
codegen/uops.py     :    86:      def cvar(name:Optional[str]=None, dtype:Optional[DType]=None): return UOp(UOps.CONST, dtype=dtype).name(name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    69:      def where(self, x, y): return UOp.alu(TernaryOps.WHERE, self, x, y)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    55:      def __mul__(self, x): return UOp.alu(BinaryOps.MUL, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    48:      def cast(self, dtype=None): return UOp(UOps.CAST, dtype, (self,))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    64:      def eq(self, x): return -self.ne(x)

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    63:        def ne(self, x): return UOp.alu(BinaryOps.CMPNE, self, ufix(self.dtype, x))

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    51:        def __neg__(self): return UOp.alu(UnaryOps.NEG, self)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    54:      def __sub__(self, x): return UOp.alu(BinaryOps.ADD, self, -ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    67:      def max(self, x): return UOp.alu(BinaryOps.MAX, self, x)

codegen/uops.py     :   100:    class UPat:
codegen/uops.py     :   101:      def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                   name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   115:          self.src = [itertools.repeat(src)]

dtype.py            :    38:            class dtypes:
dtype.py            :    44:              @staticmethod
dtype.py            :    44:              def is_unsigned(x: DType) -> bool: return x.scalar() in (dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64)

dtype.py            :    38:          class dtypes:
dtype.py            :    40:            @staticmethod
dtype.py            :    40:            def is_float(x: DType) -> bool: return x.scalar() in (dtypes.float16, dtypes.bfloat16, dtypes.float32, dtypes.float64)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    65:      def lt(self, x): return UOp.alu(BinaryOps.CMPLT, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    57:      def __floordiv__(self, x): return UOp.alu(BinaryOps.IDIV, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    58:      def __truediv__(self, x): return UOp.alu(BinaryOps.MUL, self, UOp.alu(UnaryOps.RECIP, ufix(self.dtype, x)))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    80:      @staticmethod
codegen/uops.py     :    80:      def load(*src:UOp, dtype:Optional[DType]=None, **kwargs): return UOp(UOps.LOAD, dtype, tuple(src)+tuple(kwargs.values()))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    82:      @staticmethod
codegen/uops.py     :    82:      def store(*src:UOp, **kwargs): return UOp(UOps.STORE, None, tuple(src)+tuple(kwargs.values()))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    59:      def __mod__(self, x): return UOp.alu(BinaryOps.MOD, self, ufix(self.dtype, x))

codegen/uopgraph.py :   345:  acc_number = 0
codegen/uopgraph.py :   394:  expander = PatternMatcher([
codegen/uopgraph.py :   394:    (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST, UOps.GEP, UOps.WMMA, UOps.LOAD, UOps.STORE,
codegen/uopgraph.py :   394:           UOps.VECTORIZE, UOps.REDUCE, UOps.EXPAND, UOps.IF}, name="root"), do_expand),
codegen/uopgraph.py :   394:    (UOp(UOps.REDUCE).name("root"), do_reduce_with_expand),
codegen/uopgraph.py :   394:    (UOp(UOps.CONTRACT).name("con"), do_contract),
codegen/uopgraph.py :   394:    # remove EXPANDs from SINK
codegen/uopgraph.py :   394:    (UOp(UOps.SINK).name("root"),
codegen/uopgraph.py :   394:     lambda root: UOp(UOps.SINK, root.dtype, a, root.arg)
codegen/uopgraph.py :   394:      if len(a:=tuple(flatten(x.src if x.op is UOps.EXPAND else (x,) for x in root.src))) != len(root.src) else None),
codegen/uopgraph.py :   394:    # BARRIERs aren't actually expanded
codegen/uopgraph.py :   394:    (UOp(UOps.BARRIER, src=(UOp(UOps.EXPAND).name("ex"),)), lambda ex: UOp(UOps.EXPAND, None, (UOp(UOps.BARRIER, None, ex.src),)*len(ex.src), ex.arg)),
codegen/uopgraph.py :   394:    # empty EXPAND is NOOP
codegen/uopgraph.py :   394:    (UOp(UOps.EXPAND, src=(UOp.var('x'),), arg=()), lambda x: x),
codegen/uopgraph.py :   394:    # no ALU on vectorized dtypes
codegen/uopgraph.py :   394:    (UPat({UOps.ALU, UOps.CAST}, name="alu"), no_vectorized_alu),
codegen/uopgraph.py :   394:  ])
codegen/uopgraph.py :   432:    class UOpGraph:
codegen/uopgraph.py :   464:      cnt = 0

renderer/__init__.py:    10:    @dataclass(frozen=True)
renderer/__init__.py:    10:    class TensorCore: # D = A * B + C, A is (M x K), B is (K x N), C and D are (M x N)
renderer/__init__.py:    11:      dims: Tuple[int,int,int] # N, M, K
renderer/__init__.py:    12:      dtype_in: DType # dtype for A and B
renderer/__init__.py:    13:      dtype_out: DType # dtype for C and D
renderer/__init__.py:    14:      threads: List[Tuple[int,int]] # list of (TC dim,amt) that construct the warp thread structure
renderer/__init__.py:    15:      thread_local_sizes: List[List[int]] # in each thread, the number of elements stored in registers for each TC dim

renderer/__init__.py:    19:    @dataclass(frozen=True)
renderer/__init__.py:    19:    class Program:
renderer/__init__.py:    20:      name:str
renderer/__init__.py:    21:      src:str
renderer/__init__.py:    22:      dname:str
renderer/__init__.py:    23:      global_size:Optional[List[int]]=None
renderer/__init__.py:    24:      local_size:Optional[List[int]]=None
renderer/__init__.py:    25:      uops:Optional[UOpGraph]=None
renderer/__init__.py:    26:      op_estimate:sint=0
renderer/__init__.py:    27:      mem_estimate:sint=0

renderer/__init__.py:    46:    class Renderer:
renderer/__init__.py:    47:      device: str = ""
renderer/__init__.py:    48:      suffix: str = ""
renderer/__init__.py:    50:      supports_float4: bool = True
renderer/__init__.py:    51:      has_local: bool = True
renderer/__init__.py:    52:      has_shared: bool = True
renderer/__init__.py:    54:      global_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
renderer/__init__.py:    55:      local_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
renderer/__init__.py:    56:      shared_max: int = 32768
renderer/__init__.py:    57:      tensor_cores: List[TensorCore] = []

device.py           :    13:    class _Device:

device.py           :    39:  Device = _Device()

device.py           :    13:    class _Device:
device.py           :    14:      def __init__(self) -> None: self._devices: List[str] = [x.stem[len("ops_"):].upper() for x in (pathlib.Path(__file__).parent/"runtime").iterdir() if x.stem.startswith("ops_")]  # noqa: E501

device.py           :    44:    @dataclass(frozen=True, eq=True)
device.py           :    44:    class BufferOptions:
device.py           :    45:      image: Optional[ImageDType] = None
device.py           :    46:      uncached: bool = False
device.py           :    47:      cpu_access: bool = False
device.py           :    48:      host: bool = False
device.py           :    49:      nolru: bool = False

device.py           :    51:    class Buffer:

device.py           :   131:    # TODO: size, dest, src are the same type. can we enforce this?
device.py           :   131:    class Allocator:

device.py           :   142:    class LRUAllocator(Allocator):  # pylint: disable=abstract-method
device.py           :   143:      """
device.py           :   143:      The LRU Allocator is responsible for caching buffers.
device.py           :   143:      It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.
device.py           :   143:      """

device.py           :   162:    class _MallocAllocator(LRUAllocator):

device.py           :   169:  MallocAllocator = _MallocAllocator()

device.py           :   142:    class LRUAllocator(Allocator):  # pylint: disable=abstract-method
device.py           :   147:      def __init__(self): self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)

device.py           :   173:    class CompileError(Exception): pass

device.py           :   175:    class Compiler:

device.py           :   185:    class Compiled:

device.py           :   217:    class HWCommandQueue:
device.py           :   218:      """
device.py           :   218:      A base class for hardware command queues in the HCQ (Hardware Command Queue) API.
device.py           :   218:      Both compute and copy queues should have the following commands implemented.
device.py           :   218:      """

device.py           :   199:      def hcq_command(func):
device.py           :   215:        return __wrapper

device.py           :   301:    class HWComputeQueue(HWCommandQueue):

device.py           :   338:    class HWCopyQueue(HWCommandQueue):

device.py           :   366:    class HCQSignal:

device.py           :   413:    class HCQProgram:

device.py           :   418:    class HCQCompiled(Compiled):
device.py           :   419:      """
device.py           :   419:      A base class for devices compatible with the HCQ (Hardware Command Queue) API.
device.py           :   419:      """

device.py           :   468:    # Protocol for hcq compatible allocators for allocated buffers to contain VA address and it's size.
device.py           :   468:    class HCQBuffer(Protocol): va_addr:int; size:int # noqa: E702

device.py           :   470:    class HCQAllocator(LRUAllocator): # pylint: disable=abstract-method
device.py           :   471:      """
device.py           :   471:      A base allocator class compatible with the HCQ (Hardware Command Queue) API.
device.py           :   471:    
device.py           :   471:      This class implements basic copy operations following the HCQ API, utilizing both `HWComputeQueue` and `HWCopyQueue`.
device.py           :   471:      """

lazy.py             :    11:  lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()
lazy.py             :    24:  view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}

lazy.py             :    25:    class LazyBuffer:

multi.py            :    50:    class MultiLazyBuffer:

codegen/lowerer.py  :    15:  render_ops: Any = { NumNode: lambda self, ops, ctx: UOp.const(dtypes.bigint, self.b),
codegen/lowerer.py  :    15:                      MulNode: lambda self, ops, ctx: self.a.render(ops, ctx)*variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      DivNode: lambda self, ops, ctx: self.a.render(ops, ctx)//variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      ModNode: lambda self, ops, ctx: self.a.render(ops, ctx)%variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      LtNode: lambda self, ops, ctx: self.a.render(ops, ctx).lt(variable_to_uop(self.b, ctx)),
codegen/lowerer.py  :    15:    Variable: lambda self,ops,ctx: ctx[self] if ctx is not None and self in ctx else UOp(UOps.DEFINE_VAR, dtypes.int32, (), self),
codegen/lowerer.py  :    15:    SumNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a+b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)),
codegen/lowerer.py  :    15:    AndNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a*b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)) }
codegen/lowerer.py  :    24:  if getenv("UOP_IS_SYMBOLIC"):
codegen/lowerer.py  :    90:    class IndependentLowerer:

codegen/kernel.py   :    22:    class OptOps(Enum):
codegen/kernel.py   :    23:      TC = auto(); UPCAST = auto(); UPCASTMID = auto(); UNROLL = auto(); LOCAL = auto() # noqa: E702
codegen/kernel.py   :    24:      GROUP = auto(); GROUPTOP = auto(); NOLOCALS = auto(); PADTO = auto(); MERGE = auto(); SWAP = auto() # noqa: E702

codegen/kernel.py   :    27:    class KernelOptError(Exception): pass

codegen/kernel.py   :    33:    @dataclass(frozen=True, order=True)
codegen/kernel.py   :    33:    class Opt:
codegen/kernel.py   :    34:      op: OptOps
codegen/kernel.py   :    35:      axis: Optional[int] = None
codegen/kernel.py   :    36:      amt: Optional[int] = None

codegen/kernel.py   :    45:    @dataclass
codegen/kernel.py   :    45:    class TensorCoreOptions:
codegen/kernel.py   :    46:      axes: Tuple[int, ...] # the location of the original N and M axes if still in the shape
codegen/kernel.py   :    47:      axes_exist: Tuple[bool, ...] # true if the original N and M axes are still in the shape
codegen/kernel.py   :    48:      axis_pads: Tuple[Tuple[int, int], ...]

codegen/kernel.py   :    56:    class Kernel:
codegen/kernel.py   :   638:      kernel_cnt: Final[DefaultDict[str, int]] = defaultdict(int)

engine/graph.py     :    11:  with contextlib.suppress(ImportError): import networkx as nx
engine/graph.py     :    15:  if DEBUG >= 2:

helpers.py          :    91:    class ContextVar:
helpers.py          :   101:      def __ge__(self, x): return self.value >= x

engine/graph.py     :    27:  G:Any = None
engine/graph.py     :    34:  counts: DefaultDict[type, int] = defaultdict(int)
engine/graph.py     :    47:  top_colors = {MetaOps: '#FFFFa0', UnaryOps: "#c0c0c0", ReduceOps: "#FFA0A0", BinaryOps: "#c0c0c0",
engine/graph.py     :    47:                TernaryOps: "#c0c0c0", BufferOps: '#a0a0ff'}
engine/graph.py     :    77:  graph_uops_cnt = 0
engine/schedule.py  :    16:  sys.setrecursionlimit(10000)
engine/schedule.py  :    19:  logops = open(getenv("LOGOPS", ""), "a") if getenv("LOGOPS", "") else None
engine/schedule.py  :    24:    @dataclass(frozen=True)
engine/schedule.py  :    24:    class ScheduleItem:
engine/schedule.py  :    25:      ast: LazyOp
engine/schedule.py  :    26:      bufs: Tuple[Buffer, ...]
engine/schedule.py  :    27:      metadata: Optional[List[Metadata]] = None

engine/schedule.py  :   309:  SCHEDULES: List = []
engine/realize.py   :    15:  logkerns, logkerns_level = open(getenv("LOGKERNS", ""), "a") if getenv("LOGKERNS", "") else None, getenv("LOGKERNS_LEVEL", 1)
engine/realize.py   :    63:    class Runner:

engine/realize.py   :    73:    class CompiledRunner(Runner):

engine/realize.py   :   100:    class CustomOp(Runner):

engine/realize.py   :   106:    class EmptyOp(Runner):

engine/realize.py   :   110:    class ViewOp(Runner):

engine/realize.py   :   115:    class BufferCopy(Runner):

engine/realize.py   :   138:    class BufferXfer(BufferCopy):

engine/realize.py   :   147:  method_cache: Dict[Tuple[str, LazyOp, int, bool], CompiledRunner] = {}

engine/realize.py   :   165:    @dataclass(frozen=True)
engine/realize.py   :   165:    class ExecItem:
engine/realize.py   :   166:      prg: Runner
engine/realize.py   :   167:      bufs: List[Optional[Buffer]]
engine/realize.py   :   168:      metadata: Optional[List[Metadata]] = None

engine/realize.py   :   214:  capturing: List = []  # put classes with an add method in here
tensor.py           :    23:    class Function:

function.py         :     1:  """This is where the forwards and backwards passes live."""

function.py         :    11:    class Contiguous(Function):

function.py         :    15:    class ContiguousBackward(Function):

function.py         :    19:    class Cast(Function):

function.py         :    28:    class Neg(Function):

function.py         :    32:    class Reciprocal(Function):

function.py         :    39:    class Sin(Function):

function.py         :    48:    # NOTE: maximum(x, 0) behaves differently where x=0
function.py         :    48:    class Relu(Function):

function.py         :    56:    class Log(Function):

function.py         :    63:    class Exp(Function):

function.py         :    70:    class Sqrt(Function):

function.py         :    81:    # NOTE: the implicit derivative of sigmoid is not stable
function.py         :    81:    # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e
function.py         :    81:    # TODO: have the backend automatically find this
function.py         :    81:    class Sigmoid(Function):

function.py         :    89:    class Sign(Function):

function.py         :    98:    class Less(Function):

function.py         :   102:    class Neq(Function):

function.py         :   106:    class Xor(Function):

function.py         :   109:    class BitwiseAnd(Function):

function.py         :   112:    class BitwiseOr(Function):

function.py         :   115:    class Threefry(Function):

function.py         :   118:    class Add(Function):

function.py         :   125:    class Mul(Function):

function.py         :   134:    class Div(Function):

function.py         :   145:    class Where(Function):

function.py         :   157:    class Sum(Function):

function.py         :   164:    class Max(Function):

function.py         :   179:    # NOTE: this is sum in reverse
function.py         :   179:    class Expand(Function):

function.py         :   187:    class Reshape(Function):

function.py         :   194:    class Permute(Function):

function.py         :   201:    class Pad(Function):

function.py         :   208:    class Shrink(Function):

function.py         :   215:    class Flip(Function):

tensor.py           :    92:    class Tensor:
tensor.py           :    93:      """
tensor.py           :    93:      A `Tensor` is a multi-dimensional matrix containing elements of a single data type.
tensor.py           :    93:    
tensor.py           :    93:      ```python exec="true" session="tensor"
tensor.py           :    93:      from tinygrad import Tensor, dtypes, nn
tensor.py           :    93:      import numpy as np
tensor.py           :    93:      import math
tensor.py           :    93:      np.set_printoptions(precision=4)
tensor.py           :    93:      ```
tensor.py           :    93:      """
tensor.py           :   103:      __slots__ = "lazydata", "requires_grad", "grad", "_ctx"
tensor.py           :   104:      __deletable__ = ('_ctx',)
tensor.py           :   105:      training: ClassVar[bool] = False
tensor.py           :   106:      no_grad: ClassVar[bool] = False

tensor.py           :    92:      class Tensor:
tensor.py           :   154:        class train(ContextDecorator):

tensor.py           :    92:      class Tensor:
tensor.py           :   159:        class inference_mode(ContextDecorator):

tensor.py           :   371:      _seed: int = int(time.time())
tensor.py           :   372:      _rng_counter: Optional[Tensor] = None

tensor.py           :  3123:  for device in Device._devices: setattr(Tensor, f"{device.lower()}", functools.partialmethod(Tensor.to, device))
tensor.py           :  3125:  if IMAGE:

helpers.py          :    91:    class ContextVar:
helpers.py          :   100:      def __bool__(self): return bool(self.value)

tensor.py           :  3166:  if TRACEMETA >= 1:
tensor.py           :  3167:    for name, fn in inspect.getmembers(Tensor, inspect.isfunction):
tensor.py           :  3168:      if name in ["__class__", "__init__", "__repr__", "backward", "sequential"]: continue
tensor.py           :  3169:      setattr(Tensor, name, functools.wraps(fn)(_metadata_wrapper(fn)))

tensor.py           :  3138:    def _metadata_wrapper(fn):
tensor.py           :  3164:      return _wrapper

nn/optim.py         :     7:    class Optimizer:
nn/optim.py         :     8:      """
nn/optim.py         :     8:      Base class for all optimizers.
nn/optim.py         :     8:      """

nn/optim.py         :    45:    class OptimizerGroup(Optimizer):
nn/optim.py         :    46:      """
nn/optim.py         :    46:      Combines multiple optimizers into one.
nn/optim.py         :    46:      """

nn/optim.py         :    67:    class LARS(Optimizer):
nn/optim.py         :    68:      """
nn/optim.py         :    68:      Layer-wise Adaptive Rate Scaling (LARS) optimizer with optional momentum and weight decay.
nn/optim.py         :    68:    
nn/optim.py         :    68:      - Described: https://paperswithcode.com/method/lars
nn/optim.py         :    68:      - Paper: https://arxiv.org/abs/1708.03888v3
nn/optim.py         :    68:      """

nn/optim.py         :   119:    class LAMB(Optimizer):
nn/optim.py         :   120:      """
nn/optim.py         :   120:      LAMB optimizer with optional weight decay.
nn/optim.py         :   120:    
nn/optim.py         :   120:      - Described: https://paperswithcode.com/method/lamb
nn/optim.py         :   120:      - Paper: https://arxiv.org/abs/1904.00962
nn/optim.py         :   120:      """

nn/state.py         :     9:  safe_dtypes = {"BOOL":dtypes.bool, "I8":dtypes.int8, "U8":dtypes.uint8, "I16":dtypes.int16, "U16":dtypes.uint16, "I32":dtypes.int, "U32":dtypes.uint,
nn/state.py         :     9:                 "I64":dtypes.int64, "U64":dtypes.uint64, "F16":dtypes.float16, "BF16":dtypes.bfloat16, "F32":dtypes.float32, "F64":dtypes.float64}
nn/state.py         :    11:  inverse_safe_dtypes = {v:k for k,v in safe_dtypes.items()}
nn/__init__.py      :     7:    class BatchNorm:
nn/__init__.py      :     8:      """
nn/__init__.py      :     8:      Applies Batch Normalization over a 2D or 3D input.
nn/__init__.py      :     8:    
nn/__init__.py      :     8:      - Described: https://paperswithcode.com/method/batch-normalization
nn/__init__.py      :     8:      - Paper: https://arxiv.org/abs/1502.03167v3
nn/__init__.py      :     8:    
nn/__init__.py      :     8:      See: `Tensor.batchnorm`
nn/__init__.py      :     8:    
nn/__init__.py      :     8:      ```python exec="true" session="tensor"
nn/__init__.py      :     8:      from tinygrad import Tensor, dtypes, nn
nn/__init__.py      :     8:      import numpy as np
nn/__init__.py      :     8:      np.set_printoptions(precision=4)
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:    
nn/__init__.py      :     8:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :     8:      norm = nn.BatchNorm(3)
nn/__init__.py      :     8:      t = Tensor.rand(2, 3, 4, 4)
nn/__init__.py      :     8:      print(t.mean().item(), t.std().item())
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :     8:      t = norm(t)
nn/__init__.py      :     8:      print(t.mean().item(), t.std().item())
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:      """

nn/__init__.py      :    62:  BatchNorm2d = BatchNorm3d = BatchNorm

nn/__init__.py      :    82:    class Conv2d:
nn/__init__.py      :    83:      """
nn/__init__.py      :    83:      Applies a 2D convolution over an input signal composed of several input planes.
nn/__init__.py      :    83:    
nn/__init__.py      :    83:      See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d
nn/__init__.py      :    83:    
nn/__init__.py      :    83:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :    83:      conv = nn.Conv2d(1, 1, 3)
nn/__init__.py      :    83:      t = Tensor.rand(1, 1, 4, 4)
nn/__init__.py      :    83:      print(t.numpy())
nn/__init__.py      :    83:      ```
nn/__init__.py      :    83:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :    83:      t = conv(t)
nn/__init__.py      :    83:      print(t.numpy())
nn/__init__.py      :    83:      ```
nn/__init__.py      :    83:      """

nn/__init__.py      :   126:    class ConvTranspose2d(Conv2d):
nn/__init__.py      :   127:      """
nn/__init__.py      :   127:      Applies a 2D transposed convolution operator over an input image.
nn/__init__.py      :   127:    
nn/__init__.py      :   127:      See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d
nn/__init__.py      :   127:    
nn/__init__.py      :   127:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   127:      conv = nn.ConvTranspose2d(1, 1, 3)
nn/__init__.py      :   127:      t = Tensor.rand(1, 1, 4, 4)
nn/__init__.py      :   127:      print(t.numpy())
nn/__init__.py      :   127:      ```
nn/__init__.py      :   127:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   127:      t = conv(t)
nn/__init__.py      :   127:      print(t.numpy())
nn/__init__.py      :   127:      ```
nn/__init__.py      :   127:      """

nn/__init__.py      :   152:    class Linear:
nn/__init__.py      :   153:      """
nn/__init__.py      :   153:      Applies a linear transformation to the incoming data.
nn/__init__.py      :   153:    
nn/__init__.py      :   153:      See: https://pytorch.org/docs/stable/generated/torch.nn.Linear
nn/__init__.py      :   153:    
nn/__init__.py      :   153:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   153:      lin = nn.Linear(3, 4)
nn/__init__.py      :   153:      t = Tensor.rand(2, 3)
nn/__init__.py      :   153:      print(t.numpy())
nn/__init__.py      :   153:      ```
nn/__init__.py      :   153:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   153:      t = lin(t)
nn/__init__.py      :   153:      print(t.numpy())
nn/__init__.py      :   153:      ```
nn/__init__.py      :   153:      """

nn/__init__.py      :   176:    class GroupNorm:
nn/__init__.py      :   177:      """
nn/__init__.py      :   177:      Applies Group Normalization over a mini-batch of inputs.
nn/__init__.py      :   177:    
nn/__init__.py      :   177:      - Described: https://paperswithcode.com/method/group-normalization
nn/__init__.py      :   177:      - Paper: https://arxiv.org/abs/1803.08494v3
nn/__init__.py      :   177:    
nn/__init__.py      :   177:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   177:      norm = nn.GroupNorm(2, 12)
nn/__init__.py      :   177:      t = Tensor.rand(2, 12, 4, 4) * 2 + 1
nn/__init__.py      :   177:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   177:      ```
nn/__init__.py      :   177:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   177:      t = norm(t)
nn/__init__.py      :   177:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   177:      ```
nn/__init__.py      :   177:      """

nn/__init__.py      :   207:    class InstanceNorm:
nn/__init__.py      :   208:      """
nn/__init__.py      :   208:      Applies Instance Normalization over a mini-batch of inputs.
nn/__init__.py      :   208:    
nn/__init__.py      :   208:      - Described: https://paperswithcode.com/method/instance-normalization
nn/__init__.py      :   208:      - Paper: https://arxiv.org/abs/1607.08022v3
nn/__init__.py      :   208:    
nn/__init__.py      :   208:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   208:      norm = nn.InstanceNorm(3)
nn/__init__.py      :   208:      t = Tensor.rand(2, 3, 4, 4) * 2 + 1
nn/__init__.py      :   208:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   208:      ```
nn/__init__.py      :   208:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   208:      t = norm(t)
nn/__init__.py      :   208:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   208:      ```
nn/__init__.py      :   208:      """

nn/__init__.py      :   234:    class LayerNorm:
nn/__init__.py      :   235:      """
nn/__init__.py      :   235:      Applies Layer Normalization over a mini-batch of inputs.
nn/__init__.py      :   235:    
nn/__init__.py      :   235:      - Described: https://paperswithcode.com/method/layer-normalization
nn/__init__.py      :   235:      - Paper: https://arxiv.org/abs/1607.06450v1
nn/__init__.py      :   235:    
nn/__init__.py      :   235:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   235:      norm = nn.LayerNorm(3)
nn/__init__.py      :   235:      t = Tensor.rand(2, 5, 3) * 2 + 1
nn/__init__.py      :   235:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   235:      ```
nn/__init__.py      :   235:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   235:      t = norm(t)
nn/__init__.py      :   235:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   235:      ```
nn/__init__.py      :   235:      """

nn/__init__.py      :   262:    class LayerNorm2d(LayerNorm):
nn/__init__.py      :   263:      """
nn/__init__.py      :   263:      Applies Layer Normalization over a mini-batch of 2D inputs.
nn/__init__.py      :   263:    
nn/__init__.py      :   263:      See: `LayerNorm`
nn/__init__.py      :   263:    
nn/__init__.py      :   263:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   263:      norm = nn.LayerNorm2d(3)
nn/__init__.py      :   263:      t = Tensor.rand(2, 3, 4, 4) * 2 + 1
nn/__init__.py      :   263:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   263:      ```
nn/__init__.py      :   263:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   263:      t = norm(t)
nn/__init__.py      :   263:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   263:      ```
nn/__init__.py      :   263:      """

nn/__init__.py      :   280:    class RMSNorm:
nn/__init__.py      :   281:      """
nn/__init__.py      :   281:      Applies Root Mean Square Normalization to input.
nn/__init__.py      :   281:    
nn/__init__.py      :   281:      - Described: https://paperswithcode.com/method/rmsnorm
nn/__init__.py      :   281:      - Paper: https://arxiv.org/abs/1910.07467
nn/__init__.py      :   281:    
nn/__init__.py      :   281:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   281:      norm = nn.RMSNorm(4)
nn/__init__.py      :   281:      t = Tensor.arange(12, dtype=dtypes.float).reshape(3, 4)
nn/__init__.py      :   281:      print(t.numpy())
nn/__init__.py      :   281:      ```
nn/__init__.py      :   281:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   281:      print(norm(t).numpy())
nn/__init__.py      :   281:      ```
nn/__init__.py      :   281:      """

nn/__init__.py      :   302:    class Embedding:
nn/__init__.py      :   303:      """
nn/__init__.py      :   303:      A simple lookup table that stores embeddings of a fixed dictionary and size.
nn/__init__.py      :   303:    
nn/__init__.py      :   303:      See: https://pytorch.org/docs/stable/generated/torch.nn.Embedding
nn/__init__.py      :   303:    
nn/__init__.py      :   303:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   303:      emb = nn.Embedding(10, 3)
nn/__init__.py      :   303:      print(emb(Tensor([1, 2, 3, 1])).numpy())
nn/__init__.py      :   303:      ```
nn/__init__.py      :   303:      """

engine/jit.py       :    70:    class GraphRunner(Runner):  # pylint: disable=abstract-method

engine/jit.py       :    88:    class MultiGraphRunner(GraphRunner):  # pylint: disable=abstract-method

engine/jit.py       :   108:  ReturnType = TypeVar('ReturnType')

engine/jit.py       :   109:    class TinyJit(Generic[ReturnType]):

