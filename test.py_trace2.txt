test.py             :     1:  from tinygrad.tensor import Tensor
helpers.py          :     6:  if TYPE_CHECKING:  # TODO: remove this and import TypeGuard from typing once minimum python supported version is 3.10
helpers.py          :    10:  T = TypeVar("T")
helpers.py          :    11:  U = TypeVar("U")
helpers.py          :    16:  OSX = platform.system() == "Darwin"
helpers.py          :    17:  CI = os.getenv("CI", "") != ""

helpers.py          :    79:    class GraphException(Exception): pass

helpers.py          :    81:    class Context(contextlib.ContextDecorator):
helpers.py          :    82:      stack: ClassVar[List[dict[str, int]]] = [{}]

helpers.py          :    91:    class ContextVar:
helpers.py          :    92:      _cache: ClassVar[Dict[str, ContextVar]] = {}
helpers.py          :    93:      value: int
helpers.py          :    94:      key: str

helpers.py          :   105:  DEBUG, IMAGE, BEAM, NOOPT, JIT = ContextVar("DEBUG", 0), ContextVar("IMAGE", 0), ContextVar("BEAM", 0), ContextVar("NOOPT", 0), ContextVar("JIT", 1)

helpers.py          :    91:    class ContextVar:
helpers.py          :    95:      def __new__(cls, key, default_value):
helpers.py          :    96:        if key in ContextVar._cache: return ContextVar._cache[key]
helpers.py          :    97:        instance = ContextVar._cache[key] = super().__new__(cls)
helpers.py          :    98:        instance.value, instance.key = getenv(key, default_value), key

helpers.py          :    76:      @functools.lru_cache(maxsize=None)
helpers.py          :    76:      def getenv(key:str, default=0): return type(default)(os.getenv(key, default))

helpers.py          :    99:        return instance

helpers.py          :   106:  WINO, THREEFRY, CAPTURING, TRACEMETA = ContextVar("WINO", 0), ContextVar("THREEFRY", 0), ContextVar("CAPTURING", 1), ContextVar("TRACEMETA", 1)
helpers.py          :   107:  GRAPH, GRAPHPATH, SAVE_SCHEDULE, RING = ContextVar("GRAPH", 0), getenv("GRAPHPATH", "/tmp/net"), ContextVar("SAVE_SCHEDULE", 0), ContextVar("RING", 1)
helpers.py          :   108:  MULTIOUTPUT, PROFILE, TRANSCENDENTAL = ContextVar("MULTIOUTPUT", 1), ContextVar("PROFILE", 0), ContextVar("TRANSCENDENTAL", 1)
helpers.py          :   109:  USE_TC, TC_OPT = ContextVar("TC", 1), ContextVar("TC_OPT", 0)
helpers.py          :   110:  FUSE_AS_ONE_KERNEL = ContextVar("FUSE_AS_ONE_KERNEL", 0)
helpers.py          :   113:    @dataclass(frozen=True)
helpers.py          :   113:    class Metadata:
helpers.py          :   114:      name: str
helpers.py          :   115:      caller: str
helpers.py          :   116:      backward: bool = False

helpers.py          :   120:  _METADATA: contextvars.ContextVar[Optional[Metadata]] = contextvars.ContextVar("_METADATA", default=None)

helpers.py          :   124:    class GlobalCounters:
helpers.py          :   125:      global_ops: ClassVar[int] = 0
helpers.py          :   126:      global_mem: ClassVar[int] = 0
helpers.py          :   127:      time_sum_s: ClassVar[float] = 0.0
helpers.py          :   128:      kernel_count: ClassVar[int] = 0
helpers.py          :   129:      mem_used: ClassVar[int] = 0   # NOTE: this is not reset

helpers.py          :   135:    class Timing(contextlib.ContextDecorator):

helpers.py          :   143:    class Profiling(contextlib.ContextDecorator):

helpers.py          :   161:    class ProfileLogger:
helpers.py          :   162:      writers: int = 0
helpers.py          :   163:      mjson: List[Dict] = []
helpers.py          :   164:      actors: Dict[str, int] = {}
helpers.py          :   165:      subactors: Dict[Tuple[str, str], int] = {}
helpers.py          :   166:      path = getenv("PROFILE_OUTPUT_FILE", temp("tinygrad_profile.json"))

helpers.py          :    77:      def temp(x:str) -> str: return (pathlib.Path(tempfile.gettempdir()) / x).as_posix()

helpers.py          :   191:  _cache_dir: str = getenv("XDG_CACHE_HOME", os.path.expanduser("~/Library/Caches" if OSX else "~/.cache"))
helpers.py          :   192:  CACHEDB: str = getenv("CACHEDB", os.path.abspath(os.path.join(_cache_dir, "tinygrad", "cache.db")))
helpers.py          :   193:  CACHELEVEL = getenv("CACHELEVEL", 2)
helpers.py          :   195:  VERSION = 16
helpers.py          :   196:  _db_connection = None
helpers.py          :   223:  _db_tables = set()
helpers.py          :   297:    class tqdm:

helpers.py          :   323:    class trange(tqdm):

dtype.py            :     6:  ConstType = Union[float, int, bool]

dtype.py            :     9:    @dataclass(frozen=True, order=True)
dtype.py            :     9:    class DType:
dtype.py            :    10:      priority: int  # this determines when things get upcasted
dtype.py            :    11:      itemsize: int
dtype.py            :    12:      name: str
dtype.py            :    13:      fmt: Optional[str]
dtype.py            :    14:      count: int

dtype.py            :    23:    # dependent typing?
dtype.py            :    23:    @dataclass(frozen=True, repr=False)
dtype.py            :    23:    class ImageDType(DType):
dtype.py            :    24:      shape: Tuple[int, ...]   # arbitrary arg for the dtype, used in image for the shape
dtype.py            :    25:      base: DType

dtype.py            :    31:    # @dataclass(frozen=True, init=False, repr=False, eq=False)
dtype.py            :    31:    class PtrDType(DType):

dtype.py            :    38:    class dtypes:
dtype.py            :    65:      bigint: Final[DType] = DType(-1, 0, "bigint", None, 1)   # arbitrary precision integer
dtype.py            :    66:      bool: Final[DType] = DType(0, 1, "bool", '?', 1)
dtype.py            :    67:      int8: Final[DType] = DType(1, 1, "char", 'b', 1)
dtype.py            :    68:      uint8: Final[DType] = DType(2, 1, "unsigned char", 'B', 1)
dtype.py            :    69:      int16: Final[DType] = DType(3, 2, "short", 'h', 1)
dtype.py            :    70:      uint16: Final[DType] = DType(4, 2, "unsigned short", 'H', 1)
dtype.py            :    71:      int32: Final[DType] = DType(5, 4, "int", 'i', 1)
dtype.py            :    72:      uint32: Final[DType] = DType(6, 4, "unsigned int", 'I', 1)
dtype.py            :    73:      int64: Final[DType] = DType(7, 8, "long", 'l', 1)
dtype.py            :    74:      uint64: Final[DType] = DType(8, 8, "unsigned long", 'L', 1)
dtype.py            :    75:      float16: Final[DType] = DType(9, 2, "half", 'e', 1)
dtype.py            :    77:      bfloat16: Final[DType] = DType(10, 2, "__bf16", None, 1)
dtype.py            :    78:      float32: Final[DType] = DType(11, 4, "float", 'f', 1)
dtype.py            :    79:      float64: Final[DType] = DType(12, 8, "double", 'd', 1)
dtype.py            :    82:      half = float16; float = float32; double = float64 # noqa: E702
dtype.py            :    83:      uchar = uint8; ushort = uint16; uint = uint32; ulong = uint64 # noqa: E702
dtype.py            :    84:      char = int8; short = int16; int = int32; long = int64 # noqa: E702
dtype.py            :    92:      default_float: ClassVar[DType] = float32
dtype.py            :    93:      default_int: ClassVar[DType] = int32

dtype.py            :    95:  if (env_default_float := getenv("DEFAULT_FLOAT", "")):
dtype.py            :   101:  promo_lattice = { dtypes.bool: [dtypes.int8, dtypes.uint8], dtypes.int8: [dtypes.int16], dtypes.int16: [dtypes.int32], dtypes.int32: [dtypes.int64],
dtype.py            :   101:    dtypes.int64: [dtypes.float16, dtypes.bfloat16], dtypes.uint8: [dtypes.int16, dtypes.uint16], dtypes.uint16: [dtypes.int32, dtypes.uint32],
dtype.py            :   101:    dtypes.uint32: [dtypes.int64, dtypes.uint64], dtypes.uint64: [dtypes.float16, dtypes.bfloat16],
dtype.py            :   101:    dtypes.float16: [dtypes.float32], dtypes.bfloat16: [dtypes.float32], dtypes.float32: [dtypes.float64], }
dtype.py            :   115:  DTYPES_DICT = {k: v for k, v in dtypes.__dict__.items() if not (k.startswith(('__', 'default', 'bigint')) or v.__class__ is staticmethod)}
dtype.py            :   116:  INVERSE_DTYPES_DICT = {v.name:k for k,v in DTYPES_DICT.items()}
dtype.py            :   117:  INVERSE_DTYPES_DICT['bigint'] = 'bigint'
shape/symbolic.py   :    10:    class Node:
shape/symbolic.py   :    11:      b: Union[Node, int]
shape/symbolic.py   :    12:      min: int
shape/symbolic.py   :    13:      max: sint

shape/symbolic.py   :   111:    class Variable(Node):

shape/symbolic.py   :   137:    class NumNode(Node):

shape/symbolic.py   :   183:    class OpNode(Node):

shape/symbolic.py   :   190:    class LtNode(OpNode):

shape/symbolic.py   :   198:    class MulNode(OpNode):

shape/symbolic.py   :   212:    class DivNode(OpNode):

shape/symbolic.py   :   219:    class ModNode(OpNode):

shape/symbolic.py   :   231:    class RedNode(Node):

shape/symbolic.py   :   238:    class SumNode(RedNode):

shape/symbolic.py   :   292:    class AndNode(RedNode):

shape/symbolic.py   :   309:  sint = Union[int, Variable, MulNode, SumNode]
shape/symbolic.py   :   318:    Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
shape/symbolic.py   :   318:      else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
shape/symbolic.py   :   318:      else f"{self.expr}"),
shape/symbolic.py   :   321:    NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
shape/symbolic.py   :   322:    MulNode: render_mulnode,
shape/symbolic.py   :   323:    DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
shape/symbolic.py   :   324:    ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
shape/symbolic.py   :   325:    LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
shape/symbolic.py   :   326:    SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   327:    AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   317:  render_python: Dict[Type, Callable[..., str]] = {
shape/symbolic.py   :   317:    Variable: lambda self,ops,ctx: f"{self.expr}[{self.min}-{self.max}{'='+str(self.val) if self._val is not None else ''}]" if ctx == "DEBUG" \
shape/symbolic.py   :   317:      else (f"Variable('{self.expr}', {self.min}, {self.max})"+(f".bind({self.val})" if self._val is not None else '') if ctx == "REPR" \
shape/symbolic.py   :   317:      else f"{self.expr}"),
shape/symbolic.py   :   317:    NumNode: lambda self,ops,ctx: f"NumNode({self.b})" if ctx == "REPR" else f"{self.b}",
shape/symbolic.py   :   317:    MulNode: render_mulnode,
shape/symbolic.py   :   317:    DivNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}//{self.b})",
shape/symbolic.py   :   317:    ModNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}%{self.b})",
shape/symbolic.py   :   317:    LtNode: lambda self,ops,ctx: f"({self.a.render(ops,ctx)}<{sym_render(self.b,ops,ctx)})",
shape/symbolic.py   :   317:    SumNode: lambda self,ops,ctx: f"({'+'.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   317:    AndNode: lambda self,ops,ctx: f"({' and '.join(sorted([x.render(ops,ctx) for x in self.nodes]))})",
shape/symbolic.py   :   317:  }
shape/view.py       :    85:    @dataclass(frozen=True)
shape/view.py       :    85:    class View:
shape/view.py       :    86:      shape:Tuple[sint, ...]
shape/view.py       :    87:      strides:Tuple[sint, ...]
shape/view.py       :    88:      offset:sint
shape/view.py       :    89:      mask:Optional[Tuple[Tuple[sint, sint], ...]]
shape/view.py       :    90:      contiguous:bool

...e/shapetracker.py:    10:    @dataclass(frozen=True)
...e/shapetracker.py:    10:    class ShapeTracker:
...e/shapetracker.py:    11:      views: Tuple[View, ...]

ops.py              :    15:    # these are the llops your accelerator must implement, along with toCpu
ops.py              :    15:    # the Enum class doesn't work with mypy, this is static. sorry it's ugly
ops.py              :    15:    # NOTE: MOD, CMPLT don't have to be implemented on vectors, just scalars
ops.py              :    15:    # NOTE: many GPUs don't have DIV, but UnaryOps.RECIP doesn't work for integer division
ops.py              :    15:    class UnaryOps(Enum):
ops.py              :    16:      """A -> A (elementwise)"""
ops.py              :    17:      EXP2 = auto(); LOG2 = auto(); CAST = auto(); BITCAST = auto(); SIN = auto(); SQRT = auto(); NEG = auto(); RECIP = auto() # noqa: E702

ops.py              :    18:    class BinaryOps(Enum):
ops.py              :    19:      """A + A -> A (elementwise)"""
ops.py              :    20:      ADD = auto(); MUL = auto(); IDIV = auto(); MAX = auto(); MOD = auto(); CMPLT = auto(); CMPNE = auto(); XOR = auto() # noqa: E702
ops.py              :    21:      SHL = auto(); SHR = auto(); OR = auto(); AND = auto(); THREEFRY = auto() # noqa: E702

ops.py              :    22:    class TernaryOps(Enum):
ops.py              :    23:      """A + A + A -> A (elementwise)"""
ops.py              :    24:      WHERE = auto(); MULACC = auto() # noqa: E702

ops.py              :    25:    class ReduceOps(Enum):
ops.py              :    26:      """A -> B (reduce)"""
ops.py              :    27:      SUM = auto(); MAX = auto(); WMMA = auto() # noqa: E702

ops.py              :    28:    class BufferOps(Enum): LOAD = auto(); CONST = auto(); STORE = auto() # noqa: E702

ops.py              :    29:    class MetaOps(Enum):
ops.py              :    30:      EMPTY = auto(); CONST = auto(); COPY = auto(); CONTIGUOUS = auto(); CUSTOM = auto(); ASSIGN = auto(); VIEW = auto(); KERNEL = auto() # noqa: E702

ops.py              :    31:  Op = Union[UnaryOps, BinaryOps, ReduceOps, MetaOps, TernaryOps, BufferOps]
ops.py              :    34:  UNSAFE_PAD_OPS = {UnaryOps.RECIP, UnaryOps.LOG2, UnaryOps.EXP2, BinaryOps.IDIV}

ops.py              :    37:    @dataclass(frozen=True)
ops.py              :    37:    class MemBuffer:
ops.py              :    38:      idx: int
ops.py              :    39:      dtype: DType
ops.py              :    40:      st: ShapeTracker

ops.py              :    43:    @dataclass(frozen=True)
ops.py              :    43:    class ConstBuffer:
ops.py              :    44:      val: ConstType | Variable
ops.py              :    45:      dtype: DType
ops.py              :    46:      st: ShapeTracker

ops.py              :    49:    @dataclass(frozen=True)
ops.py              :    49:    class KernelInfo:
ops.py              :    50:      local_dims: int = 0           # number of local dimensions  (this is remapping RANGE to SPECIAL)
ops.py              :    51:      upcasted: int = 0             # count that are upcasted     (this is remapping RANGE to EXPAND)
ops.py              :    52:      dont_use_locals: bool = False # don't use local indexing

ops.py              :    55:    @dataclass(frozen=True, eq=False)
ops.py              :    55:    class LazyOp:
ops.py              :    56:      op: Op
ops.py              :    57:      src: Tuple[LazyOp, ...] = ()
ops.py              :    58:      arg: Any = None

ops.py              :   109:  python_alu: Dict[Op, Callable]  = {
ops.py              :   109:    UnaryOps.LOG2: lambda x: math.log2(x) if x > 0 else -math.inf if x == 0 else math.nan,
ops.py              :   109:    UnaryOps.EXP2: hook_overflow(math.inf, lambda x: 2**x),
ops.py              :   109:    UnaryOps.SQRT: lambda x: math.sqrt(x) if x >= 0 else math.nan,
ops.py              :   109:    UnaryOps.SIN: lambda x: math.sin(x) if not math.isinf(x) else math.nan,
ops.py              :   109:    UnaryOps.RECIP: lambda x: 1/x if x != 0 else math.copysign(math.inf, x),
ops.py              :   109:    UnaryOps.NEG: lambda x: (not x) if isinstance(x, bool) else -x,
ops.py              :   109:    BinaryOps.SHR: operator.rshift, BinaryOps.SHL: operator.lshift,
ops.py              :   109:    BinaryOps.MUL: operator.mul, BinaryOps.ADD: operator.add,
ops.py              :   109:    BinaryOps.XOR: operator.xor, BinaryOps.MAX: max, BinaryOps.CMPNE: operator.ne, BinaryOps.CMPLT: operator.lt,
ops.py              :   109:    BinaryOps.OR: operator.or_, BinaryOps.AND: operator.and_,
ops.py              :   109:    BinaryOps.MOD: lambda x,y: abs(int(x))%abs(int(y))*(1,-1)[x<0], BinaryOps.IDIV: lambda x, y: int(x/y) if y != 0 else x*math.inf,
ops.py              :   109:    TernaryOps.MULACC: lambda x,y,z: (x*y)+z,
ops.py              :   109:    TernaryOps.WHERE: lambda x,y,z: y if x else z}

ops.py              :   103:    def hook_overflow(dv, fxn):
ops.py              :   107:      return wfxn

ops.py              :   131:  truncate: Dict[DType, Callable] = {dtypes.bool: bool,
ops.py              :   131:    # TODO: bfloat16
ops.py              :   131:    dtypes.float16: truncate_fp16, dtypes.float32: lambda x: ctypes.c_float(x).value, dtypes.float64: lambda x: ctypes.c_double(x).value,
ops.py              :   131:    dtypes.uint8: lambda x: ctypes.c_uint8(x).value, dtypes.uint16: lambda x: ctypes.c_uint16(x).value,
ops.py              :   131:    dtypes.uint32: lambda x: ctypes.c_uint32(x).value, dtypes.uint64: lambda x: ctypes.c_uint64(x).value,
ops.py              :   131:    dtypes.int8: lambda x: ctypes.c_int8(x).value, dtypes.int16: lambda x: ctypes.c_int16(x).value, dtypes.int32: lambda x: ctypes.c_int32(x).value \
ops.py              :   131:        if isinstance(x,int) else x, dtypes.int64: lambda x: ctypes.c_int64(x).value, dtypes.bigint: lambda x: x }
codegen/uops.py     :    13:    # the order of these UOps controls the order of the toposort
codegen/uops.py     :    13:    class UOps(Enum):
codegen/uops.py     :    15:      SINK = auto(); VAR = auto(); EXPAND = auto(); CONTRACT = auto() # noqa: E702
codegen/uops.py     :    16:      DEFINE_GLOBAL = auto(); DEFINE_VAR = auto(); DEFINE_LOCAL = auto(); DEFINE_ACC = auto() # noqa: E702
codegen/uops.py     :    17:      CONST = auto(); SPECIAL = auto() # noqa: E702
codegen/uops.py     :    18:      NOOP = auto(); UNMUL = auto(); GEP = auto() # noqa: E702
codegen/uops.py     :    20:      CAST = auto(); BITCAST = auto(); VECTORIZE = auto() # noqa: E702
codegen/uops.py     :    21:      ALU = auto(); REDUCE = auto(); WMMA = auto() # noqa: E702
codegen/uops.py     :    23:      LOAD = auto(); STORE = auto(); PHI = auto() # noqa: E702
codegen/uops.py     :    25:      BARRIER = auto(); IF = auto(); RANGE = auto() # noqa: E702
codegen/uops.py     :    27:      ENDRANGE = auto(); ENDIF = auto() # noqa: E702

codegen/uops.py     :    29:  END_FOR_UOP = {UOps.IF:(UOps.STORE, UOps.ENDIF), UOps.RANGE:(UOps.PHI, UOps.ENDRANGE)}

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    34:      op: UOps
codegen/uops.py     :    35:      dtype: Optional[DType] = None
codegen/uops.py     :    36:      src: Tuple[UOp, ...] = tuple()
codegen/uops.py     :    37:      arg: Any = None

codegen/uops.py     :   100:    class UPat:

codegen/uops.py     :   145:    class PatternMatcher:

...transcendental.py:     6:  TRANSCENDENTAL_SUPPORTED_DTYPES = {dtypes.float16, dtypes.float32, dtypes.float64}
codegen/uopgraph.py :    11:  if TYPE_CHECKING:

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    71:      def const(self:Union[UOp, DType, None], b:ConstType|Variable): return UOp._const(self.dtype if isinstance(self, UOp) else self, b)

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    74:        @staticmethod
codegen/uops.py     :    74:        @functools.lru_cache(maxsize=None)
codegen/uops.py     :    74:        def _const(dtype:Optional[DType], b:ConstType|Variable):
codegen/uops.py     :    75:          if isinstance(b, Variable): return UOp(UOps.DEFINE_VAR, dtype, (), b)
codegen/uops.py     :    76:          return UOp(UOps.CONST, dtype, arg=dtypes.as_const(b, dtype) if dtype is not None else b)

dtype.py            :    38:        class dtypes:
dtype.py            :    54:          @staticmethod
dtype.py            :    54:          def as_const(val: ConstType, dtype:DType): return int(val) if dtypes.is_int(dtype) else float(val) if dtypes.is_float(dtype) else bool(val)

dtype.py            :    38:          class dtypes:
dtype.py            :    42:            @staticmethod # static methds on top, or bool in the type info will refer to dtypes.bool
dtype.py            :    42:            def is_int(x: DType) -> bool: return x.scalar() in (dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64, dtypes.bigint) or dtypes.is_unsigned(x)

dtype.py            :     9:            @dataclass(frozen=True, order=True)
dtype.py            :     9:            class DType:
dtype.py            :    19:              def scalar(self): return DTYPES_DICT[self.name[:-len(str(self.count))]] if self.count > 1 else self

codegen/uopgraph.py :    53:  float4_folding = PatternMatcher([
codegen/uopgraph.py :    53:    # reorder index to bring const closer to store
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"), UOp.var("idx")+
codegen/uopgraph.py :    53:      (UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex")+UOp.var("idx2")), UOp.var("var"))).name("store"),
codegen/uopgraph.py :    53:      lambda buf, store, idx, idx2, ex, var: UOp(UOps.STORE, store.dtype, (buf, idx+idx2+ex, var), store.arg)),
codegen/uopgraph.py :    53:    # float(2,4) load
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex")+UOp.var("idx"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.EXPAND).name("ex"))).name("load"), float4_expand_load),
codegen/uopgraph.py :    53:    # float(2,4) store
codegen/uopgraph.py :    53:    # TODO: fold ADDs into one UOp and remove add chains
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2")+UOp.var("idx3"), UOp.var("var"))).name("store_allow_any_len"),
codegen/uopgraph.py :    53:      float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx")+UOp.var("idx2"), UOp.var("var"))).name("store_allow_any_len"),
codegen/uopgraph.py :    53:      float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex")+UOp.var("idx"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"),
codegen/uopgraph.py :    53:      UOp(UOps.EXPAND).name("ex"), UOp.var("var"))).name("store_allow_any_len"), float4_contract_store),
codegen/uopgraph.py :    53:    # image handling
codegen/uopgraph.py :    53:    (UOp(UOps.LOAD, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
codegen/uopgraph.py :    53:       UOp.var('id4'))))).name("ls_allow_any_len"), image_contract_load),
codegen/uopgraph.py :    53:    (UOp(UOps.STORE, src=(UOp.var("buf"), UOp(UOps.VECTORIZE, dtypes.int.vec(3), (UOp.var('idx'), UOp.var('idy'),
codegen/uopgraph.py :    53:       UOp(UOps.EXPAND, src=tuple(UOp.const(dtypes.int, i) for i in range(4))).name("ex"))), UOp.var("var"))).name("ls_allow_any_len"),
codegen/uopgraph.py :    53:       image_contract_store),
codegen/uopgraph.py :    53:  ])

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    84:      @staticmethod
codegen/uops.py     :    84:      def var(name:Optional[str]=None, dtype:Optional[DType]=None): return UOp(UOps.VAR, dtype=dtype, arg=name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    50:      def name(self, name:Optional[str]): return UOp(UOps.VAR, src=(self,), arg=name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    52:      def __add__(self, x): return UOp.alu(BinaryOps.ADD, self, ufix(self.dtype, x))

codegen/uops.py     :    31:      def ufix(dtype: Optional[DType], x): return UOp.const(dtype, x) if not isinstance(x, UOp) else x

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    78:        @staticmethod
codegen/uops.py     :    78:        def alu(arg, *src:UOp): return UOp(UOps.ALU, dtypes.bool if arg in {BinaryOps.CMPLT, BinaryOps.CMPNE} else src[-1].dtype, src, arg)

dtype.py            :     9:    @dataclass(frozen=True, order=True)
dtype.py            :     9:    class DType:
dtype.py            :    16:      def vec(self, sz:int):
dtype.py            :    17:        assert sz > 1 and self.count == 1, f"can't vectorize {self} with size {sz}"
dtype.py            :    18:        return DType(self.priority, self.itemsize*sz, f"{INVERSE_DTYPES_DICT[self.name]}{sz}", None, sz)

codegen/uops.py     :   145:    class PatternMatcher:
codegen/uops.py     :   146:      def __init__(self, patterns:List[Tuple[Union[UPat, UOp], Callable]]):
codegen/uops.py     :   147:        self.patterns = patterns
codegen/uops.py     :   148:        self.pdict: DefaultDict[Tuple[UOps, Any], List[Tuple[UPat, Callable]]] = defaultdict(list)
codegen/uops.py     :   150:        for p,fxn in self.patterns:
codegen/uops.py     :   151:          if isinstance(p, UOp): p = UPat.compile(p)

codegen/uops.py     :   100:      class UPat:
codegen/uops.py     :   120:        @staticmethod
codegen/uops.py     :   120:        def compile(u: UOp, name:Optional[str]=None) -> UPat:
codegen/uops.py     :   121:          if u.op is UOps.VAR: return UPat(name=name or u.arg, dtype=u.dtype) if len(u.src) == 0 else UPat.compile(u.src[0], name or u.arg)

codegen/uops.py     :   100:        class UPat:
codegen/uops.py     :   120:          @staticmethod
codegen/uops.py     :   120:          def compile(u: UOp, name:Optional[str]=None) -> UPat:
codegen/uops.py     :   122:            return UPat(u.op, u.arg, (list if u.commutative() else tuple)([UPat.compile(src) for src in u.src]) if u.src != () else None,
codegen/uops.py     :   122:                        name, u.dtype, allow_any_len=(isinstance(name, str) and 'allow_any_len' in name))

codegen/uops.py     :    33:          @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:          class UOp:
codegen/uops.py     :    38:            def commutative(self) -> bool:
codegen/uops.py     :    39:              return self.op is UOps.UNMUL or (self.op is UOps.ALU and \
codegen/uops.py     :    39:                self.arg in {BinaryOps.ADD, BinaryOps.MUL, BinaryOps.MAX, BinaryOps.CMPNE, BinaryOps.XOR, BinaryOps.AND, BinaryOps.OR})

codegen/uops.py     :   100:            class UPat:
codegen/uops.py     :   101:              def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                           name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   103:                self.op: Optional[Tuple[UOps, ...]] = None if op is None else (tuple(op) if isinstance(op, set) else (op,))
codegen/uops.py     :   104:                self.dtype: Optional[Tuple[DType, ...]] = None if dtype is None else (tuple(dtype) if isinstance(dtype, set) else (dtype,))
codegen/uops.py     :   105:                self.arg = arg
codegen/uops.py     :   106:                self.src: Any = None
codegen/uops.py     :   107:                if isinstance(src, list):
codegen/uops.py     :   110:                elif isinstance(src, tuple):
codegen/uops.py     :   113:                elif isinstance(src, UPat):
codegen/uops.py     :   116:                self.name: Optional[str] = name
codegen/uops.py     :   117:                self.allowed_len: int = 0 if allow_any_len or isinstance(src, UPat) or src is None else len(src)

codegen/uops.py     :   100:                class UPat:
codegen/uops.py     :   101:                  def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                               name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   112:                      self.src = [src]

codegen/uops.py     :   100:          class UPat:
codegen/uops.py     :   101:            def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                         name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   109:                self.src = list(itertools.permutations(src))

codegen/uops.py     :   152:        assert p.op is not None
codegen/uops.py     :   153:        for uop in p.op: self.pdict[(uop, p.arg)].append((p, fxn))
codegen/uopgraph.py :    84:  transcendental_folding = PatternMatcher([
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="x"),), arg=UnaryOps.EXP2), xexp2),
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=UnaryOps.LOG2), xlog2),
codegen/uopgraph.py :    84:    (UPat(UOps.ALU, dtype=TRANSCENDENTAL_SUPPORTED_DTYPES, src=(UPat(name="d"),), arg=UnaryOps.SIN), xsin),
codegen/uopgraph.py :    84:  ])
codegen/uopgraph.py :   130:  constant_folder = PatternMatcher([
codegen/uopgraph.py :   130:    # CONTRACT before ALU/REDUCE/CAST
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.ALU, name="alu"),)),
codegen/uopgraph.py :   130:     lambda con, alu: UOp(alu.op, con.dtype, tuple(UOp(UOps.CONTRACT, x.dtype.vec(con.dtype.count), (x,), con.arg) for x in alu.src), alu.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.REDUCE, dtype={dtypes.half, dtypes.bfloat16, dtypes.float}, name="red"),)),
codegen/uopgraph.py :   130:     lambda con, red: UOp(UOps.REDUCE, con.dtype, (UOp(UOps.CONTRACT, con.dtype, red.src[0:1], con.arg),)+red.src[1:], red.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CONTRACT, name="con", src=(UPat(UOps.CAST, dtype={dtypes.half, dtypes.bfloat16, dtypes.float}, src=(UPat(name="casted"),)),)),
codegen/uopgraph.py :   130:     lambda con, casted: UOp(UOps.CAST, con.dtype, (UOp(UOps.CONTRACT, casted.dtype.vec(con.dtype.count), (casted,), con.arg),))),
codegen/uopgraph.py :   130:    # bigint is rewritten to int32
codegen/uopgraph.py :   130:    (UPat({UOps.CONST, UOps.ALU, UOps.SPECIAL, UOps.RANGE, UOps.EXPAND}, dtype=dtypes.bigint, name="x"),
codegen/uopgraph.py :   130:     lambda x: UOp(x.op, dtypes.int32, x.src, x.arg)),
codegen/uopgraph.py :   130:    # VECTORIZE/GEP
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp(UOps.VECTORIZE).name("cast"),)).name("gep"), lambda gep, cast: cast.src[gep.arg]),
codegen/uopgraph.py :   130:    *[(UOp(UOps.VECTORIZE, dtypes.float.vec(i), tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=j)
codegen/uopgraph.py :   130:        for j in range(i))), lambda x: x) for i in [2, 4, 8]],
codegen/uopgraph.py :   130:    # tensor core with a 0 input is acc
codegen/uopgraph.py :   130:    (UOp(UOps.WMMA, src=(UOp.const(None, 0.0), UOp.var(), UOp.var('acc'))), lambda acc: acc),
codegen/uopgraph.py :   130:    (UOp(UOps.WMMA, src=(UOp.var(), UOp.const(None, 0.0), UOp.var('acc'))), lambda acc: acc),
codegen/uopgraph.py :   130:    # tensor core cleanups
codegen/uopgraph.py :   130:    (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(2))).name("expand"),))
codegen/uopgraph.py :   130:     .name("reduce_allow_any_len"), reduce_before_expand),
codegen/uopgraph.py :   130:    (UOp(UOps.REDUCE, src=(UOp(UOps.EXPAND, src=tuple(UOp(UOps.GEP, dtypes.float, src=(UOp.var('x'),), arg=i) for i in range(8))).name("expand"),))
codegen/uopgraph.py :   130:     .name("reduce_allow_any_len"), reduce_before_expand),
codegen/uopgraph.py :   130:    (UOp.var("add") + UOp(UOps.WMMA).name("wmma"),
codegen/uopgraph.py :   130:      lambda add, wmma: UOp(wmma.op, wmma.dtype, (wmma.src[0], wmma.src[1], wmma.src[2]+add), wmma.arg)),
codegen/uopgraph.py :   130:    # threefry
codegen/uopgraph.py :   130:    (UOp(UOps.ALU, dtype=dtypes.uint64, src=(UOp.var("x"), UOp.var("seed")), arg=BinaryOps.THREEFRY), threefry2x32),
codegen/uopgraph.py :   130:    # arange loop folding (early)
codegen/uopgraph.py :   130:    (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(BinaryOps.MUL,
codegen/uopgraph.py :   130:      UOp.cvar("mval"), UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
codegen/uopgraph.py :   130:      UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None,0)), loop_collapse),
codegen/uopgraph.py :   130:    (UOp.where(UOp.alu(BinaryOps.CMPLT, UOp.alu(BinaryOps.ADD, UOp.var("idx"), UOp.alu(UnaryOps.NEG,
codegen/uopgraph.py :   130:      UOp(UOps.RANGE, src=(UOp.var("loop_start"), UOp.var("loop_end"))).name("rng"))),
codegen/uopgraph.py :   130:      UOp.cvar("compval")), UOp.cvar("multconst"), UOp.const(None, 0)),
codegen/uopgraph.py :   130:      lambda **kwargs: loop_collapse(mval=UOp.const(dtypes.int, -1), **kwargs)),
codegen/uopgraph.py :   130:    # sum collapse to mul (with possible GEP)
codegen/uopgraph.py :   130:    (UPat(UOps.PHI, src=(UPat(UOps.DEFINE_ACC, name="phi_input", src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),
codegen/uopgraph.py :   130:                         UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
codegen/uopgraph.py :   130:    (UPat(UOps.PHI, src=(UPat(UOps.GEP, name="phi_input", src=(UPat(UOps.DEFINE_ACC, src=[UPat(UOps.CONST), UPat(UOps.RANGE, name="loop")]),)),
codegen/uopgraph.py :   130:                         UPat(UOps.ALU, BinaryOps.ADD, src=(UPat(name="val1"), UPat(name="val2"))))), sum_collapse),
codegen/uopgraph.py :   130:    # deal with UNMUL
codegen/uopgraph.py :   130:    (UOp.cvar('c1') * UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v'))), lambda c1,c2,v: v if c1.arg == c2.arg else None),
codegen/uopgraph.py :   130:    (UOp.cvar('c1') * (UOp.var('add') + UOp(UOps.UNMUL, src=(UOp.cvar('c2'), UOp.var('v')))),
codegen/uopgraph.py :   130:      lambda c1, add, c2, v: (add*c1+v) if c1.arg == c2.arg else None),
codegen/uopgraph.py :   130:    (UOp(UOps.UNMUL, src=(UOp.const(None, 0).name('zero'), UOp.var())), lambda zero: zero),
codegen/uopgraph.py :   130:    (UOp(UOps.UNMUL).name('unmul').cast().name('root'), lambda root,unmul: UOp(UOps.UNMUL, root.dtype, (unmul.src[0].cast(root.dtype), unmul.src[1]))),
codegen/uopgraph.py :   130:    # indexing (with a multiply offset)!
codegen/uopgraph.py :   130:    (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).cast()*
codegen/uopgraph.py :   130:      UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"),
codegen/uopgraph.py :   130:      lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
codegen/uopgraph.py :   130:    (UOp.var('idx').eq(UOp(UOps.RANGE).name("rng")).where(
codegen/uopgraph.py :   130:      UOp(UOps.LOAD, src=(UOp.var("buf"), UOp.var('add')+UOp.var('mul')*UOp(UOps.RANGE).name("rng"))).name("ld"), UOp.const(None, 0.0)),
codegen/uopgraph.py :   130:      lambda idx,rng,buf,add,mul,ld: UOp(UOps.UNMUL, ld.dtype, (UOp(ld.op, ld.dtype, (buf, add+mul*idx)), rng.src[1]-rng.src[0]))),
codegen/uopgraph.py :   130:    # other arange folders
codegen/uopgraph.py :   130:    (UOp.cvar("c1") - (UOp.var("x") + UOp.cvar("c2")), lambda c1, c2, x: (c1-c2)-x),  # c1 - (x + c2) -> (c1-c2) - x
codegen/uopgraph.py :   130:    # max on special can go away (TODO: special should be variable, same thing applies)
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')), lambda c,s: c if (s.arg[2]-1) <= c.arg else None),
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if 0 >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.SPECIAL).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.arg[2]-1+c2.arg) >= c.arg else None),
codegen/uopgraph.py :   130:    # max on range can go away (ugh: copy of SPECIAL, and with/without const)
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')), lambda c,s: s if s.src[0].arg >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), UOp(UOps.RANGE).name('s')+UOp.cvar('c2')), lambda c,s,c2: (s+c2) if s.src[0].arg >= c.arg else None),  # TODO: generic
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s'))), lambda c,s: -s if -(s.src[1].arg-1) >= c.arg else None),
codegen/uopgraph.py :   130:    (UOp.max(UOp.cvar('c'), -(UOp(UOps.RANGE).name('s')+UOp.cvar('c2'))), lambda c,s,c2: -(s+c2) if -(s.src[1].arg-1+c2.arg) >= c.arg else None),
codegen/uopgraph.py :   130:    # const rules
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp.cvar("c"),)).name("root"), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.CAST, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    (UPat(UOps.VECTORIZE, name="root", src=UPat(UOps.CONST, name="c")), lambda root, c: UOp.const(root.dtype, c.arg)),
codegen/uopgraph.py :   130:    # a phi on a DEFINE_ACC without loops or a CONST is a noop. this is for correctness, not just speed
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC).name("acc"), UOp.var("acc"))), lambda acc: UOp.cast(acc.src[0], acc.dtype)),
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)), UOp.var("x"))), lambda x: x),
codegen/uopgraph.py :   130:    (UOp(UOps.PHI, src=(UOp.cvar(), UOp.var("x"))), lambda x: x),
codegen/uopgraph.py :   130:    # a DEFINE_ACC without inputs is a const + GEP on a const is the const
codegen/uopgraph.py :   130:    (UOp(UOps.DEFINE_ACC, src=(UOp.cvar(),)).name("root"), lambda root: UOp.cast(root.src[0], root.dtype)),
codegen/uopgraph.py :   130:    (UOp(UOps.GEP, src=(UOp.cvar("x"),)).name("root"), lambda root,x: UOp.const(root.dtype, x.arg)),
codegen/uopgraph.py :   130:    # max -2147483648
codegen/uopgraph.py :   130:    (UOp.max(UOp.var('x'), UOp.const(dtypes.int, -2147483648)), lambda x: x),
codegen/uopgraph.py :   130:    # bool < False is always false, True < bool is always false
codegen/uopgraph.py :   130:    (UOp.var().lt(UOp.const(dtypes.bool, False)), lambda: UOp.const(dtypes.bool, False)),
codegen/uopgraph.py :   130:    (UOp.const(dtypes.bool, True).lt(UOp.var()), lambda: UOp.const(dtypes.bool, False)),
codegen/uopgraph.py :   130:    # a conditional with the same results either way is a noop, also fold const conditionals
codegen/uopgraph.py :   130:    (UOp.var().where(UOp.var("val"), UOp.var("val")), lambda val: val),
codegen/uopgraph.py :   130:    (UOp.cvar('gate').where(UOp.var('c0'), UOp.var('c1')), lambda gate, c0, c1: c0 if gate.arg else c1),
codegen/uopgraph.py :   130:    # ** constant folding **
codegen/uopgraph.py :   130:    (UPat(UOps.ALU, name="root", src=UPat(UOps.CONST)), lambda root: UOp.const(root.dtype, exec_alu(root.arg, root.dtype, [x.arg for x in root.src]))),
codegen/uopgraph.py :   130:    # ** self folding **
codegen/uopgraph.py :   130:    (-(-UOp.var('x')), lambda x: x),    # -(-x) -> x
codegen/uopgraph.py :   130:    (UOp.var('x') + 0, lambda x: x),    # x+0 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') * 1, lambda x: x),    # x*1 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') * -1, lambda x: -x),  # x*-1 -> -x
codegen/uopgraph.py :   130:    (UOp.var('x') // UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x//x -> 1
codegen/uopgraph.py :   130:    (UOp.var('x') // 1, lambda x: x),   # x//1 -> x
codegen/uopgraph.py :   130:    (UOp.var('x') // -1, lambda x: -x), # x//-1 -> -x
codegen/uopgraph.py :   130:    (UOp.var('x') / UOp.var('x'), lambda x: UOp.const(x.dtype, 1)), # x/x -> 1
codegen/uopgraph.py :   130:    (UOp.var('x') / UOp.cvar('c'), lambda x,c: x*exec_alu(UnaryOps.RECIP, c.dtype, [c.arg])),    # x/c -> x*(1/c)
codegen/uopgraph.py :   130:    (UOp.var('x', dtype=dtypes.bool).max(UOp.const(dtypes.bool, False)), lambda x: x),  # max(x, False) -> x
codegen/uopgraph.py :   130:    # ** zero folding **
codegen/uopgraph.py :   130:    #x*0 -> 0 or 0*x -> 0
codegen/uopgraph.py :   130:    #if x is nan or inf it should render the nan value.
codegen/uopgraph.py :   130:    # NOTE: this can be wrong for loaded NaN
codegen/uopgraph.py :   130:    (UOp.var('x') * 0, lambda x: UOp.const(x.dtype, float('nan') if isinstance(x.arg, float) and (math.isnan(x.arg) or math.isinf(x.arg)) else 0)),
codegen/uopgraph.py :   130:    (UOp.var('x') - UOp.var('x'), lambda x: UOp.const(x.dtype, 0)),   # x-x -> 0
codegen/uopgraph.py :   130:    # ** load/store folding **
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.load(UOp.var("buf"), UOp.var("idx"))), lambda buf,idx:UOp(UOps.NOOP)),
codegen/uopgraph.py :   130:    # ** two stage add/sub folding **
codegen/uopgraph.py :   130:    ((UOp.var('x') + UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, c2.arg]))),
codegen/uopgraph.py :   130:    ((UOp.var('x') - UOp.cvar('c1')) + UOp.cvar('c2'), lambda x,c1,c2: x+UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c2.arg, -c1.arg]))),
codegen/uopgraph.py :   130:    # *** rules from symbolic ***
codegen/uopgraph.py :   130:    # mod divides
codegen/uopgraph.py :   130:    ((UOp.cvar('c')*UOp.var('x')) % UOp.cvar('c'), lambda x,c: x.const(0)),
codegen/uopgraph.py :   130:    (((UOp.cvar('c')*UOp.var('x'))+UOp.var('x2')) % UOp.cvar('c'), lambda x,c,x2: x2%c),
codegen/uopgraph.py :   130:    # two stage mul, (x*c1)*c2 = x*(c1*c2)
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.cvar("c1")) * UOp.cvar("c2"), lambda x,c1,c2: x*UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c1.arg, c2.arg]))),
codegen/uopgraph.py :   130:    # -(x+y) -> -x + -y
codegen/uopgraph.py :   130:    #(-(UOp.var("x") + UOp.var("y")), lambda x,y: (-x)+(-y)),
codegen/uopgraph.py :   130:    # x%1 -> 0
codegen/uopgraph.py :   130:    (UOp.var("x") % UOp.const(None, 1), lambda x: UOp.const(x.dtype, 0)),
codegen/uopgraph.py :   130:    # (x*c0)+(x*c1) -> x*(c0+c1)
codegen/uopgraph.py :   130:    (UOp.var("x") * UOp.cvar("c0") + UOp.var("x") * UOp.cvar("c1"), lambda x,c0,c1: x*exec_alu(BinaryOps.ADD, x.dtype, [c0.arg, c1.arg])),
codegen/uopgraph.py :   130:    # (x*c0)+(y*c0) -> (x+y)*c0
codegen/uopgraph.py :   130:    #((UOp.var("x") * UOp.cvar("c0")) + (UOp.var("y") * UOp.cvar("c0")), lambda x,y,c0: c0*(x+y)),
codegen/uopgraph.py :   130:    # (x*c0)//c0 -> x
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.cvar("c0")) // UOp.cvar("c0"), lambda x,c0: x if c0.arg != 0 else None),
codegen/uopgraph.py :   130:    # (x*x2)/x2 -> x
codegen/uopgraph.py :   130:    ((UOp.var("x") * UOp.var("x2")) / UOp.var("x2"), lambda x,x2: x),
codegen/uopgraph.py :   130:    # (x//c0)//c1 -> x//(c0*c1)
codegen/uopgraph.py :   130:    ((UOp.var("x") // UOp.cvar("c0")) // UOp.cvar("c1"), lambda x,c0,c1: x//UOp.const(x.dtype, exec_alu(BinaryOps.MUL, x.dtype, [c0.arg, c1.arg]))),
codegen/uopgraph.py :   130:    # (x/x1)/x2 -> x/(x1*x2)
codegen/uopgraph.py :   130:    ((UOp.var("x") / UOp.var("x2")) / UOp.var("x3"), lambda x,x2,x3: x/(x2*x3)),
codegen/uopgraph.py :   130:    # c0 + x < c1 -> x < c1 - c0
codegen/uopgraph.py :   130:    ((UOp.cvar("c0") + UOp.var("x")).lt(UOp.cvar("c1")),
codegen/uopgraph.py :   130:      lambda x,c0,c1: UOp.lt(x, UOp.const(x.dtype, exec_alu(BinaryOps.ADD, x.dtype, [c1.arg, -c0.arg])))),
codegen/uopgraph.py :   130:    # (x+x*c0)-> x*(c0+1)
codegen/uopgraph.py :   130:    (UOp.var("x") + UOp.var("x") * UOp.cvar("c0"), lambda x,c0: x*UOp.const(x.dtype, c0.arg+1)),
codegen/uopgraph.py :   130:    # x!=0 -> (bool)x
codegen/uopgraph.py :   130:    (UOp.var("x").ne(0), lambda x: x.cast(dtypes.bool)),
codegen/uopgraph.py :   130:    # bool != 1 -> not bool
codegen/uopgraph.py :   130:    (UOp.var("x", dtype=dtypes.bool).ne(1), lambda x: -x),
codegen/uopgraph.py :   130:    # TODO: can do the invert of this (flip alt/load) when we fix double ops
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.alu(TernaryOps.WHERE, UOp.var("gate"), UOp.var("alt"), UOp.load(UOp.var("buf"), UOp.var("idx")))),
codegen/uopgraph.py :   130:     lambda buf, idx, gate, alt: UOp.store(buf, idx, alt, gate)),
codegen/uopgraph.py :   130:    # VECTORIZE-PHI-GEP -> PHI-VECTORIZE
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(4))).name("root"),
codegen/uopgraph.py :   130:     lambda root, val, v0, v1, v2, v3: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1, v2, v3))))),
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE, src=tuple(UOp(UOps.PHI, src=(UOp(UOps.GEP, src=(UOp.var("val"),), arg=i), UOp.var(f"v{i}"))) for i in range(2))).name("root"),
codegen/uopgraph.py :   130:     lambda root, val, v0, v1: UOp(UOps.PHI, root.dtype, (val, UOp(UOps.VECTORIZE, val.dtype, (v0, v1))))),
codegen/uopgraph.py :   130:    # NEG/CMPLT -> CMPLT
codegen/uopgraph.py :   130:    (UOp.lt(-UOp.var('x'), UOp.cvar('c', dtypes.int)), lambda c,x: UOp.lt(UOp.const(c.dtype, -c.arg), x)),
codegen/uopgraph.py :   130:    # cast NOOP (NOTE: it's str to deal with PtrDType)
codegen/uopgraph.py :   130:    (UOp(UOps.CAST).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
codegen/uopgraph.py :   130:    (UOp(UOps.VECTORIZE).name("root"), lambda root: root.src[0] if str(root.dtype) == str(root.src[0].dtype) else None),
codegen/uopgraph.py :   130:    # fold gated LOAD/STORE
codegen/uopgraph.py :   130:    (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var")), lambda buf,idx,var: UOp.load(buf, idx, dtype=var.dtype)),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var("buf"), UOp.var("idx"), UOp.const(dtypes.bool, True), UOp.cvar("var"), UOp.var("barrier")),
codegen/uopgraph.py :   130:     lambda buf,idx,var,barrier: UOp.load(buf, idx, barrier, dtype=var.dtype)),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var")), lambda var: var),
codegen/uopgraph.py :   130:    (UOp.load(UOp.var(), UOp.var(), UOp.const(dtypes.bool, False), UOp.cvar("var"), UOp.var()), lambda var: var),
codegen/uopgraph.py :   130:    (UOp.store(UOp.var("buf"), UOp.var("idx"), UOp.var("val"), UOp.const(dtypes.bool, True)), UOp.store),
codegen/uopgraph.py :   130:    (UOp.store(UOp.var(), UOp.var(), UOp.var(), UOp.const(dtypes.bool, False)), lambda: UOp(UOps.NOOP)),
codegen/uopgraph.py :   130:    # remove NOOPs from SINK
codegen/uopgraph.py :   130:    (UOp(UOps.SINK).name("root"),
codegen/uopgraph.py :   130:      lambda root: UOp(UOps.SINK, root.dtype, a, root.arg) if len(a:=tuple(x for x in root.src if x.op is not UOps.NOOP)) != len(root.src) else None),
codegen/uopgraph.py :   130:  ])
codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    86:      @staticmethod
codegen/uops.py     :    86:      def cvar(name:Optional[str]=None, dtype:Optional[DType]=None): return UOp(UOps.CONST, dtype=dtype).name(name)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    69:      def where(self, x, y): return UOp.alu(TernaryOps.WHERE, self, x, y)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    55:      def __mul__(self, x): return UOp.alu(BinaryOps.MUL, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    48:      def cast(self, dtype=None): return UOp(UOps.CAST, dtype, (self,))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    64:      def eq(self, x): return -self.ne(x)

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    63:        def ne(self, x): return UOp.alu(BinaryOps.CMPNE, self, ufix(self.dtype, x))

codegen/uops.py     :    33:      @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:      class UOp:
codegen/uops.py     :    51:        def __neg__(self): return UOp.alu(UnaryOps.NEG, self)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    54:      def __sub__(self, x): return UOp.alu(BinaryOps.ADD, self, -ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    67:      def max(self, x): return UOp.alu(BinaryOps.MAX, self, x)

codegen/uops.py     :   100:    class UPat:
codegen/uops.py     :   101:      def __init__(self, op:Optional[Union[UOps, Set[UOps]]]=None, arg:Any=None, src:Optional[Union[Tuple[UPat, ...], List[UPat], UPat]]=None,
codegen/uops.py     :   101:                   name:Optional[str]=None, dtype:Optional[Union[DType, Set[DType]]]=None, allow_any_len:bool=False):
codegen/uops.py     :   115:          self.src = [itertools.repeat(src)]

dtype.py            :    38:            class dtypes:
dtype.py            :    44:              @staticmethod
dtype.py            :    44:              def is_unsigned(x: DType) -> bool: return x.scalar() in (dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64)

dtype.py            :    38:          class dtypes:
dtype.py            :    40:            @staticmethod
dtype.py            :    40:            def is_float(x: DType) -> bool: return x.scalar() in (dtypes.float16, dtypes.bfloat16, dtypes.float32, dtypes.float64)

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    65:      def lt(self, x): return UOp.alu(BinaryOps.CMPLT, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    57:      def __floordiv__(self, x): return UOp.alu(BinaryOps.IDIV, self, ufix(self.dtype, x))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    58:      def __truediv__(self, x): return UOp.alu(BinaryOps.MUL, self, UOp.alu(UnaryOps.RECIP, ufix(self.dtype, x)))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    80:      @staticmethod
codegen/uops.py     :    80:      def load(*src:UOp, dtype:Optional[DType]=None, **kwargs): return UOp(UOps.LOAD, dtype, tuple(src)+tuple(kwargs.values()))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    82:      @staticmethod
codegen/uops.py     :    82:      def store(*src:UOp, **kwargs): return UOp(UOps.STORE, None, tuple(src)+tuple(kwargs.values()))

codegen/uops.py     :    33:    @dataclass(frozen=True, eq=False)
codegen/uops.py     :    33:    class UOp:
codegen/uops.py     :    59:      def __mod__(self, x): return UOp.alu(BinaryOps.MOD, self, ufix(self.dtype, x))

codegen/uopgraph.py :   345:  acc_number = 0
codegen/uopgraph.py :   394:  expander = PatternMatcher([
codegen/uopgraph.py :   394:    (UPat({UOps.ALU, UOps.CAST, UOps.BITCAST, UOps.GEP, UOps.WMMA, UOps.LOAD, UOps.STORE,
codegen/uopgraph.py :   394:           UOps.VECTORIZE, UOps.REDUCE, UOps.EXPAND, UOps.IF}, name="root"), do_expand),
codegen/uopgraph.py :   394:    (UOp(UOps.REDUCE).name("root"), do_reduce_with_expand),
codegen/uopgraph.py :   394:    (UOp(UOps.CONTRACT).name("con"), do_contract),
codegen/uopgraph.py :   394:    # remove EXPANDs from SINK
codegen/uopgraph.py :   394:    (UOp(UOps.SINK).name("root"),
codegen/uopgraph.py :   394:     lambda root: UOp(UOps.SINK, root.dtype, a, root.arg)
codegen/uopgraph.py :   394:      if len(a:=tuple(flatten(x.src if x.op is UOps.EXPAND else (x,) for x in root.src))) != len(root.src) else None),
codegen/uopgraph.py :   394:    # BARRIERs aren't actually expanded
codegen/uopgraph.py :   394:    (UOp(UOps.BARRIER, src=(UOp(UOps.EXPAND).name("ex"),)), lambda ex: UOp(UOps.EXPAND, None, (UOp(UOps.BARRIER, None, ex.src),)*len(ex.src), ex.arg)),
codegen/uopgraph.py :   394:    # empty EXPAND is NOOP
codegen/uopgraph.py :   394:    (UOp(UOps.EXPAND, src=(UOp.var('x'),), arg=()), lambda x: x),
codegen/uopgraph.py :   394:    # no ALU on vectorized dtypes
codegen/uopgraph.py :   394:    (UPat({UOps.ALU, UOps.CAST}, name="alu"), no_vectorized_alu),
codegen/uopgraph.py :   394:  ])
codegen/uopgraph.py :   432:    class UOpGraph:
codegen/uopgraph.py :   464:      cnt = 0

renderer/__init__.py:    10:    @dataclass(frozen=True)
renderer/__init__.py:    10:    class TensorCore: # D = A * B + C, A is (M x K), B is (K x N), C and D are (M x N)
renderer/__init__.py:    11:      dims: Tuple[int,int,int] # N, M, K
renderer/__init__.py:    12:      dtype_in: DType # dtype for A and B
renderer/__init__.py:    13:      dtype_out: DType # dtype for C and D
renderer/__init__.py:    14:      threads: List[Tuple[int,int]] # list of (TC dim,amt) that construct the warp thread structure
renderer/__init__.py:    15:      thread_local_sizes: List[List[int]] # in each thread, the number of elements stored in registers for each TC dim

renderer/__init__.py:    19:    @dataclass(frozen=True)
renderer/__init__.py:    19:    class Program:
renderer/__init__.py:    20:      name:str
renderer/__init__.py:    21:      src:str
renderer/__init__.py:    22:      dname:str
renderer/__init__.py:    23:      global_size:Optional[List[int]]=None
renderer/__init__.py:    24:      local_size:Optional[List[int]]=None
renderer/__init__.py:    25:      uops:Optional[UOpGraph]=None
renderer/__init__.py:    26:      op_estimate:sint=0
renderer/__init__.py:    27:      mem_estimate:sint=0

renderer/__init__.py:    46:    class Renderer:
renderer/__init__.py:    47:      device: str = ""
renderer/__init__.py:    48:      suffix: str = ""
renderer/__init__.py:    50:      supports_float4: bool = True
renderer/__init__.py:    51:      has_local: bool = True
renderer/__init__.py:    52:      has_shared: bool = True
renderer/__init__.py:    54:      global_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
renderer/__init__.py:    55:      local_max: Optional[Tuple[int, ...]] = (0x8FFFFFFF,) * (3) # TODO: UOps.SPECIAL int32 indexes right now
renderer/__init__.py:    56:      shared_max: int = 32768
renderer/__init__.py:    57:      tensor_cores: List[TensorCore] = []

device.py           :    13:    class _Device:

device.py           :    39:  Device = _Device()

device.py           :    13:    class _Device:
device.py           :    14:      def __init__(self) -> None: self._devices: List[str] = [x.stem[len("ops_"):].upper() for x in (pathlib.Path(__file__).parent/"runtime").iterdir() if x.stem.startswith("ops_")]  # noqa: E501

device.py           :    44:    @dataclass(frozen=True, eq=True)
device.py           :    44:    class BufferOptions:
device.py           :    45:      image: Optional[ImageDType] = None
device.py           :    46:      uncached: bool = False
device.py           :    47:      cpu_access: bool = False
device.py           :    48:      host: bool = False
device.py           :    49:      nolru: bool = False

device.py           :    51:    class Buffer:

device.py           :   131:    # TODO: size, dest, src are the same type. can we enforce this?
device.py           :   131:    class Allocator:

device.py           :   142:    class LRUAllocator(Allocator):  # pylint: disable=abstract-method
device.py           :   143:      """
device.py           :   143:      The LRU Allocator is responsible for caching buffers.
device.py           :   143:      It ensures that buffers are not freed until it is absolutely necessary, optimizing performance.
device.py           :   143:      """

device.py           :   162:    class _MallocAllocator(LRUAllocator):

device.py           :   169:  MallocAllocator = _MallocAllocator()

device.py           :   142:    class LRUAllocator(Allocator):  # pylint: disable=abstract-method
device.py           :   147:      def __init__(self): self.cache: Dict[Tuple[int, Optional[BufferOptions]], Any] = defaultdict(list)

device.py           :   173:    class CompileError(Exception): pass

device.py           :   175:    class Compiler:

device.py           :   185:    class Compiled:

device.py           :   217:    class HWCommandQueue:
device.py           :   218:      """
device.py           :   218:      A base class for hardware command queues in the HCQ (Hardware Command Queue) API.
device.py           :   218:      Both compute and copy queues should have the following commands implemented.
device.py           :   218:      """

device.py           :   199:      def hcq_command(func):
device.py           :   215:        return __wrapper

device.py           :   301:    class HWComputeQueue(HWCommandQueue):

device.py           :   338:    class HWCopyQueue(HWCommandQueue):

device.py           :   366:    class HCQSignal:

device.py           :   413:    class HCQProgram:

device.py           :   418:    class HCQCompiled(Compiled):
device.py           :   419:      """
device.py           :   419:      A base class for devices compatible with the HCQ (Hardware Command Queue) API.
device.py           :   419:      """

device.py           :   468:    # Protocol for hcq compatible allocators for allocated buffers to contain VA address and it's size.
device.py           :   468:    class HCQBuffer(Protocol): va_addr:int; size:int # noqa: E702

device.py           :   470:    class HCQAllocator(LRUAllocator): # pylint: disable=abstract-method
device.py           :   471:      """
device.py           :   471:      A base allocator class compatible with the HCQ (Hardware Command Queue) API.
device.py           :   471:      This class implements basic copy operations following the HCQ API, utilizing both `HWComputeQueue` and `HWCopyQueue`.
device.py           :   471:      """

lazy.py             :    11:  lazycache: WeakValueDictionary[Any, LazyBuffer] = WeakValueDictionary()
lazy.py             :    24:  view_supported_devices = {"LLVM", "CLANG", "CUDA", "NV", "AMD", "METAL", "DISK"}

lazy.py             :    25:    class LazyBuffer:
lazy.py             :    27:                   op:Optional[Op]=None, arg:Any=None, srcs:Tuple[LazyBuffer, ...]=(),
lazy.py             :    28:                   base:Optional[LazyBuffer]=None, metadata:Optional[Metadata]=None):
lazy.py             :    28:        self.device, self.st, self.dtype, self.shape, self.size, self.metadata = device, st, dtype, st.shape, st.size, metadata
lazy.py             :    28:        self._base: Optional[LazyBuffer] = None
lazy.py             :    28:        if base is None:
lazy.py             :    28:          # properties on base
lazy.py             :    28:          self.op, self.arg, self.srcs = op, arg, srcs  # this is a LazyOp, except the src is LazyBuffers and not LazyOps
lazy.py             :    28:          assert self.op is not MetaOps.ASSIGN or srcs[1].base.realized is not None, "assign target must be realized"
lazy.py             :    28:          if self.op is MetaOps.VIEW:
lazy.py             :    28:            # some LazyBuffers can be processed with only a view, no AST required
lazy.py             :    28:            self.buffer: Buffer = srcs[0].base.buffer.view(st.size, dtype, srcs[0].st.views[0].offset * srcs[0].dtype.itemsize)
lazy.py             :    28:          else:
lazy.py             :    28:            self.buffer = srcs[1].base.buffer if self.op is MetaOps.ASSIGN else Buffer(device, self.size, dtype)
lazy.py             :    28:          self.buffer.ref(1)
lazy.py             :    28:          self.contiguous_child: Optional[Tuple[ReferenceType[LazyBuffer], ShapeTracker]] = None
lazy.py             :    28:          self.forced_realize = False
lazy.py             :    28:        else:
lazy.py             :    28:          # properties on view
lazy.py             :    28:          assert base.base == base, "base must be a base itself"
lazy.py             :    28:          self._base = base
lazy.py             :    28:      def __del__(self):
lazy.py             :    28:        if hasattr(self, 'buffer'): self.buffer.ref(-1)
lazy.py             :    28:      def __repr__(self) -> str:
lazy.py             :    28:        return f"<LB {self.device} {self.shape} {str(self.dtype)[7:]} {self.st if self.base != self else (self.op, self.realized)}>"
lazy.py             :    28:      @property
lazy.py             :    28:      def realized(self) -> Optional[Buffer]:
lazy.py             :    28:        # NOTE: we check for a lack of srcs instead of an allocated buffer to make unrealized assigns return None here
lazy.py             :    28:        return self.buffer if self._base is None and not hasattr(self, 'srcs') else None
lazy.py             :    28:      # NOTE: this has to be a function to prevent self reference
lazy.py             :    28:      @property
lazy.py             :    28:      def base(self) -> LazyBuffer: return self._base if self._base is not None else self
lazy.py             :    28:      # same API as multi
lazy.py             :    28:      @property
lazy.py             :    28:      def lbs(self) -> List[LazyBuffer]: return [self]
lazy.py             :    28:      @staticmethod
lazy.py             :    28:      def metaop(op, shape:Tuple[sint,...], dtype:DType, device:str, arg=None, src:Tuple[LazyBuffer, ...]=(), enable_cache=False) -> LazyBuffer:
lazy.py             :    28:        assert isinstance(src, tuple)
lazy.py             :    28:        return create_lazybuffer(device, ShapeTracker.from_shape(shape), dtype, op, arg, src, enable_cache=enable_cache)
lazy.py             :    28:      def const(self, val:ConstType, shape:Optional[Tuple[sint,...]]=None) -> LazyBuffer:
lazy.py             :    28:        assert isinstance(val, (int,float,bool)), f"{val=} has {type(val)=}, not a ConstType"
lazy.py             :    28:        shape = self.shape if shape is None else shape
lazy.py             :    28:        return LazyBuffer.metaop(MetaOps.CONST, tuple(), self.dtype, self.device, arg=val).reshape((1,)*len(shape)).expand(shape)
lazy.py             :    28:      def is_realized(self) -> bool: return self.base.realized is not None
lazy.py             :    28:      def assign(self, x:LazyBuffer) -> LazyBuffer:
lazy.py             :    28:        assert x.size == self.size, f"assign target must have same size {self.size=} != {x.size=}"
lazy.py             :    28:        return LazyBuffer.metaop(MetaOps.ASSIGN, self.shape, self.dtype, self.device, arg=() if self.st.contiguous else (self.st,), src=(x, self.base))
lazy.py             :    28:      def can_view(self): return self.st.consecutive and not self.is_unrealized_const() and self.device.split(":")[0] in view_supported_devices
lazy.py             :    28:      def contiguous(self, allow_buffer_view=True):
lazy.py             :    28:        if not self.st.contiguous or self.size != self.base.size or self.is_unrealized_const():
lazy.py             :    28:          ret = self.e(MetaOps.VIEW) if allow_buffer_view and self.can_view() else self.e(MetaOps.CONTIGUOUS)
lazy.py             :    28:          if (sti := self.st.invert(self.base.shape)) is not None: self.base.contiguous_child = ref(ret), sti
lazy.py             :    28:          return ret
lazy.py             :    28:        self.base.forced_realize = True
lazy.py             :    28:        return self
lazy.py             :    28:      def cast(self, dtype:DType, bitcast:bool=False, allow_buffer_view=True):
lazy.py             :    28:        if self.dtype == dtype: return self
lazy.py             :    28:        if self.device.startswith("DISK") and not bitcast: raise RuntimeError("attempted to cast disk buffer (bitcast only)")
lazy.py             :    28:        if self.is_unrealized_unmasked_const() and not bitcast:
lazy.py             :    28:          return create_lazybuffer(self.device, self.st, dtype, MetaOps.CONST, dtypes.as_const(self.base.arg, dtype))
lazy.py             :    28:        new_shape = self.shape
lazy.py             :    28:        if bitcast and self.dtype.itemsize != dtype.itemsize:
lazy.py             :    28:          if not self.device.startswith("DISK"): raise RuntimeError("shape changing bitcast only supported on DISK right now")
lazy.py             :    28:          if not all_int(new_shape): raise RuntimeError("shape changing bitcast with symbolic shape isn't supported yet")
lazy.py             :    28:          # https://pytorch.org/docs/stable/generated/torch.Tensor.view.html
lazy.py             :    28:          if not (new_shape[-1]*self.dtype.itemsize) % dtype.itemsize == 0: raise RuntimeError("unsupported size in bitcast")
lazy.py             :    28:          new_shape = new_shape[:-1] + ((new_shape[-1]*self.dtype.itemsize) // dtype.itemsize,)
lazy.py             :    28:        elif getenv("CAST_BEFORE_VIEW", 1) and dtype.itemsize <= self.dtype.itemsize and self != self.base:
lazy.py             :    28:          # TODO: applying this makes gpt2 slower
lazy.py             :    28:          return self.base.cast(dtype, bitcast)._view(self.st)
lazy.py             :    28:        cast_op: Union[MetaOps, UnaryOps] = (MetaOps.VIEW if self.can_view() and allow_buffer_view else UnaryOps.BITCAST) if bitcast else UnaryOps.CAST
lazy.py             :    28:        return create_lazybuffer(self.device, ShapeTracker.from_shape(new_shape), dtype, cast_op, dtype, (self,))
lazy.py             :    28:      def is_unrealized_const(self): return self.base.realized is None and self.base.op is MetaOps.CONST and not isinstance(self.base.arg, Variable)
lazy.py             :    28:      def is_unrealized_unmasked_const(self): return self.is_unrealized_const() and all(v.mask is None for v in self.st.views)
lazy.py             :    28:      def _copy(self, device:str) -> LazyBuffer:
lazy.py             :    28:        return create_lazybuffer(device, ShapeTracker.from_shape(self.shape), self.dtype, MetaOps.COPY, self.buffer.nbytes, (self,), enable_cache=False)
lazy.py             :    28:      def copy_to_device(self, device:str, force: bool = False) -> LazyBuffer:
lazy.py             :    28:        # no COPY
lazy.py             :    28:        if self.device == device: return self
lazy.py             :    28:        # double COPY = one COPY
lazy.py             :    28:        if not force and self.st.contiguous and self.size == self.base.size and not self.base.realized and self.base.op is MetaOps.COPY:
lazy.py             :    28:          return self.base.srcs[0].copy_to_device(device).reshape(self.st.shape)
lazy.py             :    28:        # const doesn't have to be copied (issues with disk tensor)
lazy.py             :    28:        if self.is_unrealized_const():
lazy.py             :    28:          return LazyBuffer.metaop(MetaOps.CONST, tuple(), self.dtype, device, arg=self.base.arg)._view(self.st)
lazy.py             :    28:        # if it's a shrink, do the shrink before the copy with CONTIGUOUS
lazy.py             :    28:        if prod(self.st.shape) < prod(self.base.st.shape): return self.contiguous()._copy(device)
lazy.py             :    28:        # copy the base and apply the shapetracker on the new device
lazy.py             :    28:        return self.base._copy(device)._view(self.st)
lazy.py             :    28:      def e(self, op:Union[MetaOps, UnaryOps, BinaryOps, TernaryOps], *in_srcs:LazyBuffer, arg:Optional[Any]=None) -> LazyBuffer:
lazy.py             :    28:        srcs: List[LazyBuffer] = []
lazy.py             :    28:        for s in (self,)+in_srcs:
lazy.py             :    28:          if s == s.base and s.base.contiguous_child and (root:=s.base.contiguous_child[0]()) is not None:
lazy.py             :    28:            srcs.append(root._view(s.base.contiguous_child[1]))
lazy.py             :    28:          else:
lazy.py             :    28:            srcs.append(s)
lazy.py             :    28:        assert all_same(dts:=[x.dtype.scalar() for x in (srcs[1:] if op is TernaryOps.WHERE else srcs)]), f"all dtypes must match {dts} on {op}"
lazy.py             :    28:        assert all_same([x.shape for x in srcs]), f"all shapes must be the same {[x.shape for x in srcs]}"
lazy.py             :    28:        if op is TernaryOps.WHERE: assert srcs[0].dtype == dtypes.bool, "TernaryOps.WHERE must have the first arg be bool"
lazy.py             :    28:        if op is UnaryOps.NEG: assert srcs[0].dtype != dtypes.bool, "UnaryOps.NEG does not accept dtype bool"
lazy.py             :    28:        out_dtype = dtypes.bool if op in (BinaryOps.CMPLT, BinaryOps.CMPNE) else srcs[-1].dtype
lazy.py             :    28:        # const folding
lazy.py             :    28:        if op in python_alu and all(s.is_unrealized_unmasked_const() for s in srcs):
lazy.py             :    28:          return self.cast(out_dtype).const(exec_alu(op, out_dtype, [s.base.arg for s in srcs]))
lazy.py             :    28:        if op is UnaryOps.NEG and self.base.op is UnaryOps.NEG and self.base.realized is None: return self.base.srcs[0]
lazy.py             :    28:        if op in BinaryOps:
lazy.py             :    28:          x, y = self, in_srcs[0]
lazy.py             :    28:          if op is BinaryOps.ADD:
lazy.py             :    28:            if y.is_unrealized_unmasked_const() and y.base.arg == 0: return x
lazy.py             :    28:            if x.is_unrealized_unmasked_const() and x.base.arg == 0: return y
lazy.py             :    28:          if op is BinaryOps.MUL:
lazy.py             :    28:            if x.is_unrealized_unmasked_const() and (val := x.base.arg) in (1, 0, -1):
lazy.py             :    28:              return y if val == 1 else y.const(0) if val == 0 else y.e(UnaryOps.NEG)
lazy.py             :    28:            if y.is_unrealized_unmasked_const() and (val := y.base.arg) in (1, 0, -1):
lazy.py             :    28:              return x if val == 1 else x.const(0) if val == 0 else x.e(UnaryOps.NEG)
lazy.py             :    28:        return create_lazybuffer(self.device, ShapeTracker.from_shape(self.shape), out_dtype, op, arg, tuple(srcs))
lazy.py             :    28:      # *** reduce ops ***
lazy.py             :    28:      def _reduce_op(self, op:ReduceOps, axis:Tuple[int, ...]) -> LazyBuffer:
lazy.py             :    28:        assert all(0 <= x < len(self.shape) for x in axis), f"axis args {axis} out of range for shape {self.shape}"
lazy.py             :    28:        axis = tuple(sorted([x for x in axis if self.shape[x] != 1]))
lazy.py             :    28:        if len(axis) == 0: return self
lazy.py             :    28:        return create_lazybuffer(self.device, ShapeTracker.from_shape(reduce_st(self.st, axis)), self.dtype, op, axis, (self,))
lazy.py             :    28:      def r(self, op:ReduceOps, axis:Tuple[int, ...]) -> LazyBuffer:
lazy.py             :    28:        new_shape = reduce_st(self.st, axis)
lazy.py             :    28:        # TODO: this logic should move to the scheduler
lazy.py             :    28:        if 0 in self.shape and 0 not in new_shape: return self.const({ReduceOps.SUM: 0.0, ReduceOps.MAX: dtypes.min(self.dtype)}[op], new_shape)
lazy.py             :    28:        # const folding
lazy.py             :    28:        # TODO: fold this for symbolic?
lazy.py             :    28:        if self.is_unrealized_unmasked_const() and all_int(self.shape):
lazy.py             :    28:          return self.const(self.base.arg * {ReduceOps.SUM: prod(self.shape[i] for i in axis), ReduceOps.MAX: 1}[op], new_shape)
lazy.py             :    28:        # TODO: can we split symbolic shape if the reduce axis is not symbolic?
lazy.py             :    28:        if not getenv("SPLIT_REDUCEOP", 1) or not all_int(self.shape) or (0 in self.shape) or \
lazy.py             :    28:          prod(self.shape) // prod(new_shape) < getenv("REDUCEOP_SPLIT_THRESHOLD", 32768):
lazy.py             :    28:          return self._reduce_op(op, axis)
lazy.py             :    28:        # if there are few globals, make some reduces into globals by splitting into two kernels
lazy.py             :    28:        # cap output buffer to 2**22: heuristic number of global outputs to achieve max occupancy with enough locals+upcasts for gemm
lazy.py             :    28:        #   ~2**10 should be enough if GROUP is used
lazy.py             :    28:        # 256 split maximum should be "negligible reduce" for low prod(new_shape), 8 split minimum.
lazy.py             :    28:        # split is moved to the end to provide maximum locality for the second phase reduce.
lazy.py             :    28:        self_real_strides = self.st.real_strides(ignore_valid=True)
lazy.py             :    28:        split_candidates = [(i, x) for i in axis for x in range(min(256,2**getenv("REDUCEOP_SPLIT_SIZE",22)//prod(new_shape)),8-1,-1)

multi.py            :    50:    class MultiLazyBuffer:

codegen/lowerer.py  :    15:  render_ops: Any = { NumNode: lambda self, ops, ctx: UOp.const(dtypes.bigint, self.b),
codegen/lowerer.py  :    15:                      MulNode: lambda self, ops, ctx: self.a.render(ops, ctx)*variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      DivNode: lambda self, ops, ctx: self.a.render(ops, ctx)//variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      ModNode: lambda self, ops, ctx: self.a.render(ops, ctx)%variable_to_uop(self.b, ctx),
codegen/lowerer.py  :    15:                      LtNode: lambda self, ops, ctx: self.a.render(ops, ctx).lt(variable_to_uop(self.b, ctx)),
codegen/lowerer.py  :    15:    Variable: lambda self,ops,ctx: ctx[self] if ctx is not None and self in ctx else UOp(UOps.DEFINE_VAR, dtypes.int32, (), self),
codegen/lowerer.py  :    15:    SumNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a+b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)),
codegen/lowerer.py  :    15:    AndNode: lambda self,ops,ctx: functools.reduce(lambda a,b: a*b.render(ops, ctx), self.nodes[1:], self.nodes[0].render(ops,ctx)) }
codegen/lowerer.py  :    24:  if getenv("UOP_IS_SYMBOLIC"):
codegen/lowerer.py  :    90:    class IndependentLowerer:

codegen/kernel.py   :    22:    class OptOps(Enum):
codegen/kernel.py   :    23:      TC = auto(); UPCAST = auto(); UPCASTMID = auto(); UNROLL = auto(); LOCAL = auto() # noqa: E702
codegen/kernel.py   :    24:      GROUP = auto(); GROUPTOP = auto(); NOLOCALS = auto(); PADTO = auto(); MERGE = auto(); SWAP = auto() # noqa: E702

codegen/kernel.py   :    27:    class KernelOptError(Exception): pass

codegen/kernel.py   :    33:    @dataclass(frozen=True, order=True)
codegen/kernel.py   :    33:    class Opt:
codegen/kernel.py   :    34:      op: OptOps
codegen/kernel.py   :    35:      axis: Optional[int] = None
codegen/kernel.py   :    36:      amt: Optional[int] = None

codegen/kernel.py   :    45:    @dataclass
codegen/kernel.py   :    45:    class TensorCoreOptions:
codegen/kernel.py   :    46:      axes: Tuple[int, ...] # the location of the original N and M axes if still in the shape
codegen/kernel.py   :    47:      axes_exist: Tuple[bool, ...] # true if the original N and M axes are still in the shape
codegen/kernel.py   :    48:      axis_pads: Tuple[Tuple[int, int], ...]

codegen/kernel.py   :    56:    class Kernel:
codegen/kernel.py   :   638:      kernel_cnt: Final[DefaultDict[str, int]] = defaultdict(int)

engine/graph.py     :    11:  with contextlib.suppress(ImportError): import networkx as nx
engine/graph.py     :    15:  if DEBUG >= 2:

helpers.py          :    91:    class ContextVar:
helpers.py          :   101:      def __ge__(self, x): return self.value >= x

engine/graph.py     :    27:  G:Any = None
engine/graph.py     :    34:  counts: DefaultDict[type, int] = defaultdict(int)
engine/graph.py     :    47:  top_colors = {MetaOps: '#FFFFa0', UnaryOps: "#c0c0c0", ReduceOps: "#FFA0A0", BinaryOps: "#c0c0c0",
engine/graph.py     :    47:                TernaryOps: "#c0c0c0", BufferOps: '#a0a0ff'}
engine/graph.py     :    77:  graph_uops_cnt = 0
engine/schedule.py  :    16:  sys.setrecursionlimit(10000)
engine/schedule.py  :    19:  logops = open(getenv("LOGOPS", ""), "a") if getenv("LOGOPS", "") else None
engine/schedule.py  :    24:    @dataclass(frozen=True)
engine/schedule.py  :    24:    class ScheduleItem:
engine/schedule.py  :    25:      ast: LazyOp
engine/schedule.py  :    26:      bufs: Tuple[Buffer, ...]
engine/schedule.py  :    27:      metadata: Optional[List[Metadata]] = None

engine/schedule.py  :   136:      children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]], assign_targets:Dict[LazyBuffer, LazyBuffer], scheduled=False):
engine/schedule.py  :   136:    """recursively search the entire graph for all LazyBuffers, insert realizes after expands"""
engine/schedule.py  :   136:    if buf in allbufs or buf.base.realized is not None: return
engine/schedule.py  :   136:    if GRAPH: log_lazybuffer(buf, scheduled)
engine/schedule.py  :   136:    # check if we need to realize views
engine/schedule.py  :   136:    if buf is not buf.base:
engine/schedule.py  :   136:      # fuse some pads
engine/schedule.py  :   136:      if len(buf.st.views) == 1 and buf.st.views[-1].mask is not None and all_int(buf.base.st.shape) and \
engine/schedule.py  :   136:          prod(buf.base.st.shape) >= prod([y-x for x,y in buf.st.views[-1].mask]):
engine/schedule.py  :   136:        simple_pads[buf.base] = None
engine/schedule.py  :   136:      # realize all expands
engine/schedule.py  :   136:      elif prod(buf.base.st.shape) < prod(buf.st.shape):
engine/schedule.py  :   136:        # this was causing "test_lil_model" to fail
engine/schedule.py  :   136:        if buf.base.op is UnaryOps.CAST and isinstance(buf.base.srcs[0].dtype, ImageDType) and isinstance(buf.base.arg, ImageDType):
engine/schedule.py  :   136:          simple_pads[buf.base] = None # don't realize image to image casts. this is part of a larger problem
engine/schedule.py  :   136:        elif not FUSE_AS_ONE_KERNEL: realizes[buf.base] = None
engine/schedule.py  :   136:      # check all other pads for safe fusion
engine/schedule.py  :   136:      elif any(v.mask is not None for v in buf.st.views): simple_pads[buf.base] = None
engine/schedule.py  :   136:      return _recurse_lb(buf.base, realizes, allbufs, simple_pads, children, assign_targets)
engine/schedule.py  :   136:    allbufs[buf] = None
engine/schedule.py  :   136:    if buf.forced_realize or buf.op in MetaOps: realizes[buf] = None
engine/schedule.py  :   136:    if buf.op is MetaOps.ASSIGN:
engine/schedule.py  :   136:      assert buf.srcs[1].base is buf.srcs[1], f"assign must be to base {buf.srcs[1]}"
engine/schedule.py  :   136:      assert buf.srcs[1].realized is not None, f"assign must be already realized to schedule {buf.srcs[1]}"
engine/schedule.py  :   136:      assign_targets[buf.srcs[1]] = buf
engine/schedule.py  :   136:    if buf.op is MetaOps.COPY:
engine/schedule.py  :   136:      assert buf.srcs[0].st.contiguous and buf.srcs[0].size == buf.srcs[0].base.size, "can only copy contig"
engine/schedule.py  :   136:      realizes[buf.srcs[0].base] = None
engine/schedule.py  :   136:    if buf.op is MetaOps.VIEW: realizes[buf.srcs[0].base] = None
engine/schedule.py  :   136:    for x in buf.srcs:
engine/schedule.py  :   136:      if x.base.realized is None: children[x.base][buf] = None
engine/schedule.py  :   136:      _recurse_lb(x, realizes, allbufs, simple_pads, children, assign_targets)
engine/schedule.py  :   136:  def _is_padding_okay(buf:LazyBuffer, realizes:Dict[LazyBuffer, None]) -> bool:
engine/schedule.py  :   136:    if buf in realizes or buf.realized is not None: return True
engine/schedule.py  :   136:    # NOTE: this broke to_image_idx and coder with JIT
engine/schedule.py  :   136:    if buf.op in UNSAFE_PAD_OPS: return False
engine/schedule.py  :   136:    return all(_is_padding_okay(x.base, realizes) for x in buf.srcs)
engine/schedule.py  :   136:  def _recursive_group(tr:LazyBuffer, st:ShapeTracker, r:LazyBuffer, children:DefaultDict[LazyBuffer, Dict[LazyBuffer, None]],
engine/schedule.py  :   309:  SCHEDULES: List = []
engine/realize.py   :    15:  logkerns, logkerns_level = open(getenv("LOGKERNS", ""), "a") if getenv("LOGKERNS", "") else None, getenv("LOGKERNS_LEVEL", 1)
engine/realize.py   :    63:    class Runner:

engine/realize.py   :    73:    class CompiledRunner(Runner):

engine/realize.py   :   100:    class CustomOp(Runner):

engine/realize.py   :   106:    class EmptyOp(Runner):

engine/realize.py   :   110:    class ViewOp(Runner):

engine/realize.py   :   115:    class BufferCopy(Runner):

engine/realize.py   :   138:    class BufferXfer(BufferCopy):

engine/realize.py   :   147:  method_cache: Dict[Tuple[str, LazyOp, int, bool], CompiledRunner] = {}

engine/realize.py   :   165:    @dataclass(frozen=True)
engine/realize.py   :   165:    class ExecItem:
engine/realize.py   :   166:      prg: Runner
engine/realize.py   :   167:      bufs: List[Optional[Buffer]]
engine/realize.py   :   168:      metadata: Optional[List[Metadata]] = None

engine/realize.py   :   214:  capturing: List = []  # put classes with an add method in here
tensor.py           :    23:    class Function:

function.py         :     1:  """This is where the forwards and backwards passes live."""

function.py         :    11:    class Contiguous(Function):

function.py         :    15:    class ContiguousBackward(Function):

function.py         :    19:    class Cast(Function):

function.py         :    28:    class Neg(Function):

function.py         :    32:    class Reciprocal(Function):

function.py         :    39:    class Sin(Function):

function.py         :    48:    # NOTE: maximum(x, 0) behaves differently where x=0
function.py         :    48:    class Relu(Function):

function.py         :    56:    class Log(Function):

function.py         :    63:    class Exp(Function):

function.py         :    70:    class Sqrt(Function):

function.py         :    81:    # NOTE: the implicit derivative of sigmoid is not stable
function.py         :    81:    # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e
function.py         :    81:    # TODO: have the backend automatically find this
function.py         :    81:    class Sigmoid(Function):

function.py         :    89:    class Sign(Function):

function.py         :    98:    class Less(Function):

function.py         :   102:    class Neq(Function):

function.py         :   106:    class Xor(Function):

function.py         :   109:    class BitwiseAnd(Function):

function.py         :   112:    class BitwiseOr(Function):

function.py         :   115:    class Threefry(Function):

function.py         :   118:    class Add(Function):

function.py         :   125:    class Mul(Function):

function.py         :   134:    class Div(Function):

function.py         :   145:    class Where(Function):

function.py         :   157:    class Sum(Function):

function.py         :   164:    class Max(Function):

function.py         :   179:    # NOTE: this is sum in reverse
function.py         :   179:    class Expand(Function):

function.py         :   187:    class Reshape(Function):

function.py         :   194:    class Permute(Function):

function.py         :   201:    class Pad(Function):

function.py         :   208:    class Shrink(Function):

function.py         :   215:    class Flip(Function):

tensor.py           :    92:    class Tensor:
tensor.py           :    93:      """
tensor.py           :    93:      A `Tensor` is a multi-dimensional matrix containing elements of a single data type.
tensor.py           :    93:      ```python exec="true" session="tensor"
tensor.py           :    93:      from tinygrad import Tensor, dtypes, nn
tensor.py           :    93:      import numpy as np
tensor.py           :    93:      import math
tensor.py           :    93:      np.set_printoptions(precision=4)
tensor.py           :    93:      ```
tensor.py           :    93:      """
tensor.py           :   103:      __slots__ = "lazydata", "requires_grad", "grad", "_ctx"
tensor.py           :   104:      __deletable__ = ('_ctx',)
tensor.py           :   105:      training: ClassVar[bool] = False
tensor.py           :   106:      no_grad: ClassVar[bool] = False
tensor.py           :   109:                   device:Optional[Union[str, tuple, list]]=None, dtype:Optional[DType]=None, requires_grad:Optional[bool]=None):
tensor.py           :   109:        assert dtype is None or isinstance(dtype, DType), f"invalid dtype {dtype}"
tensor.py           :   109:        device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)
tensor.py           :   109:        # tensors can have gradients if you have called .backward
tensor.py           :   109:        self.grad: Optional[Tensor] = None
tensor.py           :   109:        # NOTE: this can be in three states. False and None: no gradient, True: gradient
tensor.py           :   109:        # None (the default) will be updated to True if it's put in an optimizer
tensor.py           :   109:        self.requires_grad: Optional[bool] = requires_grad
tensor.py           :   109:        # internal variable used for autograd graph construction
tensor.py           :   109:        self._ctx: Optional[Function] = None
tensor.py           :   109:        # create a LazyBuffer from the different types of inputs
tensor.py           :   109:        if isinstance(data, LazyBuffer): assert dtype is None or dtype == data.dtype, "dtype doesn't match, and casting isn't supported"
tensor.py           :   109:        elif isinstance(data, get_args(ConstType)): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data), device, data)
tensor.py           :   109:        elif isinstance(data, Variable): data = _metaop(MetaOps.CONST, tuple(), dtype or dtypes.from_py(data.unbind()[1]), device, data)
tensor.py           :   109:        elif isinstance(data, bytes): data = _frompy(data, dtypes.uint8 if dtype is None else dtype)
tensor.py           :   109:        elif isinstance(data, (list, tuple)):
tensor.py           :   109:          if dtype is None:
tensor.py           :   109:            if (d := fully_flatten(data)) and all(isinstance(s, bool) for s in d): dtype = dtypes.bool
tensor.py           :   109:            else: dtype = dtypes.default_int if d and all_int(d) else dtypes.default_float
tensor.py           :   109:          if dtype == dtypes.bfloat16: data = Tensor(_fromnp(np.array(data, np.float32)), device=device).cast(dtypes.bfloat16).lazydata
tensor.py           :   109:          else: data = _fromnp(np.array(data).astype(_to_np_dtype(dtype)))
tensor.py           :   109:        elif data is None: data = _metaop(MetaOps.EMPTY, (0,), dtype or dtypes.default_float, device)
tensor.py           :   109:        elif isinstance(data, np.ndarray):
tensor.py           :   109:          if data.shape == (): data = _metaop(MetaOps.CONST, tuple(), dtype or _from_np_dtype(data.dtype), device, data.item())
tensor.py           :   109:          else: data = _fromnp(data.astype(npdtype) if dtype is not None and (npdtype:=_to_np_dtype(dtype)) is not None else data)
tensor.py           :   109:        # by this point, it has to be a LazyBuffer
tensor.py           :   109:        if not isinstance(data, (LazyBuffer, MultiLazyBuffer)):
tensor.py           :   109:          raise RuntimeError(f"can't create Tensor from {data!r} with type {type(data)}")
tensor.py           :   109:        # data is a LazyBuffer, but it might be on the wrong device
tensor.py           :   109:        if isinstance(device, tuple):
tensor.py           :   109:          # if device is a tuple, we should have/construct a MultiLazyBuffer
tensor.py           :   109:          if isinstance(data, MultiLazyBuffer):
tensor.py           :   109:            assert data.device == device, f"MultiLazyBuffer device mismatch, {data.device} != {device}"
tensor.py           :   109:            self.lazydata: Union[LazyBuffer, MultiLazyBuffer] = data
tensor.py           :   109:          else:
tensor.py           :   109:            self.lazydata = MultiLazyBuffer.from_sharded(data, device, None)
tensor.py           :   109:        else:
tensor.py           :   109:          self.lazydata = data if data.device == device else data.copy_to_device(device)
tensor.py           :   109:      class train(ContextDecorator):
tensor.py           :   109:        def __init__(self, mode:bool = True): self.mode = mode
tensor.py           :   109:        def __enter__(self): self.prev, Tensor.training = Tensor.training, self.mode
tensor.py           :   109:        def __exit__(self, exc_type, exc_value, traceback): Tensor.training = self.prev
tensor.py           :   109:      class inference_mode(ContextDecorator):
tensor.py           :   109:        def __init__(self, mode:bool = True): self.mode = mode
tensor.py           :   109:        def __enter__(self): self.prev, Tensor.no_grad = Tensor.no_grad, self.mode
tensor.py           :   109:        def __exit__(self, exc_type, exc_value, traceback): Tensor.no_grad = self.prev
tensor.py           :   109:      def __repr__(self):
tensor.py           :   109:        return f"<Tensor {self.lazydata!r} on {self.device} with grad {(self.grad.lazydata if self.grad is not None else None)!r}>"
tensor.py           :   109:      # Python has a non moving GC, so this should be okay
tensor.py           :   109:      def __hash__(self): return id(self)
tensor.py           :   109:      def __bool__(self): raise TypeError("__bool__ on Tensor is not defined")
tensor.py           :   109:      def __len__(self):
tensor.py           :   109:        if not self.shape: raise TypeError("len() of a 0-d tensor")
tensor.py           :   109:        return self.shape[0]
tensor.py           :   109:      @property
tensor.py           :   109:      def device(self) -> Union[str, Tuple[str, ...]]: return self.lazydata.device
tensor.py           :   109:      @property
tensor.py           :   109:      def shape(self) -> Tuple[sint, ...]: return self.lazydata.shape
tensor.py           :   109:      @property
tensor.py           :   109:      def dtype(self) -> DType: return self.lazydata.dtype
tensor.py           :   109:      # ***** data handlers ****
tensor.py           :   109:      def schedule_with_vars(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> Tuple[List[ScheduleItem], Dict[Variable, int]]:
tensor.py           :   109:        """Creates the schedule needed to realize these Tensor(s), with Variables."""
tensor.py           :   109:        if getenv("FUZZ_SCHEDULE"):
tensor.py           :   109:          from test.external.fuzz_schedule import fuzz_schedule
tensor.py           :   109:          fuzz_schedule(flatten([x.lazydata.lbs for x in (self,)+lst]))
tensor.py           :   109:        schedule, var_vals = create_schedule_with_vars(flatten([x.lazydata.lbs for x in (self,)+lst]), seen)
tensor.py           :   109:        return memory_planner(schedule), var_vals
tensor.py           :   109:      def schedule(self, *lst:Tensor, seen:Optional[Set[LazyBuffer]]=None) -> List[ScheduleItem]:
tensor.py           :   109:        """Creates the schedule needed to realize these Tensor(s)."""
tensor.py           :   109:        schedule, var_vals = self.schedule_with_vars(*lst, seen=seen)
tensor.py           :   109:        assert len(var_vals) == 0
tensor.py           :   109:        return schedule
tensor.py           :   109:      def realize(self, *lst:Tensor, do_update_stats=True) -> Tensor:
tensor.py           :   109:        """Triggers the computation needed to create these Tensor(s)."""
tensor.py           :   109:        run_schedule(*self.schedule_with_vars(*lst), do_update_stats=do_update_stats)
tensor.py           :   109:        return self
tensor.py           :   109:      def replace(self, x:Tensor) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Replaces the data of this tensor with the data of another tensor. Only the shape of the tensors must match.
tensor.py           :   109:        """
tensor.py           :   109:        # used for replacing a Tensor with a new version of it (potentially with a different device and dtype)
tensor.py           :   109:        assert not x.requires_grad and getattr(self, '_ctx', None) is None
tensor.py           :   109:        assert self.shape == x.shape, f"replace shape mismatch {self.shape} != {x.shape}"
tensor.py           :   109:        self.lazydata = x.lazydata
tensor.py           :   109:        return self
tensor.py           :   109:      def assign(self, x) -> Tensor:
tensor.py           :   109:        # TODO: this is a hack for writing to DISK. remove with working assign
tensor.py           :   109:        if isinstance(self.device, str) and self.device.startswith("DISK"):
tensor.py           :   109:          if x.__class__ is not Tensor: x = Tensor(x, device="NPY", dtype=self.dtype)
tensor.py           :   109:          self.contiguous().realize().lazydata.base.realized.copyin(x.numpy().data)
tensor.py           :   109:          return self
tensor.py           :   109:        if x.__class__ is not Tensor: x = Tensor(x, device=self.device, dtype=self.dtype)
tensor.py           :   109:        if DEBUG >= 4: print(f"assign {self.lazydata} <- {x.lazydata}")
tensor.py           :   109:        if self.lazydata is x.lazydata: return self  # a self assign is a NOOP
tensor.py           :   109:        # NOTE: we allow cross device assign
tensor.py           :   109:        assert self.shape == x.shape, f"assign shape mismatch {self.shape} != {x.shape}"
tensor.py           :   109:        assert self.device == x.device, f"assign device mismatch {self.device} != {x.device}"
tensor.py           :   109:        assert self.dtype == x.dtype, f"assign dtype mismatch {self.dtype} != {x.dtype}"
tensor.py           :   109:        assert not isinstance(self.lazydata, MultiLazyBuffer) or self.lazydata.axis == x.lazydata.axis, "axis must match on MultiLazyBuffer"
tensor.py           :   109:        assert not x.requires_grad  # self requires_grad is okay?
tensor.py           :   109:        if not self.lazydata.is_realized(): return self.replace(x)
tensor.py           :   109:        self.lazydata = self.lazydata.assign(x.lazydata)
tensor.py           :   109:        return self
tensor.py           :   109:      def detach(self) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Returns a new tensor with the same data as this tensor, but detached from the autograd graph.
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor(self.lazydata, device=self.device, requires_grad=False)
tensor.py           :   109:      def _data(self) -> memoryview:
tensor.py           :   109:        if 0 in self.shape: return memoryview(bytearray(0))
tensor.py           :   109:        # NOTE: this realizes on the object from as_buffer being a Python object
tensor.py           :   109:        cpu = self.cast(self.dtype.scalar()).contiguous().to("CLANG").realize()
tensor.py           :   109:        buf = cast(Buffer, cast(LazyBuffer, cpu.lazydata).base.realized)
tensor.py           :   109:        if self.device != "CLANG": buf.options = BufferOptions(nolru=True)
tensor.py           :   109:        return buf.as_buffer(allow_zero_copy=True if self.device != "CLANG" else False)
tensor.py           :   109:      def data(self) -> memoryview:
tensor.py           :   109:        """
tensor.py           :   109:        Returns the data of this tensor as a memoryview.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor([1, 2, 3, 4])
tensor.py           :   109:        print(np.frombuffer(t.data(), dtype=np.int32))
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        assert self.dtype.fmt is not None, f"no fmt dtype for {self.dtype}"
tensor.py           :   109:        assert all_int(self.shape), f"no data if shape is symbolic, {self.shape=}"
tensor.py           :   109:        return self._data().cast(self.dtype.fmt, self.shape)
tensor.py           :   109:      def item(self) -> ConstType:
tensor.py           :   109:        """
tensor.py           :   109:        Returns the value of this tensor as a standard Python number.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor(42)
tensor.py           :   109:        print(t.item())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        assert self.dtype.fmt is not None, f"no fmt dtype for {self.dtype}"
tensor.py           :   109:        assert self.numel() == 1, "must have one element for item"
tensor.py           :   109:        return self._data().cast(self.dtype.fmt)[0]
tensor.py           :   109:      # TODO: should be Tensor.tolist() -> Union[List[ConstType], ConstType]. The List is Sequence because mypy expects memoryview.tolist() -> list[int]
tensor.py           :   109:      # src: https://github.com/python/mypy/blob/release-1.6/mypy/typeshed/stdlib/builtins.pyi#L803
tensor.py           :   109:      def tolist(self) -> Union[Sequence[ConstType], ConstType]:
tensor.py           :   109:        """
tensor.py           :   109:        Returns the value of this tensor as a nested list.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor([1, 2, 3, 4])
tensor.py           :   109:        print(t.tolist())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return self.data().tolist()
tensor.py           :   109:      def numpy(self) -> np.ndarray:
tensor.py           :   109:        """
tensor.py           :   109:        Returns the value of this tensor as a `numpy.ndarray`.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor([1, 2, 3, 4])
tensor.py           :   109:        print(repr(t.numpy()))
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        if self.dtype == dtypes.bfloat16: return self.float().numpy()
tensor.py           :   109:        assert _to_np_dtype(self.dtype) is not None, f"no np dtype for {self.dtype}"
tensor.py           :   109:        assert all_int(self.shape), f"no data if shape is symbolic, {self.shape=}"
tensor.py           :   109:        return np.frombuffer(self._data(), dtype=_to_np_dtype(self.dtype)).reshape(self.shape)
tensor.py           :   109:      def to(self, device:Optional[Union[str, Tuple[str, ...]]]) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Moves the tensor to the given device.
tensor.py           :   109:        """
tensor.py           :   109:        device = tuple(Device.canonicalize(x) for x in device) if isinstance(device, (tuple, list)) else Device.canonicalize(device)
tensor.py           :   109:        if device == self.device: return self
tensor.py           :   109:        if not isinstance(device, str): return self.shard(device)
tensor.py           :   109:        ret = Tensor(self.lazydata, device, requires_grad=self.requires_grad)
tensor.py           :   109:        if self.grad is not None: ret.grad = self.grad.to(device)
tensor.py           :   109:        if hasattr(self, '_ctx'): ret._ctx = self._ctx
tensor.py           :   109:        return ret
tensor.py           :   109:      def to_(self, device:Optional[Union[str, Tuple[str, ...]]]):
tensor.py           :   109:        """
tensor.py           :   109:        Moves the tensor to the given device in place.
tensor.py           :   109:        """
tensor.py           :   109:        real = self.to(device)
tensor.py           :   109:        # TODO: is this assign?
tensor.py           :   109:        if self.grad is not None and real.grad is not None: self.grad.lazydata = real.grad.lazydata
tensor.py           :   109:        self.lazydata = real.lazydata
tensor.py           :   109:      def shard(self, devices:Tuple[str, ...], axis:Optional[int]=None) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Shards the tensor across the given devices.
tensor.py           :   109:        """
tensor.py           :   109:        assert isinstance(self.lazydata, LazyBuffer), "can't shard a MultiLazyBuffer"
tensor.py           :   109:        canonical_devices = tuple(Device.canonicalize(x) for x in devices)
tensor.py           :   109:        if axis is not None and axis < 0: axis += len(self.shape)
tensor.py           :   109:        return Tensor(MultiLazyBuffer.from_sharded(self.lazydata, canonical_devices, axis), device=canonical_devices, requires_grad=self.requires_grad)
tensor.py           :   109:      def shard_(self, devices:Tuple[str, ...], axis:Optional[int]=None):
tensor.py           :   109:        """
tensor.py           :   109:        Shards the tensor across the given devices in place.
tensor.py           :   109:        """
tensor.py           :   109:        self.lazydata = self.shard(devices, axis).lazydata
tensor.py           :   109:        return self
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def from_node(y:Node, **kwargs) -> Tensor:
tensor.py           :   109:        if isinstance(y, NumNode): return Tensor(y.b, **kwargs, requires_grad=False)
tensor.py           :   109:        if isinstance(y, Variable): return Tensor(y, **kwargs, requires_grad=False)
tensor.py           :   109:        if isinstance(y, MulNode): return Tensor.from_node(y.a, **kwargs) * y.b
tensor.py           :   109:        if isinstance(y, SumNode): return Tensor.from_node(y.nodes[0], **kwargs) + sum(y.nodes[1:])
tensor.py           :   109:        raise RuntimeError(f"unhandled Node {y}")
tensor.py           :   109:      # ***** creation entrypoint *****
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def _metaop(op, shape, device:Optional[Union[Tuple[str, ...], str]]=None, dtype:Optional[DType]=None, arg=None, **kwargs):
tensor.py           :   109:        if isinstance(device, tuple):
tensor.py           :   109:          return Tensor(MultiLazyBuffer([LazyBuffer.metaop(op, shape, dtype or dtypes.default_float, Device.canonicalize(d), arg) \
tensor.py           :   109:                                          for d in device], None), device, dtype, **kwargs)
tensor.py           :   109:        return Tensor(LazyBuffer.metaop(op, shape, dtype or dtypes.default_float, Device.canonicalize(device), arg), device, dtype, **kwargs)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def empty(*shape, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates an empty tensor with the given shape.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor.empty(2, 3)
tensor.py           :   109:        print(t.shape)
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor._metaop(MetaOps.EMPTY, argfix(*shape), **kwargs)
tensor.py           :   109:      _seed: int = int(time.time())
tensor.py           :   109:      _rng_counter: Optional[Tensor] = None
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def manual_seed(seed=0):
tensor.py           :   109:        """
tensor.py           :   109:        Sets the seed for random operations.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.rand(5).numpy())
tensor.py           :   109:        print(Tensor.rand(5).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)  # reset to the same seed
tensor.py           :   109:        print(Tensor.rand(5).numpy())
tensor.py           :   109:        print(Tensor.rand(5).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        Tensor._seed, Tensor._rng_counter = seed, Tensor([0], dtype=dtypes.uint32, requires_grad=False)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def rand(*shape, device:Optional[Union[Tuple[str, ...], str]]=None, dtype:Optional[DType]=None, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[0, 1)`.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        t = Tensor.rand(2, 3)
tensor.py           :   109:        print(t.numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        if Tensor._rng_counter is None: Tensor._rng_counter = Tensor([0], dtype=dtypes.uint32, requires_grad=False)
tensor.py           :   109:        if not THREEFRY.value:
tensor.py           :   109:          # for bfloat16, numpy rand passes buffer in float
tensor.py           :   109:          if (dtype or dtypes.default_float) == dtypes.bfloat16:
tensor.py           :   109:            return Tensor.rand(*shape, **kwargs, device=device, dtype=dtypes.float).cast(dtypes.bfloat16)
tensor.py           :   109:          return Tensor._metaop(MetaOps.CUSTOM, argfix(*shape), arg=custom_random, device=device, dtype=dtype, **kwargs)
tensor.py           :   109:        # threefry
tensor.py           :   109:        if (num := prod((shape:=argfix(*shape)))) == 0: return Tensor.zeros(shape, device=device, dtype=dtype, **kwargs)
tensor.py           :   109:        counts1 = (Tensor.arange(math.ceil(num / 2), device=device, dtype=dtypes.uint32, requires_grad=False)+Tensor._rng_counter.to(device)).realize()
tensor.py           :   109:        counts2 = counts1 + math.ceil(num / 2)
tensor.py           :   109:        Tensor._rng_counter.assign(Tensor._rng_counter + num).realize()
tensor.py           :   109:        x = counts2.cast(dtypes.uint64) << 32 | counts1.cast(dtypes.uint64)
tensor.py           :   109:        x = F.Threefry.apply(*x._broadcasted(Tensor._seed))
tensor.py           :   109:        counts1, counts2 = (x & 0xffffffff).cast(dtypes.uint32), ((x >> 32) & 0xffffffff).cast(dtypes.uint32)
tensor.py           :   109:        out = counts1.cat(counts2).rshift(8).cast(dtypes.float32).div(2 ** 24)[:num]
tensor.py           :   109:        out = out.reshape(shape).cast(dtypes.default_float if dtype is None else dtype)
tensor.py           :   109:        out.requires_grad = kwargs.get("requires_grad")
tensor.py           :   109:        return out.contiguous()
tensor.py           :   109:      # ***** creation helper functions *****
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def full(shape:Tuple[sint, ...], fill_value:ConstType, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with the given value.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.full((2, 3), 42).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.full((2, 3), False).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor(fill_value, **kwargs).reshape((1, )*len(new_shape := argfix(shape))).expand(new_shape)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def zeros(*shape, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with zeros.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.zeros(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.zeros(2, 3, dtype=dtypes.int32).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.full(argfix(*shape), 0.0, **kwargs)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def ones(*shape, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with ones.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.ones(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.ones(2, 3, dtype=dtypes.int32).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.full(argfix(*shape), 1.0, **kwargs)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def arange(start, stop=None, step=1, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Returns a 1-D tensor of size `ceil((stop - start) / step)` with values from `[start, stop)`, with spacing between values given by `step`.
tensor.py           :   109:        If `stop` is not specified, values are generated from `[0, start)` with the given `step`.
tensor.py           :   109:        If `stop` is specified, values are generated from `[start, stop)` with the given `step`.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.arange(5).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.arange(5, 10).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.arange(5, 10, 2).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.arange(5.5, 10, 2).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        if stop is None: stop, start = start, 0
tensor.py           :   109:        assert all(isinstance(s, (int, float)) for s in (start, stop, step)), f"symbolic arange not supported {start=}, {stop=}, {step=}"
tensor.py           :   109:        dtype = kwargs.pop("dtype", dtypes.default_float if any(isinstance(x, float) for x in (start, stop, step)) else dtypes.default_int)
tensor.py           :   109:        return (Tensor.full((math.ceil((stop-start)/step),), step, dtype=dtype, **kwargs)._cumsum() + (start - step)).cast(dtype)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def eye(n:int, m:Optional[int]=None, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Returns a 2-D tensor with `n` rows and `m` columns, with ones on the diagonal and zeros elsewhere.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.eye(3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        print(Tensor.eye(2, 4).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.ones((n,1),**kwargs).pad((None,(0,n))).flatten().shrink(((0,n*n),)).reshape(n,n)._slice((None,(0,n if m is None else m)))
tensor.py           :   109:      def full_like(self, fill_value:ConstType, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the same shape as `self`, filled with the given value.
tensor.py           :   109:        If `dtype` is not specified, the dtype of `self` is used.
tensor.py           :   109:        You can pass in the `device` keyword argument to control device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor.ones(2, 3)
tensor.py           :   109:        print(Tensor.full_like(t, 42).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.full(self.shape, fill_value, dtype=kwargs.pop("dtype", self.dtype), device=kwargs.pop("device", self.device), **kwargs)
tensor.py           :   109:      def zeros_like(self, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the same shape as `self`, filled with zeros.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor.ones(2, 3)
tensor.py           :   109:        print(Tensor.zeros_like(t).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return self.full_like(0, **kwargs)
tensor.py           :   109:      def ones_like(self, **kwargs):
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the same shape as `self`, filled with ones.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor.zeros(2, 3)
tensor.py           :   109:        print(Tensor.ones_like(t).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return self.full_like(1, **kwargs)
tensor.py           :   109:      # ***** rng hlops *****
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def randn(*shape, dtype:Optional[DType]=None, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random values from a normal distribution with mean `0` and standard deviation `1`.
tensor.py           :   109:        If `dtype` is not specified, the default type is used.
tensor.py           :   109:        You can pass in the `device` keyword argument to control device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.randn(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        # https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform
tensor.py           :   109:        src = Tensor.rand((2, *argfix(*shape)), **{**kwargs, "dtype": dtypes.float32})
tensor.py           :   109:        return src[0].mul(2*math.pi).cos().mul((1 - src[1]).log().mul(-2).sqrt()).cast(dtype or dtypes.default_float)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def randint(*shape, low=0, high=10, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random integer values generated uniformly from the interval `[low, high)`.
tensor.py           :   109:        If `dtype` is not specified, the default type is used.
tensor.py           :   109:        You can pass in the `device` keyword argument to control device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.randint(2, 3, low=5, high=10).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        if not isinstance(low, int) or not isinstance(high, int): raise TypeError(f"{low=} and {high=} must be integers")
tensor.py           :   109:        dtype = kwargs.pop("dtype", dtypes.int32)
tensor.py           :   109:        if not dtypes.is_int(dtype): raise TypeError(f"{dtype=} must be int")
tensor.py           :   109:        return Tensor.uniform(*shape, low=low, high=high, dtype=dtype, **kwargs)
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def normal(*shape, mean=0.0, std=1.0, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random values from a normal distribution with the given `mean` and standard deviation `std`.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.normal(2, 3, mean=10, std=2).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return (std * Tensor.randn(*shape, **kwargs)) + mean
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def uniform(*shape, low=0.0, high=1.0, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random values from a uniform distribution over the interval `[low, high)`.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.uniform(2, 3, low=2, high=10).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        dtype = kwargs.pop("dtype", dtypes.default_float)
tensor.py           :   109:        return ((high-low) * Tensor.rand(*shape, **kwargs)).cast(dtype) + low
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def scaled_uniform(*shape, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Creates a tensor with the given shape, filled with random values from a uniform distribution
tensor.py           :   109:        over the interval `[-prod(shape)**-0.5, prod(shape)**-0.5)`.
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.scaled_uniform(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul(prod(argfix(*shape))**-0.5)
tensor.py           :   109:      # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def glorot_uniform(*shape, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        <https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform>
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.glorot_uniform(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        return Tensor.uniform(*shape, low=-1.0, high=1.0, **kwargs).mul((6/(argfix(*shape)[0]+prod(argfix(*shape)[1:])))**0.5)
tensor.py           :   109:      # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def kaiming_uniform(*shape, a:float = 0.01, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        <https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_uniform_>
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.kaiming_uniform(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        bound = math.sqrt(3.0) * math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))
tensor.py           :   109:        return Tensor.uniform(*shape, low=-bound, high=bound, **kwargs)
tensor.py           :   109:      # https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_
tensor.py           :   109:      @staticmethod
tensor.py           :   109:      def kaiming_normal(*shape, a:float = 0.01, **kwargs) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        <https://pytorch.org/docs/stable/_modules/torch/nn/init.html#kaiming_normal_>
tensor.py           :   109:        You can pass in `dtype` and `device` keyword arguments to control the data type and device of the tensor.
tensor.py           :   109:        Additionally, all other keyword arguments are passed to the constructor of the tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        Tensor.manual_seed(42)
tensor.py           :   109:        print(Tensor.kaiming_normal(2, 3).numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        std = math.sqrt(2.0 / (1 + a ** 2)) / math.sqrt(prod(argfix(*shape)[1:]))
tensor.py           :   109:        return Tensor.normal(*shape, mean=0.0, std=std, **kwargs)
tensor.py           :   109:      def multinomial(self:Tensor, num_samples:int = 1, replacement:bool = False) -> Tensor:
tensor.py           :   109:        assert 1 <= self.ndim <= 2 and num_samples > 0, f"{self.ndim=} must be 1 or 2 dim, {num_samples=} must be positive"
tensor.py           :   109:        assert replacement or num_samples == 1, "no replacement only supports num_samples = 1"
tensor.py           :   109:        weight = self.unsqueeze(0) if self.ndim == 1 else self
tensor.py           :   109:        cdf = (cw := weight.cumsum(1).float()) / cw[:, -1].unsqueeze(1)
tensor.py           :   109:        unif_samples = Tensor.rand(num_samples, cdf.shape[0], 1, device=self.device)
tensor.py           :   109:        indices = (unif_samples.expand((-1, -1, cdf.shape[1])) >= cdf).sum(2).permute((1, 0))
tensor.py           :   109:        return (indices.squeeze(0) if self.ndim == 1 else indices).cast(dtypes.int32)
tensor.py           :   109:      # ***** toposort and backward pass *****
tensor.py           :   109:      def _deepwalk(self):
tensor.py           :   109:        def _walk(node, visited):
tensor.py           :   109:          visited.add(node)
tensor.py           :   109:          if getattr(node, "_ctx", None):
tensor.py           :   109:            for i in node._ctx.parents:
tensor.py           :   109:              if i not in visited: yield from _walk(i, visited)
tensor.py           :   109:            yield node
tensor.py           :   109:        return list(_walk(self, set()))
tensor.py           :   109:      def backward(self) -> Tensor:
tensor.py           :   109:        """
tensor.py           :   109:        Propagates the gradient of a tensor backwards through the computation graph.
tensor.py           :   109:        Must be used on a scalar tensor.
tensor.py           :   109:        ```python exec="true" source="above" session="tensor" result="python"
tensor.py           :   109:        t = Tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)
tensor.py           :   109:        t.sum().backward()
tensor.py           :   109:        print(t.grad.numpy())
tensor.py           :   109:        ```
tensor.py           :   109:        """
tensor.py           :   109:        assert self.shape == tuple(), f"backward can only be called for scalar tensors, but it has shape {self.shape})"
tensor.py           :   109:        # fill in the first grad with one. don't use Tensor.ones because we don't need contiguous
tensor.py           :   109:        # this is "implicit gradient creation"
tensor.py           :   109:        self.grad = Tensor(1.0, dtype=self.dtype, device=self.device, requires_grad=False)
tensor.py           :   109:        for t0 in reversed(self._deepwalk()):
tensor.py           :   109:          if t0.grad is None: raise RuntimeError(f"tensor {t0} has no grad")
tensor.py           :   109:          token = _METADATA.set(dataclasses.replace(md, backward=True) if (md := t0._ctx.metadata) is not None else None)
tensor.py           :   109:          grads = t0._ctx.backward(t0.grad.lazydata)
tensor.py           :   109:          _METADATA.reset(token)
tensor.py           :   109:          grads = [Tensor(g, device=self.device, requires_grad=False) if g is not None else None

tensor.py           :  3123:  for device in Device._devices: setattr(Tensor, f"{device.lower()}", functools.partialmethod(Tensor.to, device))
tensor.py           :  3125:  if IMAGE:

helpers.py          :    91:    class ContextVar:
helpers.py          :   100:      def __bool__(self): return bool(self.value)

tensor.py           :  3166:  if TRACEMETA >= 1:
tensor.py           :  3167:    for name, fn in inspect.getmembers(Tensor, inspect.isfunction):
tensor.py           :  3168:      if name in ["__class__", "__init__", "__repr__", "backward", "sequential"]: continue
tensor.py           :  3169:      setattr(Tensor, name, functools.wraps(fn)(_metadata_wrapper(fn)))

tensor.py           :  3138:    def _metadata_wrapper(fn):
tensor.py           :  3164:      return _wrapper

nn/optim.py         :     7:    class Optimizer:
nn/optim.py         :     8:      """
nn/optim.py         :     8:      Base class for all optimizers.
nn/optim.py         :     8:      """

nn/optim.py         :    45:    class OptimizerGroup(Optimizer):
nn/optim.py         :    46:      """
nn/optim.py         :    46:      Combines multiple optimizers into one.
nn/optim.py         :    46:      """

nn/optim.py         :    67:    class LARS(Optimizer):
nn/optim.py         :    68:      """
nn/optim.py         :    68:      Layer-wise Adaptive Rate Scaling (LARS) optimizer with optional momentum and weight decay.
nn/optim.py         :    68:      - Described: https://paperswithcode.com/method/lars
nn/optim.py         :    68:      - Paper: https://arxiv.org/abs/1708.03888v3
nn/optim.py         :    68:      """

nn/optim.py         :   119:    class LAMB(Optimizer):
nn/optim.py         :   120:      """
nn/optim.py         :   120:      LAMB optimizer with optional weight decay.
nn/optim.py         :   120:      - Described: https://paperswithcode.com/method/lamb
nn/optim.py         :   120:      - Paper: https://arxiv.org/abs/1904.00962
nn/optim.py         :   120:      """

nn/state.py         :     9:  safe_dtypes = {"BOOL":dtypes.bool, "I8":dtypes.int8, "U8":dtypes.uint8, "I16":dtypes.int16, "U16":dtypes.uint16, "I32":dtypes.int, "U32":dtypes.uint,
nn/state.py         :     9:                 "I64":dtypes.int64, "U64":dtypes.uint64, "F16":dtypes.float16, "BF16":dtypes.bfloat16, "F32":dtypes.float32, "F64":dtypes.float64}
nn/state.py         :    11:  inverse_safe_dtypes = {v:k for k,v in safe_dtypes.items()}
nn/__init__.py      :     7:    class BatchNorm:
nn/__init__.py      :     8:      """
nn/__init__.py      :     8:      Applies Batch Normalization over a 2D or 3D input.
nn/__init__.py      :     8:      - Described: https://paperswithcode.com/method/batch-normalization
nn/__init__.py      :     8:      - Paper: https://arxiv.org/abs/1502.03167v3
nn/__init__.py      :     8:      See: `Tensor.batchnorm`
nn/__init__.py      :     8:      ```python exec="true" session="tensor"
nn/__init__.py      :     8:      from tinygrad import Tensor, dtypes, nn
nn/__init__.py      :     8:      import numpy as np
nn/__init__.py      :     8:      np.set_printoptions(precision=4)
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :     8:      norm = nn.BatchNorm(3)
nn/__init__.py      :     8:      t = Tensor.rand(2, 3, 4, 4)
nn/__init__.py      :     8:      print(t.mean().item(), t.std().item())
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :     8:      t = norm(t)
nn/__init__.py      :     8:      print(t.mean().item(), t.std().item())
nn/__init__.py      :     8:      ```
nn/__init__.py      :     8:      """

nn/__init__.py      :    62:  BatchNorm2d = BatchNorm3d = BatchNorm

nn/__init__.py      :    82:    class Conv2d:
nn/__init__.py      :    83:      """
nn/__init__.py      :    83:      Applies a 2D convolution over an input signal composed of several input planes.
nn/__init__.py      :    83:      See: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d
nn/__init__.py      :    83:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :    83:      conv = nn.Conv2d(1, 1, 3)
nn/__init__.py      :    83:      t = Tensor.rand(1, 1, 4, 4)
nn/__init__.py      :    83:      print(t.numpy())
nn/__init__.py      :    83:      ```
nn/__init__.py      :    83:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :    83:      t = conv(t)
nn/__init__.py      :    83:      print(t.numpy())
nn/__init__.py      :    83:      ```
nn/__init__.py      :    83:      """

nn/__init__.py      :   126:    class ConvTranspose2d(Conv2d):
nn/__init__.py      :   127:      """
nn/__init__.py      :   127:      Applies a 2D transposed convolution operator over an input image.
nn/__init__.py      :   127:      See: https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d
nn/__init__.py      :   127:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   127:      conv = nn.ConvTranspose2d(1, 1, 3)
nn/__init__.py      :   127:      t = Tensor.rand(1, 1, 4, 4)
nn/__init__.py      :   127:      print(t.numpy())
nn/__init__.py      :   127:      ```
nn/__init__.py      :   127:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   127:      t = conv(t)
nn/__init__.py      :   127:      print(t.numpy())
nn/__init__.py      :   127:      ```
nn/__init__.py      :   127:      """

nn/__init__.py      :   152:    class Linear:
nn/__init__.py      :   153:      """
nn/__init__.py      :   153:      Applies a linear transformation to the incoming data.
nn/__init__.py      :   153:      See: https://pytorch.org/docs/stable/generated/torch.nn.Linear
nn/__init__.py      :   153:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   153:      lin = nn.Linear(3, 4)
nn/__init__.py      :   153:      t = Tensor.rand(2, 3)
nn/__init__.py      :   153:      print(t.numpy())
nn/__init__.py      :   153:      ```
nn/__init__.py      :   153:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   153:      t = lin(t)
nn/__init__.py      :   153:      print(t.numpy())
nn/__init__.py      :   153:      ```
nn/__init__.py      :   153:      """

nn/__init__.py      :   176:    class GroupNorm:
nn/__init__.py      :   177:      """
nn/__init__.py      :   177:      Applies Group Normalization over a mini-batch of inputs.
nn/__init__.py      :   177:      - Described: https://paperswithcode.com/method/group-normalization
nn/__init__.py      :   177:      - Paper: https://arxiv.org/abs/1803.08494v3
nn/__init__.py      :   177:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   177:      norm = nn.GroupNorm(2, 12)
nn/__init__.py      :   177:      t = Tensor.rand(2, 12, 4, 4) * 2 + 1
nn/__init__.py      :   177:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   177:      ```
nn/__init__.py      :   177:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   177:      t = norm(t)
nn/__init__.py      :   177:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   177:      ```
nn/__init__.py      :   177:      """

nn/__init__.py      :   207:    class InstanceNorm:
nn/__init__.py      :   208:      """
nn/__init__.py      :   208:      Applies Instance Normalization over a mini-batch of inputs.
nn/__init__.py      :   208:      - Described: https://paperswithcode.com/method/instance-normalization
nn/__init__.py      :   208:      - Paper: https://arxiv.org/abs/1607.08022v3
nn/__init__.py      :   208:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   208:      norm = nn.InstanceNorm(3)
nn/__init__.py      :   208:      t = Tensor.rand(2, 3, 4, 4) * 2 + 1
nn/__init__.py      :   208:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   208:      ```
nn/__init__.py      :   208:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   208:      t = norm(t)
nn/__init__.py      :   208:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   208:      ```
nn/__init__.py      :   208:      """

nn/__init__.py      :   234:    class LayerNorm:
nn/__init__.py      :   235:      """
nn/__init__.py      :   235:      Applies Layer Normalization over a mini-batch of inputs.
nn/__init__.py      :   235:      - Described: https://paperswithcode.com/method/layer-normalization
nn/__init__.py      :   235:      - Paper: https://arxiv.org/abs/1607.06450v1
nn/__init__.py      :   235:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   235:      norm = nn.LayerNorm(3)
nn/__init__.py      :   235:      t = Tensor.rand(2, 5, 3) * 2 + 1
nn/__init__.py      :   235:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   235:      ```
nn/__init__.py      :   235:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   235:      t = norm(t)
nn/__init__.py      :   235:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   235:      ```
nn/__init__.py      :   235:      """

nn/__init__.py      :   262:    class LayerNorm2d(LayerNorm):
nn/__init__.py      :   263:      """
nn/__init__.py      :   263:      Applies Layer Normalization over a mini-batch of 2D inputs.
nn/__init__.py      :   263:      See: `LayerNorm`
nn/__init__.py      :   263:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   263:      norm = nn.LayerNorm2d(3)
nn/__init__.py      :   263:      t = Tensor.rand(2, 3, 4, 4) * 2 + 1
nn/__init__.py      :   263:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   263:      ```
nn/__init__.py      :   263:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   263:      t = norm(t)
nn/__init__.py      :   263:      print(t.mean().item(), t.std().item())
nn/__init__.py      :   263:      ```
nn/__init__.py      :   263:      """

nn/__init__.py      :   280:    class RMSNorm:
nn/__init__.py      :   281:      """
nn/__init__.py      :   281:      Applies Root Mean Square Normalization to input.
nn/__init__.py      :   281:      - Described: https://paperswithcode.com/method/rmsnorm
nn/__init__.py      :   281:      - Paper: https://arxiv.org/abs/1910.07467
nn/__init__.py      :   281:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   281:      norm = nn.RMSNorm(4)
nn/__init__.py      :   281:      t = Tensor.arange(12, dtype=dtypes.float).reshape(3, 4)
nn/__init__.py      :   281:      print(t.numpy())
nn/__init__.py      :   281:      ```
nn/__init__.py      :   281:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   281:      print(norm(t).numpy())
nn/__init__.py      :   281:      ```
nn/__init__.py      :   281:      """

nn/__init__.py      :   302:    class Embedding:
nn/__init__.py      :   303:      """
nn/__init__.py      :   303:      A simple lookup table that stores embeddings of a fixed dictionary and size.
nn/__init__.py      :   303:      See: https://pytorch.org/docs/stable/generated/torch.nn.Embedding
nn/__init__.py      :   303:      ```python exec="true" source="above" session="tensor" result="python"
nn/__init__.py      :   303:      emb = nn.Embedding(10, 3)
nn/__init__.py      :   303:      print(emb(Tensor([1, 2, 3, 1])).numpy())
nn/__init__.py      :   303:      ```
nn/__init__.py      :   303:      """

engine/jit.py       :    70:    class GraphRunner(Runner):  # pylint: disable=abstract-method

engine/jit.py       :    88:    class MultiGraphRunner(GraphRunner):  # pylint: disable=abstract-method

engine/jit.py       :   108:  ReturnType = TypeVar('ReturnType')

engine/jit.py       :   109:    class TinyJit(Generic[ReturnType]):

